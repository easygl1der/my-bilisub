#!/usr/bin/env python3
"""
Bç«™é¦–é¡µæ¨èæµè‡ªåŠ¨é‡‡é›†å·¥å…·

åŠŸèƒ½ï¼š
- ä½¿ç”¨ Playwright è®¿é—® B ç«™é¦–é¡µ
- è§£ææ¨èè§†é¢‘å¡ç‰‡ï¼Œæå–æ ‡é¢˜ã€é“¾æ¥ã€UPä¸»ä¿¡æ¯
- æ”¯æŒå¤šæ¬¡åˆ·æ–°é‡‡é›†
- æ”¯æŒå‘½ä»¤è¡Œæ¨¡å¼å’Œ Bot æ¨¡å¼è°ƒç”¨
- å¯é€‰ AI åˆ†æåŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬é‡‡é›†
    python bili_homepage_scraper.py --refresh 10

    # é‡‡é›† + AI åˆ†æ
    python bili_homepage_scraper.py --refresh 10 --analyze

    # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
    python bili_homepage_scraper.py --refresh 5 --output my_homepage.csv

    # æµ‹è¯•æ¨¡å¼ï¼ˆåªé‡‡é›†ä¸€æ¬¡ï¼‰
    python bili_homepage_scraper.py --test
"""

import os
import sys
import json
import time
import asyncio
import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Callable

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32' and sys.stdout.isatty():
    try:
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except (ValueError, AttributeError):
        # å¦‚æœ stdout å·²ç»å…³é—­æˆ–ä¸å¯ç”¨ï¼Œè·³è¿‡ä¿®å¤
        pass

# å¯¼å…¥ Playwright
try:
    from playwright.async_api import async_playwright, Page, Browser
except ImportError:
    print("âŒ æœªå®‰è£… playwright")
    print("è¯·è¿è¡Œ: pip install playwright")
    print("ç„¶åè¿è¡Œ: playwright install chromium")
    sys.exit(1)

# å¯¼å…¥ Cookie ç®¡ç†å™¨
try:
    from bot.cookie_manager import get_cookie, check_cookie
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥ cookie_managerï¼Œå°†ä¸ä½¿ç”¨ Cookie ç™»å½•")
    get_cookie = None
    check_cookie = None


# ==================== é…ç½® ====================

SCRAPER_CONFIG = {
    "max_refresh": 10,              # æœ€å¤§åˆ·æ–°æ¬¡æ•°
    "refresh_interval": 3,          # åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
    "headless": False,              # æ˜¯å¦æ— å¤´æ¨¡å¼
    "use_cookie": True,             # æ˜¯å¦ä½¿ç”¨ Cookie ç™»å½•
    "cookie_path": "config/cookies.txt",
    "output_dir": "output/homepage",
    "bili_homepage": "https://www.bilibili.com",
}

# Bç«™é¦–é¡µæ¨èæµçš„ DOM é€‰æ‹©å™¨ï¼ˆæ ¹æ®å®é™…é¡µé¢ç»“æ„å¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
SELECTORS = {
    # æ¨èè§†é¢‘å¡ç‰‡å®¹å™¨
    "video_card": "a.bvideo-card",

    # è§†é¢‘æ ‡é¢˜
    "title": ".info-title, .title, h3",

    # è§†é¢‘é“¾æ¥ï¼ˆä»å¡ç‰‡çš„ href å±æ€§è·å–ï¼‰
    "link": "href",

    # UPä¸»åç§°
    "uploader": ".up-name, .author-name, .info--author",

    # UPä¸»é“¾æ¥
    "uploader_link": "href",

    # "æ¢ä¸€æ¢"åˆ·æ–°æŒ‰é’®
    "refresh_button": ".refresh-btn, .feed-refresh-btn, button:has-text('æ¢ä¸€æ¢')",

    # éœ€è¦æ’é™¤çš„å†…å®¹ç±»å‹
    "exclude_selectors": [
        ".is-live",           # ç›´æ’­
        ".bangumi-card",      # ç•ªå‰§
        ".ad-card",           # å¹¿å‘Š
    ]
}


# ==================== æ•°æ®æ¨¡å‹ ====================

class VideoInfo:
    """è§†é¢‘ä¿¡æ¯"""

    def __init__(self, bvid: str, title: str, uploader: str,
                 uploader_url: str, video_url: str, duration: str = ""):
        self.bvid = bvid
        self.title = title
        self.uploader = uploader
        self.uploader_url = uploader_url
        self.video_url = video_url
        self.duration = duration
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def to_dict(self) -> Dict:
        return {
            "timestamp": self.timestamp,
            "bvid": self.bvid,
            "title": self.title,
            "uploader": self.uploader,
            "uploader_url": self.uploader_url,
            "video_url": self.video_url,
            "duration": self.duration,
        }

    def __repr__(self):
        return f"VideoInfo(bvid={self.bvid}, title={self.title[:20]}...)"


# ==================== æ ¸å¿ƒçˆ¬è™«ç±» ====================

class BiliHomepageScraper:
    """Bç«™é¦–é¡µæ¨èæµçˆ¬è™«"""

    def __init__(self,
                 max_refresh: int = 10,
                 refresh_interval: int = 3,
                 headless: bool = False,
                 use_cookie: bool = True,
                 progress_callback: Optional[Callable] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            max_refresh: æœ€å¤§åˆ·æ–°æ¬¡æ•°
            refresh_interval: åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            use_cookie: æ˜¯å¦ä½¿ç”¨ Cookie ç™»å½•
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        """
        self.max_refresh = max_refresh
        self.refresh_interval = refresh_interval
        self.headless = headless
        self.use_cookie = use_cookie
        self.progress_callback = progress_callback

        self.videos: List[VideoInfo] = []
        self.bvid_set = set()  # ç”¨äºå»é‡
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    async def _report_progress(self, message: str, level: str = "info"):
        """æŠ¥å‘Šè¿›åº¦"""
        if self.progress_callback:
            await self.progress_callback(message, level)
        else:
            prefix = {
                "info": "â„¹ï¸",
                "success": "âœ…",
                "error": "âŒ",
                "warning": "âš ï¸",
            }.get(level, "ğŸ“Œ")
            print(f"{prefix} {message}")

    def _extract_bvid(self, url: str) -> Optional[str]:
        """ä» URL ä¸­æå– BV å·"""
        if not url:
            return None

        # åŒ¹é… BV å·æ ¼å¼
        match = re.search(r'(BV[\w]+)', url)
        if match:
            return match.group(1)
        return None

    async def _setup_cookies(self):
        """è®¾ç½® Cookie"""
        if not self.use_cookie:
            return

        if get_cookie is None:
            await self._report_progress("Cookie ç®¡ç†å™¨ä¸å¯ç”¨", "warning")
            return

        if not check_cookie('bilibili'):
            await self._report_progress("Bç«™ Cookie æœªé…ç½®", "warning")
            return

        cookie_str = get_cookie('bilibili', 'string')
        if not cookie_str:
            await self._report_progress("è·å– Cookie å¤±è´¥", "warning")
            return

        # è§£æ Cookie å­—ç¬¦ä¸²
        cookies = []
        for part in cookie_str.split(';'):
            part = part.strip()
            if '=' in part:
                name, value = part.split('=', 1)
                cookies.append({
                    'name': name.strip(),
                    'value': value.strip(),
                    'domain': '.bilibili.com',
                    'path': '/',
                })

        if cookies:
            await self.page.context.add_cookies(cookies)
            await self._report_progress(f"å·²è®¾ç½® {len(cookies)} ä¸ª Cookie", "success")

    async def _parse_video_cards(self) -> List[VideoInfo]:
        """è§£æå½“å‰é¡µé¢çš„è§†é¢‘å¡ç‰‡"""
        videos = []

        try:
            # ç­‰å¾…æ¨èè§†é¢‘åŠ è½½
            await self.page.wait_for_selector(SELECTORS["video_card"], timeout=10000)

            # è·å–æ‰€æœ‰è§†é¢‘å¡ç‰‡
            cards = await self.page.query_selector_all(SELECTORS["video_card"])

            await self._report_progress(f"æ‰¾åˆ° {len(cards)} ä¸ªè§†é¢‘å¡ç‰‡", "info")

            for card in cards:
                try:
                    # è·å–è§†é¢‘é“¾æ¥
                    video_url = await card.get_attribute("href")
                    if not video_url:
                        continue

                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if video_url.startswith("//"):
                        video_url = "https:" + video_url
                    elif video_url.startswith("/"):
                        video_url = "https://www.bilibili.com" + video_url

                    # æå– BV å·
                    bvid = self._extract_bvid(video_url)
                    if not bvid:
                        continue

                    # å»é‡
                    if bvid in self.bvid_set:
                        continue
                    self.bvid_set.add(bvid)

                    # è·å–æ ‡é¢˜
                    title_elem = await card.query_selector(SELECTORS["title"])
                    title = await title_elem.inner_text() if title_elem else "æœªçŸ¥æ ‡é¢˜"
                    title = title.strip()

                    # è·å– UP ä¸»ä¿¡æ¯
                    uploader = "æœªçŸ¥UPä¸»"
                    uploader_url = ""

                    # å°è¯•å¤šç§é€‰æ‹©å™¨
                    for selector in [".up-name", ".author-name", ".info--author"]:
                        uploader_elem = await card.query_selector(selector)
                        if uploader_elem:
                            # æ£€æŸ¥æ˜¯å¦æœ‰é“¾æ¥
                            uploader_link = await uploader_elem.query_selector("a")
                            if uploader_link:
                                uploader = await uploader_link.inner_text()
                                uploader_url_attr = await uploader_link.get_attribute("href")
                                if uploader_url_attr:
                                    if uploader_url_attr.startswith("//"):
                                        uploader_url = "https:" + uploader_url_attr
                                    elif uploader_url_attr.startswith("/"):
                                        uploader_url = "https://www.bilibili.com" + uploader_url_attr
                                    else:
                                        uploader_url = uploader_url_attr
                            else:
                                uploader = await uploader_elem.inner_text()
                            break

                    uploader = uploader.strip()

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤ï¼ˆç›´æ’­ã€ç•ªå‰§ç­‰ï¼‰
                    should_exclude = False
                    for exclude_selector in SELECTORS["exclude_selectors"]:
                        exclude_elem = await card.query_selector(exclude_selector)
                        if exclude_elem:
                            should_exclude = True
                            break

                    if should_exclude:
                        continue

                    # åˆ›å»ºè§†é¢‘ä¿¡æ¯
                    video = VideoInfo(
                        bvid=bvid,
                        title=title,
                        uploader=uploader,
                        uploader_url=uploader_url,
                        video_url=video_url,
                    )
                    videos.append(video)

                except Exception as e:
                    # è·³è¿‡è§£æå¤±è´¥çš„å¡ç‰‡
                    continue

        except Exception as e:
            await self._report_progress(f"è§£æè§†é¢‘å¡ç‰‡å¤±è´¥: {e}", "error")

        return videos

    async def _click_refresh_button(self) -> bool:
        """ç‚¹å‡»åˆ·æ–°æŒ‰é’®"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            for selector in [SELECTORS["refresh_button"], "button:has-text('æ¢ä¸€æ¢')",
                           ".refresh-btn", ".feed-refresh"]:
                try:
                    button = await self.page.query_selector(selector)
                    if button:
                        await button.click()
                        await asyncio.sleep(0.5)
                        return True
                except Exception:
                    continue

            # å¦‚æœæ‰¾ä¸åˆ°åˆ·æ–°æŒ‰é’®ï¼Œå°è¯•ç›´æ¥åˆ·æ–°é¡µé¢
            await self._report_progress("æœªæ‰¾åˆ°åˆ·æ–°æŒ‰é’®ï¼Œå°è¯•ç›´æ¥åˆ·æ–°é¡µé¢", "warning")
            await self.page.reload()
            return True

        except Exception as e:
            await self._report_progress(f"ç‚¹å‡»åˆ·æ–°æŒ‰é’®å¤±è´¥: {e}", "error")
            return False

    async def start(self):
        """å¯åŠ¨çˆ¬è™«"""
        self.playwright = await async_playwright().start()

        # å¯åŠ¨æµè§ˆå™¨
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
            ]
        )

        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )

        # åˆ›å»ºé¡µé¢
        self.page = await context.new_page()

        # è®¾ç½® Cookie
        await self._setup_cookies()

        # è®¿é—®é¦–é¡µ
        await self._report_progress("æ­£åœ¨è®¿é—® B ç«™é¦–é¡µ...", "info")
        await self.page.goto(SCRAPER_CONFIG["bili_homepage"], wait_until="networkidle")
        await self._report_progress("é¦–é¡µåŠ è½½å®Œæˆ", "success")

    async def scrape(self) -> List[VideoInfo]:
        """å¼€å§‹é‡‡é›†"""
        if not self.page:
            await self.start()

        all_videos = []

        for i in range(self.max_refresh):
            round_num = i + 1
            await self._report_progress(f"\n--- ç¬¬ {round_num}/{self.max_refresh} è½®é‡‡é›† ---", "info")

            # è§£æå½“å‰é¡µé¢
            videos = await self._parse_video_cards()
            await self._report_progress(f"ç¬¬ {round_num} è½®é‡‡é›†åˆ° {len(videos)} ä¸ªæ–°è§†é¢‘", "success")

            all_videos.extend(videos)
            self.videos.extend(videos)

            # å¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼Œç‚¹å‡»åˆ·æ–°
            if i < self.max_refresh - 1:
                await self._report_progress(f"ç­‰å¾… {self.refresh_interval} ç§’ååˆ·æ–°...", "info")
                await asyncio.sleep(self.refresh_interval)

                refresh_success = await self._click_refresh_button()
                if not refresh_success:
                    await self._report_progress("åˆ·æ–°å¤±è´¥ï¼Œåœæ­¢é‡‡é›†", "error")
                    break

                # ç­‰å¾…æ–°å†…å®¹åŠ è½½
                await asyncio.sleep(2)

        return all_videos

    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    def get_videos(self) -> List[VideoInfo]:
        """è·å–é‡‡é›†çš„è§†é¢‘åˆ—è¡¨"""
        return self.videos

    def get_unique_count(self) -> int:
        """è·å–å»é‡åçš„è§†é¢‘æ•°é‡"""
        return len(self.bvid_set)


# ==================== æ•°æ®å­˜å‚¨ ====================

def save_to_csv(videos: List[VideoInfo], output_path: str):
    """ä¿å­˜è§†é¢‘ä¿¡æ¯åˆ° CSV æ–‡ä»¶"""
    import csv

    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
        if not videos:
            return

        writer = csv.DictWriter(f, fieldnames=list(videos[0].to_dict().keys()))
        writer.writeheader()

        for video in videos:
            writer.writerow(video.to_dict())

    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")


def save_to_json(videos: List[VideoInfo], output_path: str):
    """ä¿å­˜è§†é¢‘ä¿¡æ¯åˆ° JSON æ–‡ä»¶"""
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    data = {
        "é‡‡é›†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "è§†é¢‘æ€»æ•°": len(videos),
        "å”¯ä¸€è§†é¢‘æ•°": len(set(v.bvid for v in videos)),
        "è§†é¢‘åˆ—è¡¨": [v.to_dict() for v in videos]
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")


# ==================== AI åˆ†æ ====================

async def analyze_with_ai(videos: List[VideoInfo], model: str = 'flash-lite') -> str:
    """ä½¿ç”¨ Gemini API åˆ†æè§†é¢‘ç±»å‹

    Args:
        videos: è§†é¢‘åˆ—è¡¨
        model: Gemini æ¨¡å‹

    Returns:
        åˆ†ææŠ¥å‘Šæ–‡æœ¬
    """
    try:
        from analysis.gemini_subtitle_summary import GeminiClient, GEMINI_MODELS
    except ImportError:
        return "âŒ æ— æ³•å¯¼å…¥ Gemini å®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥ analysis/gemini_subtitle_summary.py æ˜¯å¦å­˜åœ¨"

    if not videos:
        return "âŒ æ²¡æœ‰è§†é¢‘å¯ä¾›åˆ†æ"

    # æ„å»ºè§†é¢‘åˆ—è¡¨æ–‡æœ¬
    videos_text = ""
    for i, video in enumerate(videos, 1):
        videos_text += f"{i}. {video.title}\n   UPä¸»: {video.uploader}\n   é“¾æ¥: {video.video_url}\n\n"

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹Bç«™é¦–é¡µæ¨èè§†é¢‘åˆ—è¡¨ï¼Œå°†å®ƒä»¬åˆ†ç±»ç»Ÿè®¡ã€‚

è§†é¢‘åˆ—è¡¨:
{videos_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä½¿ç”¨ Markdown æ ¼å¼ï¼‰:

## è§†é¢‘ç±»å‹åˆ†å¸ƒ
| ç±»å‹ | æ•°é‡ | å æ¯” |
|------|------|------|
| ... | ... | ... |

## æ¨èåå¥½åˆ†æ
[æè¿°è´¦å·çš„æ¨èåå¥½ï¼Œåå‘å“ªäº›ç±»å‹çš„å†…å®¹]

## é«˜é¢‘ UP ä¸»
| UPä¸» | å‡ºç°æ¬¡æ•° |
|------|----------|
| ... | ... |

## å†…å®¹ç‰¹è‰²åˆ†æ
[åˆ†ææ¨èå†…å®¹çš„ç‰¹ç‚¹ï¼Œå¦‚è§†é¢‘é•¿åº¦ã€æ›´æ–°æ—¶é—´ã€ä¸»é¢˜ç‰¹ç‚¹ç­‰]

è§†é¢‘ç±»å‹å‚è€ƒåˆ†ç±»:
- AI/å¤§æ¨¡å‹/ç§‘æŠ€
- çŸ¥è¯†/ç¤¾ç§‘/äººæ–‡
- è´¢ç»/èŒåœº
- Vlog/æ—…è¡Œ
- æ•°ç è¯„æµ‹
- æ¸¸æˆå¨±ä¹
- åŠ¨æ¼«/å½±è§†
- éŸ³ä¹/èˆè¹ˆ
- ç¾é£Ÿ/ç”Ÿæ´»
- ç¤¾ä¼šçºªå®
- å…¶ä»–

è¯·ç¡®ä¿åˆ†ç±»å‡†ç¡®ï¼Œç»Ÿè®¡æ•°æ®çœŸå®ã€‚"""

    try:
        client = GeminiClient(model=model)
        result = client.generate_content(prompt)

        if result['success']:
            return result['text']
        else:
            return f"âŒ AI åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"

    except Exception as e:
        return f"âŒ AI åˆ†æå¼‚å¸¸: {str(e)}"


# ==================== ä¸»ç¨‹åº ====================

async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = BiliHomepageScraper(
        max_refresh=args.refresh,
        refresh_interval=args.interval,
        headless=args.headless,
        use_cookie=not args.no_cookie,
    )

    try:
        # å¯åŠ¨çˆ¬è™«
        await scraper.start()

        # å¼€å§‹é‡‡é›†
        videos = await scraper.scrape()

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print(f"ğŸ“Š é‡‡é›†å®Œæˆ!")
        print(f"  æ€»è®¡é‡‡é›†: {len(videos)} ä¸ªè§†é¢‘")
        print(f"  å”¯ä¸€è§†é¢‘: {scraper.get_unique_count()} ä¸ª")
        print("=" * 60)

        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(SCRAPER_CONFIG["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if args.output:
            csv_path = args.output
            json_path = args.output.replace('.csv', '.json')
        else:
            csv_path = output_dir / f"homepage_videos_{timestamp}.csv"
            json_path = output_dir / f"homepage_videos_{timestamp}.json"

        save_to_csv(videos, csv_path)
        save_to_json(videos, json_path)

        # AI åˆ†æ
        if args.analyze:
            print("\n" + "=" * 60)
            print("ğŸ¤– æ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
            print("=" * 60)

            report = await analyze_with_ai(videos, args.model)

            # ä¿å­˜åˆ†ææŠ¥å‘Š
            report_path = output_dir / f"homepage_analysis_{timestamp}.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"# Bç«™é¦–é¡µæ¨èåˆ†ææŠ¥å‘Š\n\n")
                f.write(f"**é‡‡é›†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"**åˆ·æ–°æ¬¡æ•°**: {args.refresh}\n\n")
                f.write(f"**è§†é¢‘æ€»æ•°**: {len(videos)}\n\n")
                f.write(f"**å”¯ä¸€è§†é¢‘æ•°**: {scraper.get_unique_count()}\n\n")
                f.write("---\n\n")
                f.write(report)

            print(f"\nâœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

            # æ‰“å°æŠ¥å‘Šæ‘˜è¦
            print("\n" + "=" * 60)
            print("ğŸ“‹ åˆ†ææŠ¥å‘Š:")
            print("=" * 60)
            print(report[:1000])
            if len(report) > 1000:
                print("...")
                print(f"\n(å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_path})")

    finally:
        await scraper.close()


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="Bç«™é¦–é¡µæ¨èæµè‡ªåŠ¨é‡‡é›†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # åŸºæœ¬é‡‡é›†
    python bili_homepage_scraper.py --refresh 10

    # é‡‡é›† + AI åˆ†æ
    python bili_homepage_scraper.py --refresh 10 --analyze

    # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
    python bili_homepage_scraper.py --refresh 5 --output my_homepage.csv

    # æµ‹è¯•æ¨¡å¼ï¼ˆåªé‡‡é›†ä¸€æ¬¡ï¼‰
    python bili_homepage_scraper.py --test

    # æ— å¤´æ¨¡å¼
    python bili_homepage_scraper.py --refresh 5 --headless
        """
    )

    parser.add_argument('-r', '--refresh', type=int, default=10,
                        help='åˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('-i', '--interval', type=int, default=3,
                        help='åˆ·æ–°é—´éš”ç§’æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('-o', '--output', type=str,
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: output/homepage/homepage_videos_æ—¶é—´æˆ³.csvï¼‰')
    parser.add_argument('--analyze', action='store_true',
                        help='é‡‡é›†å®Œæˆåè¿›è¡Œ AI åˆ†æ')
    parser.add_argument('--model', choices=['flash', 'flash-lite', 'pro'],
                        default='flash-lite', help='Gemini æ¨¡å‹ï¼ˆé»˜è®¤: flash-liteï¼‰')
    parser.add_argument('--headless', action='store_true',
                        help='æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰')
    parser.add_argument('--no-cookie', action='store_true',
                        help='ä¸ä½¿ç”¨ Cookie ç™»å½•')
    parser.add_argument('--test', action='store_true',
                        help='æµ‹è¯•æ¨¡å¼ï¼ˆåªé‡‡é›†ä¸€æ¬¡ï¼‰')

    args = parser.parse_args()

    # æµ‹è¯•æ¨¡å¼
    if args.test:
        args.refresh = 1
        args.headless = False
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªé‡‡é›†ä¸€æ¬¡ï¼Œæ˜¾ç¤ºæµè§ˆå™¨çª—å£")

    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
