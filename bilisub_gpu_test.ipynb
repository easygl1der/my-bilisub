{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Whisper GPU vs CPU é€Ÿåº¦å¯¹æ¯”æµ‹è¯•\n",
    "\n",
    "æµ‹è¯• GPU åŠ é€Ÿå¯¹ Whisper å­—å¹•æå–çš„æ€§èƒ½æå‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q openai-whisper yt-dlp\n",
    "\n",
    "import torch\n",
    "import whisper\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"\\nğŸ”¥ GPU å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU åç§°: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ä¸‹è½½æµ‹è¯•éŸ³é¢‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ä½¿ç”¨ä¸€ä¸ªçŸ­è§†é¢‘æµ‹è¯•ï¼ˆBç«™çŸ­è§†é¢‘ï¼‰\n# ä½ å¯ä»¥æ›¿æ¢æˆä»»æ„è§†é¢‘é“¾æ¥\nTEST_URL = \"https://www.bilibili.com/video/BV1GJ411x7h7\"  # @param {type:\"string\"}\n\nimport yt_dlp\n\nprint(\"ä¸‹è½½æµ‹è¯•éŸ³é¢‘...\")\nydl_opts = {\n    'format': 'bestaudio/best',\n    'outtmpl': 'test_audio.%(ext)s',\n    'quiet': True,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    info = ydl.extract_info(TEST_URL, download=True)\n    duration = info.get('duration', 0)\n    title = info.get('title', 'test')\n\naudio_file = list(Path('.').glob('test_audio.*'))[0]\nprint(f\"\\néŸ³é¢‘: {audio_file}\")\nprint(f\"æ—¶é•¿: {duration}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ CPU vs GPU é€Ÿåº¦å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æµ‹è¯•æ¨¡å‹\nMODEL_SIZE = \"small\"  # @param [\"tiny\", \"base\", \"small\", \"medium\"]\n\nresults = {}\n\n# ============ CPU æµ‹è¯• ============\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ–¥ï¸  CPU æ¨¡å¼æµ‹è¯•\")\nprint(\"=\"*60)\n\nimport gc\ngc.collect()\n\nload_start = time.time()\nmodel_cpu = whisper.load_model(MODEL_SIZE, device=\"cpu\")\nload_time_cpu = time.time() - load_start\nprint(f\"æ¨¡å‹åŠ è½½: {load_time_cpu:.2f}ç§’\")\n\ntranscribe_start = time.time()\nresult_cpu = model_cpu.transcribe(str(audio_file), language='zh', verbose=False)\ntranscribe_time_cpu = time.time() - transcribe_start\nprint(f\"è½¬å½•è€—æ—¶: {transcribe_time_cpu:.2f}ç§’\")\nprint(f\"é€Ÿåº¦æ¯”: {duration/transcribe_time_cpu:.2f}x å®æ—¶\")\n\ndel model_cpu\ngc.collect()\n\nresults['cpu'] = {\n    'load': load_time_cpu,\n    'transcribe': transcribe_time_cpu,\n    'total': load_time_cpu + transcribe_time_cpu,\n    'speed_ratio': duration / transcribe_time_cpu\n}\n\n# ============ GPU æµ‹è¯• ============\nif torch.cuda.is_available():\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸš€ GPU æ¨¡å¼æµ‹è¯•\")\n    print(\"=\"*60)\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    load_start = time.time()\n    model_gpu = whisper.load_model(MODEL_SIZE, device=\"cuda\")\n    load_time_gpu = time.time() - load_start\n    print(f\"æ¨¡å‹åŠ è½½: {load_time_gpu:.2f}ç§’\")\n    \n    transcribe_start = time.time()\n    result_gpu = model_gpu.transcribe(str(audio_file), language='zh', verbose=False)\n    transcribe_time_gpu = time.time() - transcribe_start\n    print(f\"è½¬å½•è€—æ—¶: {transcribe_time_gpu:.2f}ç§’\")\n    print(f\"é€Ÿåº¦æ¯”: {duration/transcribe_time_gpu:.2f}x å®æ—¶\")\n    \n    del model_gpu\n    torch.cuda.empty_cache()\n    \n    results['gpu'] = {\n        'load': load_time_gpu,\n        'transcribe': transcribe_time_gpu,\n        'total': load_time_gpu + transcribe_time_gpu,\n        'speed_ratio': duration / transcribe_time_gpu\n    }\nelse:\n    print(\"\\nâš ï¸ GPU ä¸å¯ç”¨ï¼Œè·³è¿‡ GPU æµ‹è¯•\")\n    print(\"   è¯·åœ¨: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ å¯ç”¨ GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ æµ‹è¯•ç»“æœå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æµ‹è¯•ç»“æœå¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\næµ‹è¯•éŸ³é¢‘æ—¶é•¿: {duration}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)\")\n",
    "print(f\"Whisper æ¨¡å‹: {MODEL_SIZE}\")\n",
    "\n",
    "if 'cpu' in results:\n",
    "    print(f\"\\nğŸ–¥ï¸  CPU:\")\n",
    "    print(f\"   æ¨¡å‹åŠ è½½: {results['cpu']['load']:.2f}ç§’\")\n",
    "    print(f\"   è½¬å½•è€—æ—¶: {results['cpu']['transcribe']:.2f}ç§’\")\n",
    "    print(f\"   æ€»è€—æ—¶: {results['cpu']['total']:.2f}ç§’\")\n",
    "    print(f\"   é€Ÿåº¦æ¯”: {results['cpu']['speed_ratio']:.2f}x å®æ—¶\")\n",
    "\n",
    "if 'gpu' in results:\n",
    "    print(f\"\\nğŸš€ GPU:\")\n",
    "    print(f\"   æ¨¡å‹åŠ è½½: {results['gpu']['load']:.2f}ç§’\")\n",
    "    print(f\"   è½¬å½•è€—æ—¶: {results['gpu']['transcribe']:.2f}ç§’\")\n",
    "    print(f\"   æ€»è€—æ—¶: {results['gpu']['total']:.2f}ç§’\")\n",
    "    print(f\"   é€Ÿåº¦æ¯”: {results['gpu']['speed_ratio']:.2f}x å®æ—¶\")\n",
    "    \n",
    "    # è®¡ç®—åŠ é€Ÿæ¯”\n",
    "    speedup = results['cpu']['transcribe'] / results['gpu']['transcribe']\n",
    "    time_saved = results['cpu']['transcribe'] - results['gpu']['transcribe']\n",
    "    \n",
    "    print(f\"\\nğŸ† GPU åŠ é€Ÿæ•ˆæœ:\")\n",
    "    print(f\"   è½¬å½•åŠ é€Ÿ: {speedup:.2f}x\")\n",
    "    print(f\"   æ—¶é—´èŠ‚çœ: {time_saved:.2f}ç§’ ({time_saved/duration*100:.1f}%)\")\n",
    "    \n",
    "    # æ¨ç®—é•¿è§†é¢‘æ—¶é—´\n",
    "    for video_len in [10, 30, 60]:  # åˆ†é’Ÿ\n",
    "        seconds = video_len * 60\n",
    "        cpu_time = seconds / results['cpu']['speed_ratio']\n",
    "        gpu_time = seconds / results['gpu']['speed_ratio']\n",
    "        print(f\"   {video_len}åˆ†é’Ÿè§†é¢‘: CPU {cpu_time/60:.1f}åˆ†é’Ÿ vs GPU {gpu_time/60:.1f}åˆ†é’Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ éªŒè¯ç»“æœä¸€è‡´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯ CPU å’Œ GPU ç»“æœæ˜¯å¦ä¸€è‡´\n",
    "if 'cpu' in results and 'gpu' in results:\n",
    "    print(\"\\néªŒè¯ç»“æœä¸€è‡´æ€§...\")\n",
    "    \n",
    "    # å¯¹æ¯”æ–‡æœ¬é•¿åº¦\n",
    "    cpu_len = len(result_cpu['text'])\n",
    "    gpu_len = len(result_gpu['text'])\n",
    "    print(f\"æ–‡æœ¬é•¿åº¦: CPU {cpu_len} vs GPU {gpu_len}\")\n",
    "    \n",
    "    # å¯¹æ¯”æ®µè½æ•°\n",
    "    cpu_segments = len(result_cpu['segments'])\n",
    "    gpu_segments = len(result_gpu['segments'])\n",
    "    print(f\"æ®µè½æ•°é‡: CPU {cpu_segments} vs GPU {gpu_segments}\")\n",
    "    \n",
    "    # å¯¹æ¯”å‰3ä¸ªæ®µè½\n",
    "    print(\"\\nå‰3ä¸ªæ®µè½å¯¹æ¯”:\")\n",
    "    for i in range(min(3, cpu_segments)):\n",
    "        cpu_text = result_cpu['segments'][i]['text'].strip()\n",
    "        gpu_text = result_gpu['segments'][i]['text'].strip()\n",
    "        match = \"âœ…\" if cpu_text == gpu_text else \"âš ï¸\"\n",
    "        print(f\"  [{i+1}] {match}\")\n",
    "        print(f\"      CPU: {cpu_text}\")\n",
    "        print(f\"      GPU: {gpu_text}\")\n",
    "    \n",
    "    print(\"\\nç»“è®º: CPU å’Œ GPU ç»“æœåŸºæœ¬ä¸€è‡´ï¼Œå·®å¼‚å¯èƒ½æ¥è‡ªæµ®ç‚¹è¿ç®—ç²¾åº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ é¢„æœŸç»“æœ\n",
    "\n",
    "| é¡¹ç›® | CPU | GPU | åŠ é€Ÿæ¯” |\n",
    "|------|-----|-----|--------|\n",
    "| æ¨¡å‹åŠ è½½ | ~3ç§’ | ~2ç§’ | 1.5x |\n",
    "| è½¬å½•é€Ÿåº¦ | ~5-10xå®æ—¶ | ~30-50xå®æ—¶ | **5-10x** |\n",
    "\n",
    "### å…¸å‹åœºæ™¯æ¨ç®—ï¼ˆsmall æ¨¡å‹ï¼ŒColab T4 GPUï¼‰:\n",
    "\n",
    "| è§†é¢‘æ—¶é•¿ | CPU è€—æ—¶ | GPU è€—æ—¶ |\n",
    "|----------|----------|----------|\n",
    "| 5åˆ†é’Ÿ | ~1åˆ†é’Ÿ | ~10ç§’ |\n",
    "| 10åˆ†é’Ÿ | ~2åˆ†é’Ÿ | ~20ç§’ |\n",
    "| 30åˆ†é’Ÿ | ~6åˆ†é’Ÿ | ~1åˆ†é’Ÿ |\n",
    "| 60åˆ†é’Ÿ | ~12åˆ†é’Ÿ | ~2åˆ†é’Ÿ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}