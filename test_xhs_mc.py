#!/usr/bin/env python3
"""
æµ‹è¯•å°çº¢ä¹¦å›¾æ–‡æå– - ä½¿ç”¨ MediaCrawler
"""

import os
import sys
import asyncio
import re
import json
from pathlib import Path

# æ·»åŠ  MediaCrawler åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "MediaCrawler"))

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


async def get_user_notes_media_crawler(user_id: str, cookie: str = ""):
    """
    ä½¿ç”¨ MediaCrawler è·å–ç”¨æˆ·ç¬”è®°

    éœ€è¦å…ˆåœ¨ MediaCrawler ä¸­é…ç½®å¥½ cookie
    """
    try:
        from media_platform.xiaohongshu.client import XiaoHongShuClient
        from media_platform.xiaohongshu.exception import DataFetchError

        client = XiaoHongShuClient()

        # è·å–ç”¨æˆ·ä¿¡æ¯
        print(f"ğŸ“¡ è·å–ç”¨æˆ·ä¿¡æ¯...")
        user_info = await client.get_creator_info(user_id, "", "")

        if user_info:
            print(f"âœ… ç”¨æˆ·å: {user_info.get('nickname', 'æœªçŸ¥')}")
            print(f"   ç¬”è®°æ•°: {user_info.get('notes_count', 0)}")

        return None

    except ImportError as e:
        print(f"âŒ MediaCrawler å¯¼å…¥å¤±è´¥: {e}")
        print(f"   è¯·ç¡®ä¿ MediaCrawler å­æ¨¡å—å·²åˆå§‹åŒ–")
        return None
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")
        return None


def test_extract_from_url(note_url: str):
    """
    ç›´æ¥ä»ç¬”è®°é“¾æ¥æå–å›¾ç‰‡
    """
    import requests

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.xiaohongshu.com/',
    }

    print(f"ğŸ“¡ è¯·æ±‚ç¬”è®°: {note_url[:80]}...")

    response = requests.get(note_url, headers=headers, timeout=30)
    print(f"çŠ¶æ€ç : {response.status_code}")

    if 'ä½ è®¿é—®çš„é¡µé¢ä¸è§äº†' in response.text:
        print("âŒ é¡µé¢æ— æ³•è®¿é—® - éœ€è¦å®Œæ•´çš„ xsec_token")
        return None

    html = response.text

    # æŸ¥æ‰¾ç¬”è®°ID
    note_id_match = re.search(r'/explore/([a-f0-9]+)', note_url)
    if note_id_match:
        note_id = note_id_match.group(1)
        print(f"ğŸ†” ç¬”è®°ID: {note_id}")

    # æå–æ ‡é¢˜
    title_match = re.search(r'<title[^>]*>(.+?)</title>', html)
    if title_match:
        title = title_match.group(1).replace(' - å°çº¢ä¹¦', '').strip()
        print(f"ğŸ“ æ ‡é¢˜: {title}")

    # æœç´¢å›¾ç‰‡URL
    print(f"\nğŸ” æœç´¢å›¾ç‰‡...")

    # æ–¹æ³•1: urlDefault
    urls1 = re.findall(r'"urlDefault":"([^"]+)"', html)
    # æ–¹æ³•2: sns-webpic
    urls2 = re.findall(r'(https://sns-webpic[^"\s]+)', html)

    all_urls = []
    for url in urls1 + urls2:
        url = url.split('?')[0]
        try:
            url = url.encode('utf-8').decode('unicode_escape')
        except:
            pass
        url = url.replace(r'\/', '/')
        if 'xhscdn' in url and url not in all_urls:
            all_urls.append(url)

    print(f"âœ… æ‰¾åˆ° {len(all_urls)} å¼ å›¾ç‰‡")

    if all_urls:
        print(f"\nğŸ–¼ï¸  å›¾ç‰‡åˆ—è¡¨:")
        for i, url in enumerate(all_urls[:10], 1):
            print(f"   [{i}] {url}")

    return {
        'title': title if title_match else "æœªçŸ¥",
        'images': all_urls
    }


def main():
    print("\n" + "=" * 80)
    print("ğŸ” å°çº¢ä¹¦å›¾æ–‡æå–æµ‹è¯•")
    print("=" * 80)
    print()

    # æµ‹è¯•ç¬”è®°é“¾æ¥ - è¯·æ›¿æ¢ä¸ºæœ‰æ•ˆçš„é“¾æ¥
    test_url = "https://www.xiaohongshu.com/explore/64e6403e0000000012004563?xsec_source=pc_feed"

    print("ğŸ“ æµ‹è¯•é“¾æ¥ï¼ˆé»˜è®¤ï¼‰")
    print(f"   {test_url}")
    print()

    # ä½ å¯ä»¥è¾“å…¥è‡ªå·±çš„é“¾æ¥
    import sys
    if len(sys.argv) > 1:
        test_url = sys.argv[1]
        print(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰é“¾æ¥")
        print(f"   {test_url}")
        print()

    result = test_extract_from_url(test_url)

    if result and result['images']:
        print(f"\nâœ… æå–æˆåŠŸ!")
        print(f"   æ ‡é¢˜: {result['title']}")
        print(f"   å›¾ç‰‡æ•°: {len(result['images'])}")

        # ä¿å­˜ç»“æœ
        output_dir = Path("output/test_xhs")
        output_dir.mkdir(parents=True, exist_ok=True)

        import datetime
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = output_dir / f"result_{timestamp}.txt"

        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {result['title']}\n\n")
            f.write(f"å›¾ç‰‡åˆ—è¡¨ ({len(result['images'])}å¼ ):\n\n")
            for i, url in enumerate(result['images'], 1):
                f.write(f"[{i}] {url}\n")

        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
    else:
        print("\nâŒ æå–å¤±è´¥")
        print("\nğŸ’¡ æç¤º:")
        print("   1. ç¡®ä¿é“¾æ¥åŒ…å«å®Œæ•´çš„ xsec_token å‚æ•°")
        print("   2. ä»å°çº¢ä¹¦APPåˆ†äº«è·å–é“¾æ¥")
        print("   3. ç¬”è®°å¿…é¡»æ˜¯å›¾æ–‡ç±»å‹ï¼Œä¸èƒ½æ˜¯è§†é¢‘")


if __name__ == "__main__":
    main()
