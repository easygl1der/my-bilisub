{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¬ Bç«™/å°çº¢ä¹¦è§†é¢‘å­—å¹•æå– - Colabç‰ˆ\n",
    "\n",
    "## åŠŸèƒ½\n",
    "1. **ä¸‹è½½è§†é¢‘** - æ”¯æŒBç«™ã€å°çº¢ä¹¦ç­‰å¹³å°\n",
    "2. **Whisper è¯†åˆ«** - GPU åŠ é€Ÿè¯­éŸ³è¯†åˆ«\n",
    "3. **GLM ä¼˜åŒ–** - æ™ºè°± API ä¼˜åŒ–å­—å¹•\n",
    "4. **Google Drive ä¿å­˜** - ç»“æœè‡ªåŠ¨ä¿å­˜åˆ°äº‘ç«¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ æŒ‚è½½ Google Driveï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‚è½½ Google Drive ç”¨äºä¿å­˜ç»“æœ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# åˆ›å»ºå·¥ä½œç›®å½•\n",
    "!mkdir -p /content/drive/MyDrive/bilisub_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…æ ¸å¿ƒä¾èµ–\n",
    "!pip install -q openai-whisper yt-dlp torch tqdm requests\n",
    "\n",
    "# æ£€æŸ¥ GPU\n",
    "import torch\n",
    "print(f\"\\nğŸ”¥ GPU å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU åç§°: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ é…ç½® APIï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®æ™ºè°± GLM APIï¼ˆç”¨äºå­—å¹•ä¼˜åŒ–ï¼‰\n",
    "# å¦‚æœä¸éœ€è¦ GLM ä¼˜åŒ–ï¼Œå¯ä»¥è·³è¿‡\n",
    "\n",
    "ZHIPU_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "import os\n",
    "if ZHIPU_API_KEY:\n",
    "    os.environ['ZHIPU_API_KEY'] = ZHIPU_API_KEY\n",
    "    print(\"âœ… API å¯†é’¥å·²è®¾ç½®\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªè®¾ç½® API å¯†é’¥ï¼Œå°†è·³è¿‡ GLM ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= é…ç½®åŒºåŸŸ =============\n",
    "\n",
    "# è§†é¢‘é“¾æ¥ï¼ˆæ”¯æŒ Bç«™ã€å°çº¢ä¹¦ç­‰ï¼‰\n",
    "VIDEO_URL = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Whisper æ¨¡å‹é€‰æ‹©\n",
    "# tiny: æœ€å¿« (~1GB), base: è¾ƒå¿« (~1GB), small: å¹³è¡¡ (~2GB)\n",
    "# medium: è¾ƒå‡† (~5GB), large: æœ€å‡† (~10GB, éœ€è¦æ›´å¤§æ˜¾å­˜)\n",
    "WHISPER_MODEL = \"small\"  # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
    "\n",
    "# æ˜¯å¦å¯ç”¨ GLM ä¼˜åŒ–\n",
    "ENABLE_GLM = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"/content/bilisub_output\"\n",
    "\n",
    "# =====================================\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ¬ æ¨¡å‹: {WHISPER_MODEL}\")\n",
    "print(f\"ğŸ¤– GLMä¼˜åŒ–: {'å¯ç”¨' if ENABLE_GLM and ZHIPU_API_KEY else 'ç¦ç”¨'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ è¿è¡Œå®Œæ•´æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import yt_dlp\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "# ==================== æ ¸å¿ƒå‡½æ•° ====================\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"æ¸…ç†æ–‡ä»¶å\"\"\"\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)[:100]\n",
    "\n",
    "\n",
    "def download_audio(url, output_dir):\n",
    "    \"\"\"ä¸‹è½½éŸ³é¢‘\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"â¬‡ï¸ æ­¥éª¤ 1/3: ä¸‹è½½éŸ³é¢‘\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': str(Path(output_dir) / 'audio.%(ext)s'),\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        title = info.get('title', 'unknown')\n",
    "        duration = info.get('duration', 0)\n",
    "    \n",
    "    # æ‰¾åˆ°ä¸‹è½½çš„éŸ³é¢‘æ–‡ä»¶\n",
    "    audio_files = list(Path(output_dir).glob('audio.*'))\n",
    "    if not audio_files:\n",
    "        raise Exception(\"éŸ³é¢‘ä¸‹è½½å¤±è´¥\")\n",
    "    \n",
    "    audio_file = audio_files[0]\n",
    "    print(f\"âœ… ä¸‹è½½å®Œæˆ: {time.time()-start:.1f}ç§’\")\n",
    "    print(f\"ğŸ“Œ æ ‡é¢˜: {title}\")\n",
    "    print(f\"â±ï¸ æ—¶é•¿: {duration}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)\")\n",
    "    \n",
    "    return audio_file, title, duration\n",
    "\n",
    "\n",
    "def transcribe_whisper(audio_file, model_size, output_dir):\n",
    "    \"\"\"Whisper è¯­éŸ³è¯†åˆ«\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ™ï¸ æ­¥éª¤ 2/3: Whisper è¯­éŸ³è¯†åˆ«\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"ğŸ”§ è®¾å¤‡: {device.upper()}\")\n",
    "    print(f\"ğŸ“¦ æ¨¡å‹: {model_size}\")\n",
    "    print(f\"â³ åŠ è½½æ¨¡å‹...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    load_start = time.time()\n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½: {time.time()-load_start:.1f}ç§’\")\n",
    "    \n",
    "    # è½¬å½•\n",
    "    print(\"ğŸ”Š æ­£åœ¨è½¬å½•...\")\n",
    "    transcribe_start = time.time()\n",
    "    result = model.transcribe(\n",
    "        str(audio_file),\n",
    "        language=\"zh\",\n",
    "        task=\"transcribe\",\n",
    "        verbose=False\n",
    "    )\n",
    "    transcribe_time = time.time() - transcribe_start\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    print(f\"âœ… è½¬å½•å®Œæˆ: {transcribe_time:.1f}ç§’\")\n",
    "    print(f\"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def save_srt(segments, output_path):\n",
    "    \"\"\"ä¿å­˜ä¸º SRT æ ¼å¼\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for i, seg in enumerate(segments, 1):\n",
    "            start_time = seg['start']\n",
    "            end_time = seg['end']\n",
    "            \n",
    "            # è½¬æ¢æ—¶é—´æ ¼å¼\n",
    "            def format_time(t):\n",
    "                hours = int(t // 3600)\n",
    "                minutes = int((t % 3600) // 60)\n",
    "                seconds = int(t % 60)\n",
    "                ms = int((t % 1) * 1000)\n",
    "                return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{ms:03d}\"\n",
    "            \n",
    "            f.write(f\"{i}\\n\")\n",
    "            f.write(f\"{format_time(start_time)} --> {format_time(end_time)}\\n\")\n",
    "            f.write(f\"{seg['text'].strip()}\\n\\n\")\n",
    "\n",
    "\n",
    "def optimize_with_glm(segments, api_key, output_dir):\n",
    "    \"\"\"ä½¿ç”¨ GLM ä¼˜åŒ–å­—å¹•\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¤– æ­¥éª¤ 3/3: GLM å­—å¹•ä¼˜åŒ–\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    import requests\n",
    "    \n",
    "    BATCH_SIZE = 5\n",
    "    optimized_segments = []\n",
    "    \n",
    "    total_batches = (len(segments) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    for i in range(0, len(segments), BATCH_SIZE):\n",
    "        batch = segments[i:i+BATCH_SIZE]\n",
    "        batch_num = i // BATCH_SIZE + 1\n",
    "        \n",
    "        # åˆå¹¶æ–‡æœ¬\n",
    "        combined_text = '\\n'.join([s['text'].strip() for s in batch])\n",
    "        \n",
    "        # æ„é€  prompt\n",
    "        prompt = f\"\"\"è¯·ä¼˜åŒ–ä»¥ä¸‹è§†é¢‘å­—å¹•æ–‡æœ¬ï¼š\n",
    "\n",
    "1. ä¿®æ­£é”™åˆ«å­—\n",
    "2. æ·»åŠ æ ‡ç‚¹ç¬¦å·\n",
    "3. æ”¹å–„è¯­å¥æµç•…åº¦\n",
    "4. ä¿æŒåŸæ„ï¼Œä¸æ·»åŠ æ–°å†…å®¹\n",
    "\n",
    "åŸæ–‡ï¼ˆ{len(batch)}è¡Œï¼‰ï¼š\n",
    "{combined_text}\n",
    "\n",
    "ä¼˜åŒ–åï¼ˆ{len(batch)}è¡Œï¼Œæ¯è¡Œä¸€ä¸ªï¼‰ï¼š\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://open.bigmodel.cn/api/paas/v4/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": \"glm-4-flash\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"temperature\": 0.3\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            optimized_text = result['choices'][0]['message']['content'].strip()\n",
    "            \n",
    "            # åˆ†å‰²å¹¶æ›´æ–°\n",
    "            opt_lines = optimized_text.split('\\n')\n",
    "            for j, seg in enumerate(batch):\n",
    "                if j < len(opt_lines):\n",
    "                    optimized_segments.append({**seg, 'text': opt_lines[j].strip()})\n",
    "                else:\n",
    "                    optimized_segments.append(seg)\n",
    "            \n",
    "            print(f\"   æ‰¹æ¬¡ [{batch_num}/{total_batches}] å®Œæˆ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ æ‰¹æ¬¡ [{batch_num}/{total_batches}] å¤±è´¥: {e}\")\n",
    "            optimized_segments.extend(batch)\n",
    "    \n",
    "    return optimized_segments\n",
    "\n",
    "\n",
    "def process_video(url, model_size, enable_glm, output_dir):\n",
    "    \"\"\"å®Œæ•´å¤„ç†æµç¨‹\"\"\"\n",
    "    print(\"\\n\" + \"ğŸ”¥\"*30)\n",
    "    print(\"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘\")\n",
    "    print(\"ğŸ”¥\"*30)\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    # 1. ä¸‹è½½éŸ³é¢‘\n",
    "    audio_file, title, duration = download_audio(url, output_dir)\n",
    "    safe_title = sanitize_filename(title)\n",
    "    \n",
    "    # 2. Whisper è½¬å½•\n",
    "    result = transcribe_whisper(audio_file, model_size, output_dir)\n",
    "    segments = result['segments']\n",
    "    \n",
    "    # ä¿å­˜åŸå§‹ SRT\n",
    "    original_srt = Path(output_dir) / f\"{safe_title}_original.srt\"\n",
    "    save_srt(segments, original_srt)\n",
    "    print(f\"\\nğŸ“„ åŸå§‹å­—å¹•: {original_srt.name}\")\n",
    "    \n",
    "    # 3. GLM ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰\n",
    "    if enable_glm and ZHIPU_API_KEY:\n",
    "        segments = optimize_with_glm(segments, ZHIPU_API_KEY, output_dir)\n",
    "        \n",
    "        # ä¿å­˜ä¼˜åŒ–åçš„ SRT\n",
    "        optimized_srt = Path(output_dir) / f\"{safe_title}_optimized.srt\"\n",
    "        save_srt(segments, optimized_srt)\n",
    "        print(f\"ğŸ“„ ä¼˜åŒ–å­—å¹•: {optimized_srt.name}\")\n",
    "    \n",
    "    # ä¿å­˜çº¯æ–‡æœ¬\n",
    "    txt_file = Path(output_dir) / f\"{safe_title}.txt\"\n",
    "    with open(txt_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(result['text'])\n",
    "    print(f\"ğŸ“„ çº¯æ–‡æœ¬: {txt_file.name}\")\n",
    "    \n",
    "    total_time = time.time() - total_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… å¤„ç†å®Œæˆï¼\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)\")\n",
    "    print(f\"ğŸ“Š è¯†åˆ«æ®µè½æ•°: {len(segments)}\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'duration': duration,\n",
    "        'segments_count': len(segments),\n",
    "        'total_time': total_time,\n",
    "        'files': {\n",
    "            'original_srt': str(original_srt),\n",
    "            'optimized_srt': str(Path(output_dir) / f\"{safe_title}_optimized.srt\") if enable_glm else None,\n",
    "            'txt': str(txt_file)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== æ‰§è¡Œ ====================\n",
    "\n",
    "if not VIDEO_URL:\n",
    "    print(\"âŒ è¯·å…ˆè®¾ç½® VIDEO_URL\")\n",
    "else:\n",
    "    result = process_video(\n",
    "        url=VIDEO_URL,\n",
    "        model_size=WHISPER_MODEL,\n",
    "        enable_glm=ENABLE_GLM,\n",
    "        output_dir=OUTPUT_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ ä¿å­˜åˆ° Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤åˆ¶ç»“æœåˆ° Google Drive\n",
    "import shutil\n",
    "\n",
    "drive_output = \"/content/drive/MyDrive/bilisub_output\"\n",
    "\n",
    "# å¤åˆ¶æ‰€æœ‰æ–‡ä»¶\n",
    "for file in Path(OUTPUT_DIR).glob('*'):\n",
    "    if file.is_file():\n",
    "        dest = Path(drive_output) / file.name\n",
    "        shutil.copy2(file, dest)\n",
    "        print(f\"ğŸ“ å·²å¤åˆ¶: {file.name}\")\n",
    "\n",
    "print(f\"\\nâœ… æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ° Google Drive:\")\n",
    "print(f\"   {drive_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ—å‡ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶\n",
    "print(\"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\\n\")\n",
    "for file in Path(OUTPUT_DIR).glob('*'):\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size / 1024\n",
    "        print(f\"  â€¢ {file.name} ({size:.1f} KB)\")\n",
    "\n",
    "# ä¸‹è½½æ–‡ä»¶\n",
    "from google.colab import files\n",
    "\n",
    "# ä¸‹è½½æ‰€æœ‰ SRT å’Œ TXT æ–‡ä»¶\n",
    "for file in Path(OUTPUT_DIR).glob('*.srt'):\n",
    "    files.download(str(file))\n",
    "for file in Path(OUTPUT_DIR).glob('*.txt'):\n",
    "    files.download(str(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ä½¿ç”¨è¯´æ˜\n",
    "\n",
    "### 1. é€‰æ‹©è¿è¡Œæ—¶\n",
    "- é¡¶éƒ¨èœå•: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹\n",
    "- ç¡¬ä»¶åŠ é€Ÿå™¨: **GPU** (æ¨è T4 æˆ– A100)\n",
    "\n",
    "### 2. è®¾ç½®å‚æ•°\n",
    "- ç¬¬ 4 ä¸ªå•å…ƒæ ¼ä¸­è®¾ç½® `VIDEO_URL` (Bç«™/å°çº¢ä¹¦é“¾æ¥)\n",
    "- é€‰æ‹© `WHISPER_MODEL` (æ¨è `small` æˆ– `medium`)\n",
    "- å¦‚éœ€ GLM ä¼˜åŒ–ï¼Œè®¾ç½® `ZHIPU_API_KEY` å’Œ `ENABLE_GLM`\n",
    "\n",
    "### 3. è¿è¡Œ\n",
    "- æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼\n",
    "- æˆ–ç‚¹å‡»: è¿è¡Œæ—¶ â†’ å…¨éƒ¨è¿è¡Œ\n",
    "\n",
    "### 4. è·å–ç»“æœ\n",
    "- è‡ªåŠ¨ä¿å­˜åˆ° Google Drive\n",
    "- æˆ–æ‰‹åŠ¨ä¸‹è½½åˆ°æœ¬åœ°\n",
    "\n",
    "### âš ï¸ æ³¨æ„äº‹é¡¹\n",
    "- Colab å…è´¹ç‰ˆæœ‰ä¼šè¯æ—¶é—´é™åˆ¶ï¼ˆ~90åˆ†é’Ÿï¼‰\n",
    "- é•¿è§†é¢‘å»ºè®®ç”¨ `medium` æˆ– `small` æ¨¡å‹\n",
    "- GPU æ˜¾å­˜ä¸è¶³æ—¶ï¼Œä½¿ç”¨æ›´å°çš„æ¨¡å‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
