# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (ä½¿ç”¨ MediaCrawler)
åŸºäº MediaCrawler çš„å®Œæ•´åŠŸèƒ½ï¼Œæ”¯æŒç­¾åéªŒè¯

ä½¿ç”¨æ–¹æ³•:
    python fetch_xhs_comments.py

åŠŸèƒ½:
    1. çˆ¬å–æŒ‡å®šç¬”è®°çš„æ‰€æœ‰è¯„è®º
    2. ä¿å­˜ä¸º CSV æ ¼å¼
    3. æ”¯æŒä½¿ç”¨å·²æœ‰ Cookie æˆ–æ‰«ç ç™»å½•
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# ============================================================================
# é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œä¿®æ”¹è®¾ç½®
# ============================================================================

ENABLE_COMMENTS = True   # å¯ç”¨è¯„è®ºçˆ¬å–
MAX_COMMENTS_COUNT = 50  # æœ€å¤§è¯„è®ºæ•°é‡
HEADLESS = True          # æ— å¤´æ¨¡å¼ï¼ˆæœ‰ cookie æ—¶ï¼‰
SAVE_LOGIN_STATE = True  # ä¿å­˜ç™»å½•çŠ¶æ€

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "xhs_comments_output"


# ============================================================================
# Cookie åŠ è½½ - ä½¿ç”¨ç»Ÿä¸€ Cookie ç®¡ç†å™¨
# ============================================================================

def load_cookies_from_file():
    """ä» config/cookies.txt åŠ è½½ cookiesï¼ˆç»Ÿä¸€ç®¡ç†ï¼‰"""
    try:
        from cookie_manager import get_cookie, check_cookie

        # æ£€æŸ¥ cookie æ˜¯å¦é…ç½®
        if not check_cookie('xiaohongshu'):
            return None

        # è·å– cookie
        cookie_str = get_cookie('xiaohongshu', 'string')

        if cookie_str:
            print(f"  [Cookie] âœ… å·²ä» config/cookies.txt åŠ è½½å°çº¢ä¹¦ Cookie")
            return cookie_str
        else:
            return None

    except Exception as e:
        print(f"  [Cookie] è¯»å–å¤±è´¥: {e}")
        return None


# ============================================================================
# é…ç½® MediaCrawler
# ============================================================================

def setup_mediacrawler_config(note_url: str):
    """é…ç½® MediaCrawler"""
    try:
        # å¯¼å…¥é…ç½®
        sys.path.insert(0, str(Path(__file__).parent / "MediaCrawler"))
        import config

        # è®¾ç½®å¹³å°å’Œç±»å‹
        config.PLATFORM = "xhs"
        config.CRAWLER_TYPE = "detail"

        # è®¾ç½®ç›®æ ‡ç¬”è®°
        config.XHS_SPECIFIED_NOTE_URL_LIST = [note_url]

        # è¯„è®ºé…ç½®
        config.ENABLE_GET_COMMENTS = ENABLE_COMMENTS
        config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = MAX_COMMENTS_COUNT
        config.SAVE_LOGIN_STATE = SAVE_LOGIN_STATE
        config.SAVE_DATA_OPTION = "json"

        # å°è¯•åŠ è½½ cookies
        cookie_str = load_cookies_from_file()

        if cookie_str:
            config.COOKIES = cookie_str
            config.LOGIN_TYPE = "cookie"
            config.HEADLESS = HEADLESS
            print(f"  [é…ç½®] ç™»å½•æ–¹å¼: Cookie (Headless={HEADLESS})")
        else:
            config.LOGIN_TYPE = "qrcode"
            config.HEADLESS = False
            print(f"  [é…ç½®] ç™»å½•æ–¹å¼: æ‰«ç ç™»å½•")

        print(f"  [é…ç½®] ç¬”è®°é“¾æ¥: {note_url[:80]}...")
        print(f"  [é…ç½®] æœ€å¤§è¯„è®ºæ•°: {MAX_COMMENTS_COUNT}")

        return True
    except Exception as e:
        print(f"  [é…ç½®] å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# è¿è¡Œçˆ¬è™«
# ============================================================================

async def run_crawler():
    """è¿è¡Œçˆ¬è™«"""
    try:
        sys.path.insert(0, str(Path(__file__).parent / "MediaCrawler"))
        from main import main as crawler_main
        await crawler_main()
        return True
    except Exception as e:
        print(f"  [çˆ¬è™«] è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# æŸ¥æ‰¾å¹¶è¯»å–çˆ¬å–çš„è¯„è®ºæ•°æ®
# ============================================================================

def find_latest_comments():
    """æŸ¥æ‰¾æœ€æ–°çš„è¯„è®ºæ–‡ä»¶"""
    possible_dirs = [
        Path("MediaCrawler/data/xhs/json"),
        Path("MediaCrawler/data/xhs"),
    ]

    # æŸ¥æ‰¾åŒ…å«è¯„è®ºçš„ JSON æ–‡ä»¶
    for data_dir in possible_dirs:
        if not data_dir.exists():
            continue

        for json_file in data_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯„è®ºæ•°æ®
                has_comments = False
                if isinstance(data, list):
                    for item in data:
                        if 'comments' in item or 'comment_list' in item:
                            has_comments = True
                            break

                if has_comments:
                    return str(json_file), data
            except:
                continue

    return None, None


def extract_comments_from_data(data: list) -> list:
    """ä»æ•°æ®ä¸­æå–è¯„è®º"""
    comments = []

    for item in data:
        # å°è¯•å¤šç§è¯„è®ºå­—æ®µ
        comments_list = (
            item.get('comments', []) or
            item.get('comment_list', []) or
            item.get('note_comments', [])
        )

        for comment in comments_list:
            if not isinstance(comment, dict):
                continue

            # è§£æè¯„è®ºå­—æ®µ
            parsed = {
                'comment_id': comment.get('id', comment.get('comment_id', '')),
                'content': (
                    comment.get('content', '') or
                    comment.get('text', '') or
                    comment.get('note_comment', '') or
                    comment.get('comment_text', '')
                ),
                'likes': (
                    comment.get('like_count', 0) or
                    comment.get('likes', 0) or
                    comment.get('liked_count', 0) or 0
                ),
                'author': (
                    comment.get('nickname', '') or
                    comment.get('user_name', '') or
                    comment.get('author', '') or '[æœªçŸ¥]'
                ),
                'ip_location': comment.get('ip_location', ''),
                'create_time': comment.get('create_time', comment.get('ctime', '')),
                'platform': 'xiaohongshu'
            }

            if parsed['content']:
                comments.append(parsed)

    return comments


def save_comments_csv(comments: list, note_id: str) -> str:
    """ä¿å­˜è¯„è®ºåˆ° CSV"""
    if not comments:
        return None

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    csv_file = os.path.join(OUTPUT_DIR, f"xhs_comments_{note_id}_{timestamp}.csv")

    import csv
    with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
        fieldnames = ['comment_id', 'author', 'content', 'likes', 'ip_location', 'create_time', 'platform']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(comments)

    return csv_file


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

async def main_async(note_url: str = None, count: int = None):
    """å¼‚æ­¥ä¸»ç¨‹åº"""
    global MAX_COMMENTS_COUNT

    print("\n" + "="*70)
    print("å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (MediaCrawlerç‰ˆ)")
    print("="*70)

    # è·å–ç¬”è®°é“¾æ¥
    if not note_url:
        print("\nè¯·è¾“å…¥å°çº¢ä¹¦ç¬”è®°é“¾æ¥:")
        print("ç¤ºä¾‹: https://www.xiaohongshu.com/explore/694f9e53000000001e013674")
        note_url = input("\nç¬”è®°é“¾æ¥: ").strip()

    if not note_url:
        print("âŒ é“¾æ¥ä¸èƒ½ä¸ºç©º")
        return

    # æ›´æ–°é…ç½®
    if count:
        MAX_COMMENTS_COUNT = count

    print(f"\n[æ­¥éª¤ 1] é…ç½® MediaCrawler")
    print("-" * 50)

    if not setup_mediacrawler_config(note_url):
        print("\nâŒ é…ç½®å¤±è´¥")
        return

    print(f"\n[æ­¥éª¤ 2] è¿è¡Œçˆ¬è™«")
    print("-" * 50)
    print("  æç¤º: é¦–æ¬¡è¿è¡Œéœ€è¦æ‰«ç ç™»å½•ï¼Œç™»å½•çŠ¶æ€ä¼šè‡ªåŠ¨ä¿å­˜")

    success = await run_crawler()

    if not success:
        print("\nâŒ çˆ¬è™«è¿è¡Œå¤±è´¥")

    print(f"\n[æ­¥éª¤ 3] æå–è¯„è®ºæ•°æ®")
    print("-" * 50)

    json_file, data = find_latest_comments()

    if not json_file:
        print("  âš ï¸  æœªæ‰¾åˆ°è¯„è®ºæ•°æ®æ–‡ä»¶")
        print("  ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("     1. ç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Cookie æ˜¯å¦æœ‰æ•ˆ")
        print("     2. ç¬”è®°æ²¡æœ‰è¯„è®º")
        print("     3. è§¦å‘äº†é£æ§")
        return

    print(f"  âœ… æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {Path(json_file).name}")

    # æå–è¯„è®º
    comments = extract_comments_from_data(data)

    if not comments:
        print("  âš ï¸  æ•°æ®ä¸­æœªæ‰¾åˆ°è¯„è®º")
        return

    print(f"  âœ… æå–åˆ° {len(comments)} æ¡è¯„è®º")

    # ä¿å­˜åˆ° CSV
    note_id = note_url.split('/')[-1].split('?')[0]
    csv_file = save_comments_csv(comments, note_id)

    print(f"\n[æ­¥éª¤ 4] ä¿å­˜ç»“æœ")
    print("-" * 50)
    print(f"  âœ… å·²ä¿å­˜åˆ°: {csv_file}")

    # æ˜¾ç¤ºé¢„è§ˆ
    print(f"\n[è¯„è®ºé¢„è§ˆ]")
    print("-" * 50)
    for i, comment in enumerate(comments[:5], 1):
        content = comment.get('content', '')[:80]
        if len(content) == 80:
            content += "..."
        print(f"  {i}. [{comment['likes']}èµ] {comment['author']}: {content}")

    if len(comments) > 5:
        print(f"  ... è¿˜æœ‰ {len(comments) - 5} æ¡")

    print("\n" + "="*70)
    print("âœ… å®Œæˆï¼")
    print("="*70)
    print(f"\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ†æè¯„è®º:")
    print(f"   python comment_analyzer.py -csv {csv_file} -o analysis.md")
    print("="*70)


def main(note_url: str = None, count: int = None):
    """åŒæ­¥ä¸»ç¨‹åºå…¥å£"""
    asyncio.run(main_async(note_url, count))


if __name__ == "__main__":
    try:
        # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
        url = sys.argv[1] if len(sys.argv) > 1 else None
        cnt = int(sys.argv[2]) if len(sys.argv) > 2 else None
        main(url, cnt)
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
