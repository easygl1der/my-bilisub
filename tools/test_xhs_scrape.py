#!/usr/bin/env python3
"""
å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
- æµ‹è¯•çˆ¬å–å°çº¢ä¹¦æ¨èé¡µ
- éªŒè¯å®Œæ•´é“¾æ¥æ˜¯å¦åŒ…å« xsec_token
- éªŒè¯çˆ¬å–æ‰¹æ¬¡æ˜¯å¦æ­£ç¡®è®°å½•
- è¾“å‡ºæµ‹è¯•ç»“æœåˆ° CSV

ä½¿ç”¨ç¤ºä¾‹:
    python tools/test_xhs_scrape.py
    python tools/test_xhs_scrape.py --refresh-count 2
"""

import argparse
import asyncio
import sys
import csv
from pathlib import Path
from datetime import datetime

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from playwright.async_api import async_playwright

# ==================== è·¯å¾„é…ç½® ====================
PROJECT_DIR = Path(__file__).parent.parent
OUTPUT_DIR = PROJECT_DIR / "output" / "xiaohongshu_test"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# ==================== Cookie è¯»å– ====================
def read_xhs_cookie():
    """ä» config/cookies.txt è¯»å–å°çº¢ä¹¦Cookie"""
    cookie_file = PROJECT_DIR / "config" / "cookies.txt"
    if not cookie_file.exists():
        return ""

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾ xiaohongshu_full= æ ¼å¼
    import re
    match = re.search(r'xiaohongshu_full=([^\n]+)', content)
    if match:
        return match.group(1)

    # æŸ¥æ‰¾ [xiaohongshu] éƒ¨åˆ†
    xhs_section = re.search(r'\[xiaohongshu\](.*?)\[', content, re.DOTALL)
    if xhs_section:
        section = xhs_section.group(1)
        cookies = []
        for line in section.split('\n'):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                cookies.append(f"{key.strip()}={value.strip()}")
        return '; '.join(cookies)

    return ""


# ==================== çˆ¬å–æµ‹è¯• ====================
async def test_scrape(
    refresh_count: int = 2,
    max_notes: int = 20,
    cookie: str = ""
) -> list:
    """æµ‹è¯•çˆ¬å–åŠŸèƒ½"""
    notes_collected = []
    seen_urls = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )

        # è®¾ç½®Cookie
        if cookie:
            try:
                cookies = []
                for item in cookie.split(';'):
                    item = item.strip()
                    if '=' in item:
                        key, value = item.split('=', 1)
                        cookies.append({
                            'name': key,
                            'value': value,
                            'domain': '.xiaohongshu.com',
                            'path': '/'
                        })
                await context.add_cookies(cookies)
                print("âœ… Cookieå·²è®¾ç½®")
            except Exception as e:
                print(f"âš ï¸  Cookieè®¾ç½®å¤±è´¥: {e}")

        page = await context.new_page()

        print(f"\nğŸ“¡ è®¿é—®å°çº¢ä¹¦é¦–é¡µ...")
        try:
            await page.goto('https://www.xiaohongshu.com/', wait_until='domcontentloaded', timeout=60000)
            await asyncio.sleep(5)
        except Exception as e:
            print(f"âš ï¸  é¡µé¢åŠ è½½é—®é¢˜: {e}")

        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        page_content = await page.content()
        if 'ç™»å½•' in page_content and 'æ³¨å†Œ' in page_content:
            print("\nâš ï¸  æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€")
            print("ğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•")
            print("â³ ç­‰å¾…90ç§’...ç™»å½•å®Œæˆåä¼šè‡ªåŠ¨ç»§ç»­")
            await asyncio.sleep(90)
            print("âœ… ç»§ç»­æ‰§è¡Œ...")

        print(f"\nğŸ”„ å¼€å§‹æµ‹è¯•é‡‡é›†ï¼ˆåˆ·æ–°{refresh_count}æ¬¡ï¼‰...")

        for i in range(refresh_count):
            print(f"\n{'='*60}")
            print(f"  æ‰¹æ¬¡ {i+1}/{refresh_count}")
            print(f"{'='*60}")

            # æ»šåŠ¨åŠ è½½
            for scroll in range(10):
                await page.evaluate('window.scrollBy(0, window.innerHeight)')
                await asyncio.sleep(1)

            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(2)

            # è·å–æ‰€æœ‰é“¾æ¥å’Œä¿¡æ¯
            try:
                notes_data = await page.evaluate('''
                    () => {
                        const notes = [];
                        const seen = new Set();

                        // æŸ¥æ‰¾æ‰€æœ‰ç¬”è®°å¡ç‰‡
                        const cards = document.querySelectorAll('section, article, [class*="note"], [class*="card"], div[class*="item"]');

                        cards.forEach(card => {
                            // æŸ¥æ‰¾é“¾æ¥
                            const link = card.querySelector('a[href*="/explore/"], a[href*="/discovery/item/"]');
                            if (!link) return;

                            const url = link.href;

                            // è·å– xsec_token
                            let xsecToken = '';
                            let xsecSource = 'pc_homepage';

                            // ä» URL ä¸­æå– xsec_token
                            try {
                                const urlParams = new URLSearchParams(url.split('?')[1]);
                                xsecToken = urlParams.get('xsec_token') || '';
                                if (urlParams.get('xsec_source')) {
                                    xsecSource = urlParams.get('xsec_source');
                                }
                            } catch (e) {}

                            // æå–ç¬”è®°ID
                            let noteId = "";
                            const idMatch = url.match(/([a-f0-9]{32})/);
                            if (idMatch) noteId = idMatch[1];

                            if (!noteId) return;
                            if (seen.has(noteId)) return;
                            seen.add(noteId);

                            // è·å–æ ‡é¢˜
                            let title = "æ— æ ‡é¢˜";
                            const textNodes = card.querySelectorAll('span, div, p, h1, h2, h3');
                            for (const node of textNodes) {
                                const text = node.textContent?.trim();
                                if (text && text.length > 3 && text.length < 100 && !/^\\d+$/.test(text)) {
                                    if (!text.includes('èµ') && !text.includes('å…³æ³¨') &&
                                        !text.includes('åˆ†äº«') && !text.includes('æ”¶è—')) {
                                        title = text.substring(0, 100);
                                        break;
                                    }
                                }
                            }

                            // è·å–ä½œè€…
                            let author = "æœªçŸ¥ä½œè€…";
                            const authorNodes = card.querySelectorAll('span, a');
                            for (const node of authorNodes) {
                                const text = node.textContent?.trim();
                                if (text && text.length > 1 && text.length < 30) {
                                    if (!/\\d/.test(text)) {
                                        author = text;
                                        break;
                                    }
                                }
                            }

                            // è·å–ç‚¹èµæ•°
                            let likes = "0";
                            const allNodes = card.querySelectorAll('*');
                            for (const node of allNodes) {
                                const text = node.textContent?.trim();
                                if (text && /^\\d+/.test(text)) {
                                    const parentClass = node.parentElement?.className || '';
                                    if (parentClass.includes('like') || parentClass.includes('count')) {
                                        const num = parseInt(text);
                                        if (num < 1000000 && num > 0) {
                                            likes = text;
                                            break;
                                        }
                                    }
                                }
                            }

                            // åˆ¤æ–­ç±»å‹
                            let type = 'image';
                            const hasVideo = card.querySelector('video');
                            if (hasVideo) {
                                type = 'video';
                            } else {
                                const hasPlayIcon = card.querySelector('[class*="play"], [class*="video"]');
                                if (hasPlayIcon) {
                                    type = 'video';
                                }
                            }

                            notes.push({
                                url: url,
                                noteId: noteId,
                                title: title,
                                author: author,
                                likes: likes,
                                type: type,
                                xsecToken: xsecToken,
                                xsecSource: xsecSource
                            });
                        });

                        return notes;
                    }
                ''')

                print(f"    æ‰¾åˆ° {len(notes_data)} ä¸ªç¬”è®°")

                for note in notes_data:
                    note_id = note['noteId']

                    # å»é‡
                    if note_id in seen_urls:
                        continue
                    seen_urls.add(note_id)

                    # æ„å»ºå®Œæ•´é“¾æ¥
                    xsec_token = note.get('xsecToken', '')
                    xsec_source = note.get('xsecSource', 'pc_homepage')
                    note['original_url'] = note['url']  # ä¿å­˜åŸå§‹URL

                    if xsec_token:
                        full_url = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source={xsec_source}"
                        note['å®Œæ•´é“¾æ¥'] = full_url
                    else:
                        note['å®Œæ•´é“¾æ¥'] = note['url']

                    # æ·»åŠ æ‰¹æ¬¡ä¿¡æ¯
                    note['çˆ¬å–æ‰¹æ¬¡'] = i + 1
                    note['é‡‡é›†æ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                    notes_collected.append(note)
                    type_emoji = 'ğŸ¬' if note['type'] == 'video' else 'ğŸ–¼ï¸'

                    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    print(f"    [{len(notes_collected)}] {type_emoji} {note['title']}")
                    print(f"        ç¬”è®°ID: {note_id}")
                    print(f"        ä½œè€…: {note['author']} | ç‚¹èµ: {note['likes']}")
                    print(f"        xsec_token: {'âœ…' if xsec_token else 'âŒ æ— '}")
                    if xsec_token:
                        print(f"        åŸå§‹URL: {note['original_url'][:80]}...")
                        print(f"        å®Œæ•´é“¾æ¥: {note['å®Œæ•´é“¾æ¥'][:80]}...")
                    else:
                        print(f"        é“¾æ¥: {note['url'][:80]}...")

                    if len(notes_collected) >= max_notes:
                        break

            except Exception as e:
                print(f"    âš ï¸  è§£æå‡ºé”™: {e}")

            # åˆ·æ–°
            if i < refresh_count - 1:
                print("    åˆ·æ–°é¡µé¢...")
                await page.reload(wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(3)

        await browser.close()

    return notes_collected


# ==================== æµ‹è¯•æŠ¥å‘Š ====================
def generate_test_report(notes: list) -> str:
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report_lines = [
        "# å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•æŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**é‡‡é›†æ•°é‡**: {len(notes)}",
        "",
        "---",
        "",
    ]

    # ç»Ÿè®¡ xsec_token è¦†ç›–ç‡
    with_token = sum(1 for n in notes if n.get('xsecToken'))
    without_token = len(notes) - with_token
    token_rate = (with_token / len(notes) * 100) if notes else 0

    report_lines.extend([
        "## ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡",
        "",
        f"- **æ€»é‡‡é›†æ•°**: {len(notes)}",
        f"- **åŒ…å« xsec_token**: {with_token} ({token_rate:.1f}%)",
        f"- **ä¸å« xsec_token**: {without_token} ({100-token_rate:.1f}%)",
        "",
    ])

    # æ‰¹æ¬¡åˆ†å¸ƒ
    batch_count = {}
    for note in notes:
        batch = note.get('çˆ¬å–æ‰¹æ¬¡', 0)
        batch_count[batch] = batch_count.get(batch, 0) + 1

    report_lines.extend([
        "## ğŸ”„ æ‰¹æ¬¡åˆ†å¸ƒ",
        "",
        "| æ‰¹æ¬¡ | æ•°é‡ |",
        "|------|------|",
    ])
    for batch in sorted(batch_count.keys()):
        report_lines.append(f"| ç¬¬{batch}æ¬¡ | {batch_count[batch]} |")
    report_lines.append("")

    # ç±»å‹åˆ†å¸ƒ
    video_count = sum(1 for n in notes if n.get('type') == 'video')
    image_count = len(notes) - video_count

    report_lines.extend([
        "## ğŸ“¹ ç±»å‹åˆ†å¸ƒ",
        "",
        f"- **è§†é¢‘**: {video_count} ({video_count/len(notes)*100:.1f}%)" if notes else "- **è§†é¢‘**: 0",
        f"- **å›¾æ–‡**: {image_count} ({image_count/len(notes)*100:.1f}%)" if notes else "- **å›¾æ–‡**: 0",
        "",
    ])

    # å®Œæ•´æ•°æ®åˆ—è¡¨
    report_lines.extend([
        "## ğŸ“‹ å®Œæ•´æ•°æ®åˆ—è¡¨",
        "",
        "| åºå· | æ‰¹æ¬¡ | ç±»å‹ | æ ‡é¢˜ | ä½œè€… | ç‚¹èµ | xsec_token | é“¾æ¥ |",
        "|------|------|------|------|------|------|------------|------|",
    ])

    for i, note in enumerate(notes, 1):
        type_emoji = 'ğŸ¬' if note.get('type') == 'video' else 'ğŸ–¼ï¸'
        title = note.get('title', 'æ— æ ‡é¢˜')[:30] + '...' if len(note.get('title', '')) > 30 else note.get('title', 'æ— æ ‡é¢˜')
        link = note.get('å®Œæ•´é“¾æ¥', '')[:50] + '...' if len(note.get('å®Œæ•´é“¾æ¥', '')) > 50 else note.get('å®Œæ•´é“¾æ¥', '')
        token_status = 'âœ…' if note.get('xsecToken') else 'âŒ'

        report_lines.append(f"| {i} | ç¬¬{note.get('çˆ¬å–æ‰¹æ¬¡', 0)}æ¬¡ | {type_emoji} | {title} | {note.get('author', 'æœªçŸ¥')} | {note.get('likes', '0')} | {token_status} | {link} |")

    return "\n".join(report_lines)


# ==================== CSVå¯¼å‡º ====================
def export_to_csv(notes, output_path):
    """å¯¼å‡ºæµ‹è¯•ç»“æœåˆ°CSV"""
    csv_columns = [
        'åºå·', 'æ‰¹æ¬¡', 'æ ‡é¢˜', 'åŸå§‹é“¾æ¥', 'å®Œæ•´é“¾æ¥', 'ç¬”è®°ID',
        'ä½œè€…', 'ç‚¹èµæ•°', 'ç±»å‹', 'xsec_token', 'xsec_source', 'é‡‡é›†æ—¶é—´'
    ]

    with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=csv_columns)
        writer.writeheader()

        for i, note in enumerate(notes, 1):
            writer.writerow({
                'åºå·': i,
                'æ‰¹æ¬¡': note.get('çˆ¬å–æ‰¹æ¬¡', ''),
                'æ ‡é¢˜': note.get('title', ''),
                'åŸå§‹é“¾æ¥': note.get('original_url', ''),
                'å®Œæ•´é“¾æ¥': note.get('å®Œæ•´é“¾æ¥', ''),
                'ç¬”è®°ID': note.get('noteId', ''),
                'ä½œè€…': note.get('author', ''),
                'ç‚¹èµæ•°': note.get('likes', ''),
                'ç±»å‹': note.get('type', ''),
                'xsec_token': note.get('xsecToken', ''),
                'xsec_source': note.get('xsecSource', ''),
                'é‡‡é›†æ—¶é—´': note.get('é‡‡é›†æ—¶é—´', ''),
            })

    print(f"\nğŸ“ CSVå·²ä¿å­˜: {output_path}")


# ==================== ä¸»ç¨‹åº ====================
async def main():
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•è„šæœ¬')

    parser.add_argument('--refresh-count', type=int, default=2,
                       help='åˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤: 2ï¼‰')
    parser.add_argument('--max-notes', type=int, default=20,
                       help='æœ€å¤šé‡‡é›†ç¬”è®°æ•°ï¼ˆé»˜è®¤: 20ï¼‰')

    args = parser.parse_args()

    print(f"\n{'='*60}")
    print(f"  å°çº¢ä¹¦çˆ¬è™«æµ‹è¯•")
    print(f"{'='*60}")
    print(f"\nğŸ“Š é…ç½®:")
    print(f"  â€¢ åˆ·æ–°æ¬¡æ•°: {args.refresh_count}")
    print(f"  â€¢ æœ€å¤šç¬”è®°: {args.max_notes}")

    # è¯»å–Cookie
    cookie = read_xhs_cookie()
    if not cookie:
        print("\nâš ï¸  æœªæ‰¾åˆ°Cookieï¼Œå°†ä½¿ç”¨æ— Cookieæ¨¡å¼ï¼ˆéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰")
    else:
        print("\nâœ… å·²è¯»å–Cookie")

    # é‡‡é›†æµ‹è¯•æ•°æ®
    notes = await test_scrape(
        refresh_count=args.refresh_count,
        max_notes=args.max_notes,
        cookie=cookie
    )

    if not notes:
        print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•ç¬”è®°")
        return

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = generate_test_report(notes)

    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = OUTPUT_DIR / f"test_report_{timestamp}.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\nğŸ“ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # å¯¼å‡ºCSV
    csv_path = OUTPUT_DIR / f"test_result_{timestamp}.csv"
    export_to_csv(notes, csv_path)

    # æ‰“å°æŠ¥å‘Šæ‘˜è¦
    print("\n" + "=" * 60)
    print("  ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)

    with_token = sum(1 for n in notes if n.get('xsecToken'))
    token_rate = (with_token / len(notes) * 100) if notes else 0

    print(f"  æ€»é‡‡é›†æ•°: {len(notes)}")
    print(f"  xsec_token è¦†ç›–ç‡: {token_rate:.1f}% ({with_token}/{len(notes)})")

    video_count = sum(1 for n in notes if n.get('type') == 'video')
    print(f"  è§†é¢‘: {video_count} | å›¾æ–‡: {len(notes) - video_count}")

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    print(f"   æŠ¥å‘Š: {report_path}")
    print(f"   CSV: {csv_path}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
