#!/usr/bin/env python3
"""
AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èå¹¶æ€»ç»“

ä¸€é”®å®Œæˆï¼š
1. åˆ·æ–°å°çº¢ä¹¦æ¨èé¡µï¼ˆè‡ªå®šä¹‰æ¬¡æ•°ï¼‰
2. é‡‡é›†æ¨èå†…å®¹ï¼ˆè§†é¢‘/å›¾æ–‡ã€ä½œè€…ä¿¡æ¯ï¼‰
3. å¯¼å‡ºCSV
4. AIç”Ÿæˆåˆ†ææŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹:
    # é»˜è®¤é…ç½®ï¼ˆåˆ·æ–°3æ¬¡ï¼Œæœ€å¤š50ä¸ªç¬”è®°ï¼‰
    python ai_xiaohongshu_homepage.py

    # ä»…é‡‡é›†ï¼Œç”ŸæˆCSV
    python ai_xiaohongshu_homepage.py --mode scrape

    # é‡‡é›†+AIåˆ†æ
    python ai_xiaohongshu_homepage.py --mode full

    # è‡ªå®šä¹‰åˆ·æ–°æ¬¡æ•°å’Œç¬”è®°æ•°
    python ai_xiaohongshu_homepage.py --refresh-count 5 --max-notes 100 --mode full
"""

import argparse
import asyncio
import sys
import csv
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
import re

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from playwright.async_api import async_playwright
import httpx
from bs4 import BeautifulSoup
import time

# å»¶è¿Ÿå¯¼å…¥ Gemini API
_gemini_available = False
try:
    from google import genai
    _gemini_available = True
except ImportError:
    try:
        import google.generativeai as genai
        _gemini_available = True
    except ImportError:
        pass


# ==================== è·¯å¾„é…ç½® ====================
PROJECT_DIR = Path(__file__).parent
OUTPUT_DIR = PROJECT_DIR / "output" / "xiaohongshu_homepage"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==================== Cookie è¯»å– ====================
def read_xhs_cookie():
    """ä» config/cookies.txt è¯»å–å°çº¢ä¹¦Cookie"""
    cookie_file = PROJECT_DIR / "config" / "cookies.txt"
    if not cookie_file.exists():
        print("âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: config/cookies.txt")
        print("ğŸ’¡ è¯·å…ˆç™»å½•å°çº¢ä¹¦å¹¶å¯¼å‡ºCookie")
        return ""

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # å°è¯•å¤šç§æ ¼å¼
    cookie = ""

    # æ–¹æ³•1: æŸ¥æ‰¾ xiaohongshu_full= æ ¼å¼
    match = re.search(r'xiaohongshu_full=([^\n]+)', content)
    if match:
        cookie = match.group(1)
        print("âœ… ä½¿ç”¨ xiaohongshu_full Cookie")
        return cookie

    # æ–¹æ³•2: æŸ¥æ‰¾ [xiaohongshu] éƒ¨åˆ†
    xhs_section = re.search(r'\[xiaohongshu\](.*?)\[', content, re.DOTALL)
    if xhs_section:
        section = xhs_section.group(1)
        # æå–æ‰€æœ‰ key=value å¯¹
        cookies = []
        for line in section.split('\n'):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                cookies.append(f"{key.strip()}={value.strip()}")
        cookie = '; '.join(cookies)
        if cookie:
            print("âœ… ä½¿ç”¨ [xiaohongshu] éƒ¨åˆ† Cookie")
            return cookie

    # æ–¹æ³•3: ç›´æ¥æŸ¥æ‰¾å…³é”®Cookie
    a1_match = re.search(r'a1=([^\s\n;]+)', content)
    web_session_match = re.search(r'web_session=([^\s\n;]+)', content)
    webid_match = re.search(r'webId=([^\s\n;]+)', content)

    if a1_match and web_session_match and webid_match:
        cookie = f"a1={a1_match.group(1)}; web_session={web_session_match.group(1)}; webId={webid_match.group(1)}"
        print("âœ… æ‰‹åŠ¨æå–å…³é”®Cookie")
        return cookie

    print("âš ï¸  Cookieæ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å°çº¢ä¹¦Cookie")
    return ""


# ==================== Playwright é‡‡é›† ====================
async def scrape_xiaohongshu_homepage(
    refresh_count: int = 3,
    max_notes: int = 50,
    cookie: str = ""
) -> List[Dict]:
    """
    ä½¿ç”¨Playwrightçˆ¬å–å°çº¢ä¹¦æ¨èé¡µ

    Args:
        refresh_count: åˆ·æ–°æ¬¡æ•°
        max_notes: æœ€å¤šé‡‡é›†ç¬”è®°æ•°
        cookie: å°çº¢ä¹¦Cookie

    Returns:
        ç¬”è®°åˆ—è¡¨
    """
    notes_collected = []
    seen_urls = set()  # å»é‡

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(headless=False)  # éæ— å¤´æ¨¡å¼ï¼Œå¯ä»¥çœ‹åˆ°ç™»å½•
        context = await browser.new_context()

        # è®¾ç½®Cookie
        if cookie:
            # è§£æCookieå­—ç¬¦ä¸²å¹¶æ·»åŠ åˆ°context
            cookies = []
            for item in cookie.split(';'):
                item = item.strip()
                if '=' in item:
                    key, value = item.split('=', 1)
                    cookies.append({
                        'name': key,
                        'value': value,
                        'domain': '.xiaohongshu.com',
                        'path': '/'
                    })
            await context.add_cookies(cookies)
            print("âœ… Cookieå·²è®¾ç½®")

        page = await context.new_page()

        print(f"\nğŸ“¡ è®¿é—®å°çº¢ä¹¦é¦–é¡µ...")
        await page.goto('https://www.xiaohongshu.com/', wait_until='networkidle')
        await asyncio.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        try:
            login_btn = await page.query_selector('button:has-text("ç™»å½•")')
            if login_btn:
                print("\nâš ï¸  æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€")
                print("ğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•")
                print("â³ ç­‰å¾…30ç§’...")
                await asyncio.sleep(30)
        except:
            pass

        print(f"\nğŸ”„ å¼€å§‹é‡‡é›†æ¨èå†…å®¹ï¼ˆåˆ·æ–°{refresh_count}æ¬¡ï¼‰...")

        for i in range(refresh_count):
            print(f"\n  åˆ·æ–° {i+1}/{refresh_count}")

            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
            for scroll in range(5):
                await page.evaluate('window.scrollBy(0, window.innerHeight)')
                await asyncio.sleep(1)

            # è·å–é¡µé¢å†…å®¹
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')

            # æŸ¥æ‰¾ç¬”è®°å…ƒç´ 
            # å°çº¢ä¹¦çš„ç¬”è®°é€šå¸¸åœ¨ç‰¹å®šçš„divä¸­
            note_items = soup.find_all('section')  # å°çº¢ä¹¦å¸¸ç”¨çš„æ ‡ç­¾

            for item in note_items:
                try:
                    # æå–ç¬”è®°IDå’Œé“¾æ¥
                    link_elem = item.find('a', href=re.compile(r'/explore/'))
                    if not link_elem:
                        continue

                    note_url = link_elem.get('href', '')
                    if not note_url:
                        continue

                    # è¡¥å…¨URL
                    if note_url.startswith('//'):
                        note_url = 'https:' + note_url
                    elif note_url.startswith('/'):
                        note_url = 'https://www.xiaohongshu.com' + note_url

                    # æå–ç¬”è®°ID
                    note_id_match = re.search(r'/explore/([a-f0-9]+)', note_url)
                    if not note_id_match:
                        continue
                    note_id = note_id_match.group(1)

                    # å»é‡
                    if note_id in seen_urls:
                        continue
                    seen_urls.add(note_id)

                    # æå–æ ‡é¢˜/æè¿°
                    title_elem = link_elem.find('span', class_=re.compile(r'title'))
                    title = title_elem.get_text(strip=True) if title_elem else "æ— æ ‡é¢˜"

                    # æå–ä½œè€…ä¿¡æ¯
                    author_elem = item.find('span', class_=re.compile(r'user.*name|nickname'))
                    author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"

                    # æå–ç‚¹èµæ•°
                    like_elem = item.find('span', class_=re.compile(r'like|count'))
                    likes = like_elem.get_text(strip=True) if like_elem else "0"

                    # åˆ¤æ–­ç±»å‹ï¼ˆè§†é¢‘/å›¾æ–‡ï¼‰
                    video_elem = item.find('video')
                    note_type = 'video' if video_elem else 'image'

                    note_data = {
                        'åºå·': len(notes_collected) + 1,
                        'æ ‡é¢˜': title,
                        'é“¾æ¥': note_url,
                        'ç¬”è®°ID': note_id,
                        'ä½œè€…': author,
                        'ç‚¹èµæ•°': likes,
                        'ç±»å‹': note_type,
                        'é‡‡é›†æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    notes_collected.append(note_data)
                    print(f"    âœ“ [{len(notes_collected)}] {note_type} - {title[:30]}...")

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™
                    if len(notes_collected) >= max_notes:
                        break

                except Exception as e:
                    # å•ä¸ªç¬”è®°è§£æå¤±è´¥ä¸å½±å“æ•´ä½“
                    continue

            # åˆ·æ–°é¡µé¢
            if i < refresh_count - 1:
                await page.reload(wait_until='networkidle')
                await asyncio.sleep(2)

        await browser.close()

    print(f"\nâœ… é‡‡é›†å®Œæˆï¼å…±è·å– {len(notes_collected)} ä¸ªç¬”è®°")
    return notes_collected


# ==================== CSVå¯¼å‡º ====================
def export_to_csv(notes: List[Dict], output_path: Path):
    """å¯¼å‡ºç¬”è®°åˆ°CSV"""

    csv_columns = [
        'åºå·',
        'æ ‡é¢˜',
        'é“¾æ¥',
        'ç¬”è®°ID',
        'ä½œè€…',
        'ç‚¹èµæ•°',
        'ç±»å‹',
        'é‡‡é›†æ—¶é—´'
    ]

    with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=csv_columns)
        writer.writeheader()
        writer.writerows(notes)

    print(f"ğŸ“ CSVå·²ä¿å­˜: {output_path}")


# ==================== AIåˆ†æ ====================
def generate_ai_summary(notes: List[Dict], model: str = 'flash-lite') -> Optional[str]:
    """ä½¿ç”¨Geminiç”ŸæˆAIæ‘˜è¦"""

    if not _gemini_available:
        print("âš ï¸  Gemini APIæœªå®‰è£…ï¼Œè·³è¿‡AIåˆ†æ")
        return None

    # é…ç½®API
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        print("âš ï¸  æœªè®¾ç½®GEMINI_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡AIåˆ†æ")
        return None

    genai.configure(api_key=api_key)
    model_obj = genai.GenerativeModel(model)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    notes_text = "\n\n".join([
        f"{i+1}. {note['æ ‡é¢˜']}\n"
        f"   ä½œè€…: {note['ä½œè€…']}\n"
        f"   ç±»å‹: {note['ç±»å‹']}\n"
        f"   ç‚¹èµ: {note['ç‚¹èµæ•°']}\n"
        f"   é“¾æ¥: {note['é“¾æ¥']}"
        for i, note in enumerate(notes[:20])  # æœ€å¤šåˆ†æ20ä¸ª
    ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦æ¨èå†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¶‹åŠ¿æŠ¥å‘Šã€‚

å°çº¢ä¹¦æ¨èå†…å®¹ï¼š
{notes_text}

è¯·ç”Ÿæˆä»¥ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š

## ğŸ“Š å°çº¢ä¹¦æ¨èè¶‹åŠ¿åˆ†æ

### ğŸ¯ å†…å®¹æ¦‚è§ˆ
- é‡‡é›†ç¬”è®°æ•°ï¼š{len(notes)}ç¯‡
- è§†é¢‘å æ¯”ï¼šXX%
- å›¾æ–‡å æ¯”ï¼šXX%
- å¹³å‡ç‚¹èµæ•°ï¼šXX

### ğŸ”¥ çƒ­é—¨ä¸»é¢˜ï¼ˆTop 5ï¼‰
æå–æœ€å—æ¬¢è¿çš„5ä¸ªä¸»é¢˜/è¯é¢˜

### ğŸ‘¥ çƒ­é—¨ä½œè€…ï¼ˆTop 5ï¼‰
åˆ—ä¸¾å‘å¸ƒæœ€å¤šå†…å®¹çš„5ä¸ªä½œè€…

### ğŸ“ˆ è¶‹åŠ¿åˆ†æ
åˆ†æå½“å‰å°çº¢ä¹¦æ¨èçš„å†…å®¹è¶‹åŠ¿ï¼ŒåŒ…æ‹¬ï¼š
- çƒ­é—¨è¯é¢˜
- å†…å®¹åå¥½
- å—æ¬¢è¿çš„å†…å®¹ç±»å‹

### ğŸ’ å€¼å¾—å…³æ³¨çš„ç¬”è®°
æ¨è3-5ä¸ªå€¼å¾—æ·±å…¥é˜…è¯»çš„ç¬”è®°ï¼ˆé™„é“¾æ¥ï¼‰

### ğŸ“ å†…å®¹è´¨é‡è¯„ä¼°
- å†…å®¹å¤šæ ·æ€§è¯„åˆ†ï¼š1-5æ˜Ÿ
- äº’åŠ¨çƒ­åº¦ï¼šé«˜/ä¸­/ä½
- æ¨èåº¦ï¼š1-5æ˜Ÿ

è¯·ç¡®ä¿æŠ¥å‘Šç»“æ„å®Œæ•´ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å®è´¨å†…å®¹ã€‚"""

    try:
        print("\nğŸ¤– æ­£åœ¨ä½¿ç”¨AIåˆ†æ...")
        response = model_obj.generate_content(prompt)

        if response.text:
            print("âœ… AIåˆ†æå®Œæˆï¼")
            return response.text
        else:
            print("âŒ AIåˆ†æå¤±è´¥ï¼šæ— å“åº”")
            return None

    except Exception as e:
        print(f"âŒ AIåˆ†æå‡ºé”™: {e}")
        return None


# ==================== ä¸»ç¨‹åº ====================
async def main():
    parser = argparse.ArgumentParser(description='AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èå¹¶æ€»ç»“')

    parser.add_argument('--refresh-count', type=int, default=3,
                       help='åˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('--max-notes', type=int, default=50,
                       help='æœ€å¤šé‡‡é›†ç¬”è®°æ•°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--mode', type=str, default='full',
                       choices=['scrape', 'full'],
                       help='æ¨¡å¼: scrape=ä»…é‡‡é›†, full=é‡‡é›†+AIåˆ†æ')
    parser.add_argument('--model', type=str, default='flash-lite',
                       help='Geminiæ¨¡å‹ï¼ˆé»˜è®¤: flash-liteï¼‰')

    args = parser.parse_args()

    print(f"\n{'='*70}")
    print(f"  AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨è")
    print(f"{'='*70}")
    print(f"\nğŸ“Š é…ç½®:")
    print(f"  â€¢ åˆ·æ–°æ¬¡æ•°: {args.refresh_count}")
    print(f"  â€¢ æœ€å¤šç¬”è®°: {args.max_notes}")
    print(f"  â€¢ åˆ†ææ¨¡å¼: {args.mode}")

    # è¯»å–Cookie
    cookie = read_xhs_cookie()
    if not cookie:
        print("\nâŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å°çº¢ä¹¦Cookie")
        print("ğŸ’¡ è¯·å…ˆåœ¨æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦ï¼Œç„¶åå¯¼å‡ºCookieåˆ° config/cookies.txt")
        return

    # é‡‡é›†æ•°æ®
    notes = await scrape_xiaohongshu_homepage(
        refresh_count=args.refresh_count,
        max_notes=args.max_notes,
        cookie=cookie
    )

    if not notes:
        print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•ç¬”è®°")
        return

    # å¯¼å‡ºCSV
    date_str = datetime.now().strftime('%Y-%m-%d')
    csv_path = OUTPUT_DIR / f"xiaohongshu_homepage_{date_str}.csv"
    export_to_csv(notes, csv_path)

    # AIåˆ†æ
    if args.mode == 'full':
        summary = generate_ai_summary(notes, args.model)
        if summary:
            # ä¿å­˜æ‘˜è¦
            summary_path = OUTPUT_DIR / f"xiaohongshu_homepage_{date_str}_AIæ€»ç»“.md"
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary)
            print(f"ğŸ“ AIæ€»ç»“å·²ä¿å­˜: {summary_path}")

    print(f"\n{'='*70}")
    print(f"  âœ… å®Œæˆï¼")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    asyncio.run(main())
