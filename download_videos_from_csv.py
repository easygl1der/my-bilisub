#!/usr/bin/env python3
"""
ä»CSVæ–‡ä»¶æ‰¹é‡ä¸‹è½½è§†é¢‘

åŠŸèƒ½ï¼š
1. è¯»å– xhs_videos_output ç›®å½•ä¸‹çš„CSVæ–‡ä»¶
2. è‡ªåŠ¨è¯†åˆ«å°çº¢ä¹¦/Bç«™é“¾æ¥
3. ä¸‹è½½è§†é¢‘åˆ°ä»¥ä½œè€…å‘½åçš„æ–‡ä»¶å¤¹
4. æ–‡ä»¶åä½¿ç”¨è§†é¢‘æ ‡é¢˜
5. æ˜¾ç¤ºä¸‹è½½è¿›åº¦å’Œè€—æ—¶

ä½¿ç”¨ç¤ºä¾‹:
    # ä¸‹è½½å•ä¸ªCSVæ–‡ä»¶
    python download_videos_from_csv.py -csv "MediaCrawler/xhs_videos_output/æ¨é›¨å¤-Yukun.csv"

    # ä¸‹è½½ç›®å½•ä¸‹æ‰€æœ‰CSV
    python download_videos_from_csv.py -dir "MediaCrawler/xhs_videos_output"

    # åªä¸‹è½½æŒ‡å®šç±»å‹çš„è§†é¢‘
    python download_videos_from_csv.py -csv "xxx.csv" --type video
"""

import os
import sys
import csv
import re
import time
import threading
import argparse
import shutil
from pathlib import Path
from datetime import datetime

import yt_dlp
import requests
from bs4 import BeautifulSoup

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# ==================== Bç«™ Cookie é…ç½® ====================
# ä» fetch_bilibili_videos.py å¤åˆ¶çš„ B ç«™ Cookie
BILI_COOKIE = "buvid3=ED836AB2-1A1F-83B3-C368-EC717E8514CC52442infoc; b_nut=1768880952; lang=zh-Hans; theme-tip-show=SHOWED; buvid4=E6C199FE-5C98-198C-D77F-9B183C96AC6657438-026012011-zxmN2%2Bh1P%2F0eoan1hmmTzg%3D%3D; buvid_fp=bdde8cc73192655bb657c6b1b634831a; rpdid=|(Jl|J~JlJu)0J'u~Y)))u|Rl; theme-avatar-tip-show=SHOWED; DedeUserID=352314171; DedeUserID__ckMd5=8753aa0a6f5400e0; CURRENT_QUALITY=80; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NzEzNTA4OTgsImlhdCI6MTc3MTA5MTYzOCwicGx0IjotMX0.7NGUxpL_Kpz6MIafuGccDUrwQ0MYWTJIdZbcWzRFbK0; bili_ticket_expires=1771350838; SESSDATA=340e7534%2C1786643702%2C8ff5f%2A22CjBmNdSHwh1cJexOwoyFWM5LODSzCLixmDSo8umHTW2VrYyVmwwZMAH0xptDSCSuoaoSVnJ1UF9Lc0pockFlLTlKMEYteUdfNFhSbUxYTDlZak1sMHd1MHlpRTJKUzg3WGpYbVpNbEFNNlZyczJuMUZObW5mOVgtWjJQZnJ0TFhHY1NnbnA1c1lRIIEC; bili_jct=00bda0ae20a58226c7ab7c0198f889e8; bmg_af_switch=1; bmg_src_def_domain=i2.hdslb.com; sid=8khlk9a0; bp_t_offset_352314171=1169997504301760512; CURRENT_FNVAL=2000; home_feed_column=4; brows"

# ==================== YouTube Cookie é…ç½® ====================
# YouTube ä¸‹è½½å¯èƒ½éœ€è¦ cookies.txt æ–‡ä»¶æ¥é¿å… 403 é”™è¯¯
# å¯ä»¥ä½¿ç”¨æµè§ˆå™¨æ‰©å±• "Get cookies.txt LOCALLY" å¯¼å‡º YouTube cookies
# ä¿å­˜ä¸º cookies_youtube.txt æ”¾åœ¨å½“å‰ç›®å½•ä¸‹
YOUTUBE_COOKIE_FILE = "cookies_youtube.txt"
YOUTUBE_COOKIE_FILE_ALT = "youtube_cookies.txt"  # å¤‡ç”¨æ–‡ä»¶å

# Chrome cookies è·¯å¾„ï¼ˆWindowsï¼‰
CHROME_COOKIE_PATHS = [
    os.path.expandvars(r"%LOCALAPPDATA%\Google\Chrome\User Data\Default\Cookies"),
    os.path.expandvars(r"%LOCALAPPDATA%\Google\Chrome\User Data\Default\Network\Cookies"),
]

# Edge cookies è·¯å¾„ï¼ˆWindowsï¼‰
EDGE_COOKIE_PATHS = [
    os.path.expandvars(r"%LOCALAPPDATA%\Microsoft\Edge\User Data\Default\Cookies"),
    os.path.expandvars(r"%LOCALAPPDATA%\Microsoft\Edge\User Data\Default\Network\Cookies"),
]


def get_browser_cookies_youtube() -> str:
    """
    å°è¯•ä»æµè§ˆå™¨è·å– YouTube çš„ cookies

    Returns:
        cookies å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        import sqlite3
        import tempfile
        from shutil import copy2

        # æŸ¥æ‰¾æµè§ˆå™¨ cookie æ–‡ä»¶
        cookie_paths = EDGE_COOKIE_PATHS + CHROME_COOKIE_PATHS
        cookie_file = None

        for path in cookie_paths:
            if os.path.exists(path):
                cookie_file = path
                break

        if not cookie_file:
            return None

        # å¤åˆ¶ cookie æ–‡ä»¶ï¼ˆå› ä¸ºæµè§ˆå™¨å¯èƒ½æ­£åœ¨ä½¿ç”¨ï¼‰
        with tempfile.NamedTemporaryFile(delete=False, suffix='.db') as tmp:
            tmp_path = tmp.name

        try:
            copy2(cookie_file, tmp_path)
        except Exception:
            return None

        # è¯»å– cookies
        conn = sqlite3.connect(tmp_path)
        cursor = conn.cursor()

        # æŸ¥è¯¢ YouTube cookies
        cursor.execute("""
            SELECT name, value
            FROM cookies
            WHERE host_key LIKE '%.youtube.com'
            OR host_key = '.youtube.com'
        """)

        cookies = {}
        for name, value in cursor.fetchall():
            if name in ['SID', 'HSID', 'SSID', 'APISID', 'SAPISID', 'LOGIN_INFO', 'PREF', 'VISITOR_INFO1_LIVE']:
                cookies[name] = value

        conn.close()
        os.unlink(tmp_path)

        # æ£€æŸ¥å…³é”® cookie æ˜¯å¦å­˜åœ¨
        if 'SID' not in cookies or 'HSID' not in cookies:
            return None

        # è½¬æ¢ä¸º cookie å­—ç¬¦ä¸²
        cookie_str = '; '.join([f"{name}={value}" for name, value in cookies.items()])
        return cookie_str

    except Exception as e:
        return None


# ==================== è¿›åº¦æ¡ ====================

class ProgressHook:
    """yt-dlpä¸‹è½½è¿›åº¦é’©å­"""

    def __init__(self):
        self.downloaded_bytes = 0
        self.total_bytes = 0
        self.speed = 0
        self.eta = 0
        self.filename = ""
        self.status = ""
        self._last_update = 0

    def __call__(self, d):
        if d['status'] == 'downloading':
            self.downloaded_bytes = d.get('downloaded_bytes', 0)
            self.total_bytes = d.get('total_bytes', 0) or d.get('total_bytes_estimate', 0)
            self.speed = d.get('speed', 0)
            self.eta = d.get('eta', 0)
            self.filename = d.get('filename', '')
            self.status = 'downloading'

            # é™åˆ¶æ›´æ–°é¢‘ç‡ï¼ˆæ¯0.5ç§’ï¼‰
            now = time.time()
            if now - self._last_update > 0.5:
                self._print_progress()
                self._last_update = now

        elif d['status'] == 'finished':
            self.status = 'finished'
            print(f"\r   â””â”€ ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨å¤„ç†...{' ' * 40}")

    def _print_progress(self):
        if self.total_bytes and self.total_bytes > 0:
            percent = self.downloaded_bytes / self.total_bytes * 100
            bar_length = 25
            filled = int(bar_length * percent / 100)
            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

            # é€Ÿåº¦æ˜¾ç¤º
            speed = self.speed or 0
            if speed > 0:
                if speed >= 1024 * 1024:
                    speed_str = f"{speed / 1024 / 1024:.1f}MB/s"
                elif speed >= 1024:
                    speed_str = f"{speed / 1024:.1f}KB/s"
                else:
                    speed_str = f"{speed:.0f}B/s"
            else:
                speed_str = "--"

            # ETAæ˜¾ç¤º
            eta = self.eta if self.eta and self.eta > 0 else 0
            if eta > 0:
                eta_str = f"{eta:.0f}s"
            else:
                eta_str = "--"

            # å·²ä¸‹è½½å¤§å°
            downloaded_mb = self.downloaded_bytes / 1024 / 1024
            total_mb = self.total_bytes / 1024 / 1024

            print(f"\r   â””â”€ [{bar}] {percent:5.1f}% | {downloaded_mb:.1f}/{total_mb:.1f}MB | {speed_str} | ETA: {eta_str}{' ' * 10}", end='', flush=True)


# ==================== å·¥å…·å‡½æ•° ====================

def sanitize_filename(name: str, max_length: int = 200) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    illegal_chars = r'[<>:"/\\|?*]'
    name = re.sub(illegal_chars, '_', name)

    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    name = ''.join(char for char in name if ord(char) >= 32)

    # å»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    name = name.strip('. ')

    # é™åˆ¶é•¿åº¦
    if len(name) > max_length:
        name = name[:max_length].rsplit(' ', 1)[0]  # å°è¯•åœ¨ç©ºæ ¼å¤„æˆªæ–­

    return name or "untitled"


def detect_platform(url: str) -> str:
    """æ£€æµ‹è§†é¢‘å¹³å°"""
    if 'bilibili.com' in url or 'b23.tv' in url:
        return 'bilibili'
    elif 'xiaohongshu.com' in url or 'xhslink.com' in url:
        return 'xiaohongshu'
    elif 'youtube.com' in url or 'youtu.be' in url:
        return 'youtube'
    else:
        return 'unknown'


def detect_bilibili_type(url: str) -> str:
    """
    æ£€æµ‹Bç«™é“¾æ¥çš„ç±»å‹

    Args:
        url: Bç«™é“¾æ¥

    Returns:
        'video' (è§†é¢‘) æˆ– 'normal' (å›¾æ–‡/ä¸“æ )
    """
    url_lower = url.lower()

    # b23.tv çŸ­é“¾æ¥é»˜è®¤ä¸ºè§†é¢‘
    if 'b23.tv' in url_lower:
        return 'video'

    # Bç«™å›¾æ–‡/ä¸“æ URLç‰¹å¾ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼‰
    article_patterns = [
        '/read/',       # ä¸“æ  https://www.bilibili.com/read/...
        '/opus/',       # åŠ¨æ€æŠ•ç¨¿ https://www.bilibili.com/opus/...
        'article',      # åŒ…å«articleå…³é”®å­—
    ]

    # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾æ–‡/ä¸“æ 
    for pattern in article_patterns:
        if pattern in url_lower:
            return 'normal'

    # Bç«™è§†é¢‘URLç‰¹å¾
    video_patterns = [
        '/video/',      # æ™®é€šè§†é¢‘ https://www.bilibili.com/video/BV...
        '/av',          # avå· https://www.bilibili.com/av...
        'bilibili.com/bvid',  # BVå·
        '/bangumi/',    # ç•ªå‰§
        '/medialist/',  # æ’­æ”¾åˆ—è¡¨
    ]

    # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘
    for pattern in video_patterns:
        if pattern in url_lower:
            return 'video'

    # å¦‚æœURLåŒ…å« bilibili.com ä½†ä¸åŒ¹é…ä¸Šè¿°æ¨¡å¼ï¼Œé»˜è®¤ä¸ºè§†é¢‘
    if 'bilibili.com' in url_lower:
        return 'video'

    # æœªçŸ¥æƒ…å†µé»˜è®¤ä¸ºè§†é¢‘
    return 'video'


def get_author_name_from_csv(csv_path: str) -> str:
    """ä»CSVæ–‡ä»¶åæå–ä½œè€…å"""
    filename = Path(csv_path).stem
    # ç§»é™¤å¯èƒ½çš„æ—¶é—´æˆ³åç¼€
    if re.search(r'_\d{8}', filename):
        filename = re.sub(r'_\d{8}.*', '', filename)
    return filename


def parse_csv(csv_path: str, filter_type: str = None) -> list:
    """
    è§£æCSVæ–‡ä»¶ï¼Œæå–è§†é¢‘ä¿¡æ¯

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        filter_type: ç­›é€‰ç±»å‹ (video/normal)ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨

    Returns:
        list: è§†é¢‘ä¿¡æ¯åˆ—è¡¨ [{index, title, url, type, ...}, ...]
    """
    videos = []

    print(f"\nğŸ“– è¯»å–CSVæ–‡ä»¶: {Path(csv_path).name}")

    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            # æ£€æŸ¥åˆ—å
            if not reader.fieldnames:
                print("âŒ CSVæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return videos

            for row in reader:
                url = row.get('é“¾æ¥', '') or row.get('url', '')
                title = row.get('æ ‡é¢˜', '') or row.get('title', '')
                note_type = row.get('ç±»å‹', '') or row.get('type', '')
                index = row.get('åºå·', '') or row.get('index', '')

                # è·³è¿‡ç©ºURL
                if not url or not url.startswith('http'):
                    continue

                # å¦‚æœç±»å‹ä¸ºç©ºï¼Œè‡ªåŠ¨æ£€æµ‹
                if not note_type:
                    platform = detect_platform(url)
                    if platform == 'bilibili':
                        note_type = detect_bilibili_type(url)
                    elif platform == 'xiaohongshu':
                        # å°çº¢ä¹¦æš‚æ—¶é»˜è®¤ä¸ºè§†é¢‘ï¼Œä¸‹è½½æ—¶å†åˆ¤æ–­
                        note_type = 'video'
                    else:
                        note_type = 'video'  # æœªçŸ¥å¹³å°é»˜è®¤ä¸ºè§†é¢‘

                # ç±»å‹ç­›é€‰
                if filter_type and filter_type.lower() != note_type.lower():
                    continue

                videos.append({
                    'index': index,
                    'title': title or f"è§†é¢‘_{index}",
                    'url': url,
                    'type': note_type,
                    'row': row
                })

        print(f"âœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘é“¾æ¥")

    except Exception as e:
        print(f"âŒ è¯»å–CSVå¤±è´¥: {e}")

    return videos


def show_video_list(videos: list, show_count: int = 10):
    """
    æ˜¾ç¤ºå¾…ä¸‹è½½è§†é¢‘åˆ—è¡¨

    Args:
        videos: è§†é¢‘åˆ—è¡¨
        show_count: æ¯é¡µæ˜¾ç¤ºæ•°é‡
    """
    print("\n" + "=" * 80)
    print("ğŸ“‹ å¾…ä¸‹è½½è§†é¢‘åˆ—è¡¨")
    print("=" * 80)

    # ç»Ÿè®¡ç±»å‹
    video_count = sum(1 for v in videos if v.get('type', '').lower() == 'video')
    normal_count = sum(1 for v in videos if v.get('type', '').lower() == 'normal')
    unknown_count = len(videos) - video_count - normal_count

    for i, video in enumerate(videos, 1):
        platform = detect_platform(video['url'])
        platform_icon = {'xiaohongshu': 'ğŸ“•', 'bilibili': 'ğŸ“º', 'youtube': 'â–¶ï¸', 'unknown': 'ğŸ“„'}.get(platform, 'ğŸ“„')

        # ç±»å‹å›¾æ ‡
        note_type = video.get('type', '').lower()
        if note_type == 'normal':
            type_icon = 'ğŸ“·'
            type_text = 'å›¾æ–‡'
        elif note_type == 'video':
            type_icon = 'ğŸ¬'
            type_text = 'è§†é¢‘'
        else:
            type_icon = 'â“'
            type_text = video['type'] or 'æœªçŸ¥'

        # åºå· | å¹³å° | ç±»å‹ | æ ‡é¢˜
        print(f"{i:3}. [{platform_icon}] [{type_icon}] {video['title'][:45]}... | {type_text}")

    print("=" * 80)
    print(f"æ€»è®¡: {len(videos)} | ğŸ¬è§†é¢‘: {video_count} | ğŸ“·å›¾æ–‡: {normal_count} | â“æœªçŸ¥: {unknown_count}")
    print("=" * 80)


# ==================== ä¸‹è½½æ ¸å¿ƒ ====================

def download_images_from_note(url: str, title: str, output_dir: Path, platform: str) -> dict:
    """
    ä¸‹è½½å°çº¢ä¹¦/Bç«™å›¾æ–‡å†…å®¹ä¸­çš„å›¾ç‰‡

    Args:
        url: å›¾æ–‡é“¾æ¥
        title: æ ‡é¢˜
        output_dir: è¾“å‡ºç›®å½•
        platform: å¹³å°åç§°

    Returns:
        dict: ä¸‹è½½ç»“æœ {success, files, count, error}
    """
    import json
    import re

    result = {
        'success': False,
        'files': [],
        'count': 0,
        'error': None
    }

    safe_title = sanitize_filename(title)
    note_dir = output_dir / safe_title
    note_dir.mkdir(parents=True, exist_ok=True)

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.xiaohongshu.com/' if platform == 'xiaohongshu' else 'https://www.bilibili.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }

    unique_images = []

    try:
        # ============ æ–¹æ³•1: å°çº¢ä¹¦ä¸“ç”¨ - ç›´æ¥è§£æé¡µé¢è·å–å›¾ç‰‡ ============
        if platform == 'xiaohongshu':
            print(f"   â””â”€ ğŸ“¡ è§£æå°çº¢ä¹¦é¡µé¢è·å–å›¾ç‰‡...")

            try:
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()

                html = response.text

                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°404
                if '/404?' in response.url or 'ä½ è®¿é—®çš„é¡µé¢ä¸è§äº†' in html:
                    print(f"   â””â”€ âš ï¸  é¡µé¢æ— æ³•è®¿é—®ï¼ˆåçˆ¬è™«ä¿æŠ¤æˆ–é“¾æ¥å¤±æ•ˆï¼‰")
                    # ç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•

                # æ”¹è¿›ç‰ˆï¼šä½¿ç”¨æ‹¬å·åŒ¹é…æ³•å®Œæ•´æå– imageList
                start_idx = html.find('window.__INITIAL_STATE__=')
                if start_idx >= 0:
                    start_idx += len('window.__INITIAL_STATE__=')
                    end_idx = html.find('</script>', start_idx)
                    json_str = html[start_idx:end_idx]

                    # æŸ¥æ‰¾ imageList æ•°ç»„ - ä½¿ç”¨è®¡æ•°å™¨åŒ¹é…å®Œæ•´çš„æ•°ç»„
                    list_start = json_str.find('"imageList"')
                    if list_start >= 0:
                        bracket_start = json_str.find('[', list_start)
                        if bracket_start >= 0:
                            # æ‰‹åŠ¨åŒ¹é…å¯¹åº”çš„ ]
                            depth = 0
                            i = bracket_start
                            while i < len(json_str):
                                if json_str[i] == '[':
                                    depth += 1
                                elif json_str[i] == ']':
                                    depth -= 1
                                    if depth == 0:
                                        bracket_end = i
                                        break
                                i += 1

                            list_content = json_str[bracket_start+1:bracket_end]

                            # åªæå– urlDefaultï¼ˆé»˜è®¤/åŸå›¾ï¼‰ï¼Œè·³è¿‡å…¶ä»–å˜ä½“
                            url_pattern = r'"urlDefault":"([^"]+)"'
                            for match in re.finditer(url_pattern, list_content):
                                img_url = match.group(1)
                                if img_url:
                                    # è§£ç  Unicode è½¬ä¹‰
                                    try:
                                        img_url = img_url.encode('utf-8').decode('unicode_escape')
                                    except:
                                        pass
                                    # ç¡®ä¿ https åè®®
                                    if img_url.startswith('http://'):
                                        img_url = 'https://' + img_url[7:]
                                    if 'xhscdn' in img_url:
                                        unique_images.append(img_url)

                # å¤‡ç”¨ï¼šå¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯• JSON è§£æ
                if not unique_images:
                    initial_state_pattern = r'window\.__INITIAL_STATE__\s*=\s*({.+?});'
                    match = re.search(initial_state_pattern, html)
                    if match:
                        try:
                            initial_state = json.loads(match.group(1))
                            image_list = initial_state.get('note', {}).get('noteDetail', {}).get('imageList', [])
                            if isinstance(image_list, list):
                                for img_obj in image_list:
                                    if isinstance(img_obj, dict):
                                        img_url = img_obj.get('urlDefault') or img_obj.get('url')
                                        if img_url:
                                            unique_images.append(img_url)
                        except json.JSONDecodeError:
                            pass

                # å¤‡ç”¨2: ä»æ•´ä¸ª HTML ä¸­æœç´¢ sns-webpic å›¾ç‰‡URL
                if not unique_images:
                    all_urls = re.findall(r'(https://sns-webpic[^\"\s\'<>]+)', html)
                    unique_urls = list(set(all_urls))
                    if unique_urls:
                        unique_images.extend(unique_urls[:10])

            except Exception as e:
                print(f"   â””â”€ âš ï¸  å°çº¢ä¹¦é¡µé¢è§£æå¤±è´¥: {str(e)[:40]}")

        # ============ æ–¹æ³•2: ä½¿ç”¨ yt-dlp æå–å›¾ç‰‡ä¿¡æ¯ ============
        if not unique_images:
            print(f"   â””â”€ ğŸ“¡ ä½¿ç”¨yt-dlpè·å–å›¾ç‰‡ä¿¡æ¯...")

            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'extract_flat': 'independent',
                'http_headers': headers,
            }

            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(url, download=False)

                    if info:
                        # thumbnails å­—æ®µ
                        if 'thumbnails' in info and info['thumbnails']:
                            seen_urls = set()
                            for thumb in info['thumbnails']:
                                img_url = thumb.get('url') or thumb.get('data', {}).get('url')
                                if img_url and img_url not in seen_urls:
                                    seen_urls.add(img_url)
                                    unique_images.append(img_url)

                        # pictures å­—æ®µ
                        elif 'pictures' in info and info['pictures']:
                            for pic in info['pictures']:
                                img_url = pic.get('url') or pic.get('data', {}).get('url_default')
                                if img_url:
                                    unique_images.append(img_url)

                        # images å­—æ®µ
                        elif 'images' in info and isinstance(info['images'], list):
                            unique_images.extend(info['images'])

            except Exception as e:
                print(f"   â””â”€ âš ï¸  yt-dlpæå–å¤±è´¥: {str(e)[:40]}")

        # ============ æ–¹æ³•3: é€šç”¨é¡µé¢è§£æ ============
        if not unique_images:
            print(f"   â””â”€ ğŸ“· å°è¯•é€šç”¨é¡µé¢è§£æ...")

            try:
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

                # ä» meta æ ‡ç­¾è·å–
                meta_og_image = soup.find('meta', property='og:image')
                if meta_og_image:
                    img_url = meta_og_image.get('content')
                    if img_url:
                        unique_images.append(img_url)

                # ä» twitter:image è·å–
                meta_twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
                if meta_twitter_image:
                    img_url = meta_twitter_image.get('content')
                    if img_url:
                        unique_images.append(img_url)

            except Exception as e:
                print(f"   â””â”€ âš ï¸  é€šç”¨è§£æå¤±è´¥: {str(e)[:40]}")

        if not unique_images:
            result['error'] = "æœªæ‰¾åˆ°å›¾ç‰‡"
            return result

        # æ¸…ç†å’Œå»é‡å›¾ç‰‡URL
        seen = set()
        cleaned_images = []
        for img in unique_images:
            if img and isinstance(img, str) and img.startswith('http') and img not in seen:
                # è½¬æ¢ä¸ºé«˜è´¨é‡URL
                if 'xhscdn.com' in img or 'xhslink' in img:
                    # ç§»é™¤å°ºå¯¸é™åˆ¶å‚æ•°è·å–åŸå›¾
                    img = img.split('?')[0]
                cleaned_images.append(img)
                seen.add(img)

        unique_images = cleaned_images

        if not unique_images:
            result['error'] = "å›¾ç‰‡URLä¸ºç©º"
            return result

        print(f"   â””â”€ ğŸ“· æ‰¾åˆ° {len(unique_images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")

        # ============ ä¸‹è½½å›¾ç‰‡ ============
        downloaded_files = []
        for i, img_url in enumerate(unique_images, 1):
            try:
                img_response = requests.get(img_url, headers=headers, timeout=30)
                img_response.raise_for_status()

                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                ext = '.jpg'
                content_type = img_response.headers.get('Content-Type', '')
                if 'png' in content_type:
                    ext = '.png'
                elif 'webp' in content_type:
                    ext = '.webp'
                elif 'jpeg' in content_type:
                    ext = '.jpg'

                img_filename = f"{safe_title}_{i:02d}{ext}"
                img_path = note_dir / img_filename

                with open(img_path, 'wb') as f:
                    f.write(img_response.content)

                downloaded_files.append(str(img_path))
                print(f"   â””â”€ [{i}/{len(unique_images)}] âœ… {img_filename}")

            except Exception as e:
                print(f"   â””â”€ [{i}/{len(unique_images)}] âŒ ä¸‹è½½å¤±è´¥: {str(e)[:40]}")

        if downloaded_files:
            result['success'] = True
            result['files'] = downloaded_files
            result['count'] = len(downloaded_files)
        else:
            result['error'] = "æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å¤±è´¥"

    except Exception as e:
        result['error'] = f"è·å–å›¾æ–‡å¤±è´¥: {str(e)}"

    return result


def download_video(video_info: dict, index: int, total: int, output_dir: Path, headers: dict = None) -> dict:
    """
    ä¸‹è½½å•ä¸ªè§†é¢‘æˆ–å›¾æ–‡

    Args:
        video_info: è§†é¢‘ä¿¡æ¯å­—å…¸
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°
        output_dir: è¾“å‡ºç›®å½•
        headers: è‡ªå®šä¹‰è¯·æ±‚å¤´

    Returns:
        dict: ä¸‹è½½ç»“æœ
    """
    url = video_info['url']
    title = video_info['title']
    note_type = video_info.get('type', 'video').lower()
    platform = detect_platform(url)

    # åˆ¤æ–­æ˜¯å¦ä¸ºå›¾æ–‡ç±»å‹
    is_normal = note_type == 'normal'

    result = {
        'url': url,
        'title': title,
        'platform': platform,
        'type': note_type,
        'success': False,
        'error': None,
        'output_file': None,
        'elapsed': 0,
        'is_normal': is_normal
    }

    # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
    safe_title = sanitize_filename(title)

    # ============ å›¾æ–‡ç±»å‹å¤„ç† ============
    if is_normal:
        note_dir = output_dir / safe_title

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if note_dir.exists():
            img_count = len(list(note_dir.glob('*.*')))
            # å¦‚æœæ–‡ä»¶å¤¹å­˜åœ¨ä½†å›¾ç‰‡æ•°ä¸º0ï¼Œåˆ é™¤ç©ºæ–‡ä»¶å¤¹é‡æ–°ä¸‹è½½
            if img_count == 0:
                print(f"\n[{index}/{total}] ğŸ“• {title[:50]}... | [å›¾æ–‡]")
                print(f"   â””â”€ âš ï¸  æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œé‡æ–°ä¸‹è½½...")
                shutil.rmtree(note_dir, ignore_errors=True)
            else:
                result['success'] = True
                result['output_file'] = str(note_dir)
                result['skip_reason'] = f'å·²å­˜åœ¨({img_count}å¼ å›¾)'
                result['elapsed'] = 0
                result['count'] = img_count
                result['skip_is_normal'] = True  # æ ‡è®°è·³è¿‡çš„æ˜¯å›¾æ–‡
                result['skip_count'] = img_count

                print(f"\n[{index}/{total}] ğŸ“• {title[:50]}... | [å›¾æ–‡]")
                print(f"   â””â”€ â­ï¸  å·²å­˜åœ¨ ({img_count}å¼ å›¾ç‰‡)")
                return result

        # ä¸‹è½½å›¾æ–‡
        print(f"\n[{index}/{total}] ğŸ“• {title[:50]}... | [å›¾æ–‡]")
        print(f"   â””â”€ å¹³å°: {platform}")

        start_time = time.time()
        img_result = download_images_from_note(url, title, output_dir, platform)
        result['elapsed'] = time.time() - start_time

        if img_result['success']:
            result['success'] = True
            result['output_file'] = str(output_dir / safe_title)
            result['count'] = img_result['count']
            print(f"\r   â””â”€ âœ… å®Œæˆ! ä¸‹è½½äº† {img_result['count']} å¼ å›¾ç‰‡ | {result['elapsed']:.1f}ç§’{' ' * 20}")
        else:
            result['error'] = img_result.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")

        return result

    # ============ è§†é¢‘ç±»å‹å¤„ç† ============
    output_file = output_dir / f"{safe_title}.mp4"

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if output_file.exists():
        file_size = output_file.stat().st_size / 1024 / 1024
        result['success'] = True
        result['output_file'] = str(output_file)
        result['skip_reason'] = 'å·²å­˜åœ¨'
        result['elapsed'] = 0

        print(f"\n[{index}/{total}] {title[:50]}...")
        print(f"   â””â”€ â­ï¸  å·²å­˜åœ¨ ({file_size:.1f}MB)")
        return result

    # ä¸‹è½½ä¿¡æ¯
    print(f"\n[{index}/{total}] {title[:50]}...")
    print(f"   â””â”€ å¹³å°: {platform} | ç±»å‹: ğŸ¬è§†é¢‘")

    try:
        # åŸºç¡€é…ç½®
        progress_hook = ProgressHook()

        ydl_opts = {
            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
            'outtmpl': str(output_dir / f"{safe_title}.%(ext)s"),
            'quiet': True,
            'no_warnings': True,
            'progress_hooks': [progress_hook],
            'concurrentfragments': 4,  # æµå¼ä¸‹è½½
        }

        # å°çº¢ä¹¦ç‰¹æ®Šå¤„ç†ï¼šéœ€è¦Refererå’ŒCookie
        if platform == 'xiaohongshu':
            ydl_opts.update({
                'http_headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://www.xiaohongshu.com/',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                }
            })

        # Bç«™ç‰¹æ®Šå¤„ç†
        elif platform == 'bilibili':
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.bilibili.com/',
            }
            # æ·»åŠ  Cookie
            if BILI_COOKIE:
                headers['Cookie'] = BILI_COOKIE
            ydl_opts.update({
                'http_headers': headers
            })

        # YouTubeç‰¹æ®Šå¤„ç†ï¼ˆä½¿ç”¨æœ€ä½³è´¨é‡ï¼‰
        elif platform == 'youtube':
            # YouTube ä¸‹è½½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå¯èƒ½è¢« 403 é˜»æ­¢
            ydl_opts.update({
                'format': 'bestvideo+bestaudio/best',  # ç®€åŒ–æ ¼å¼é€‰æ‹©
                'http_headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                },
                # SSL/ç½‘ç»œç›¸å…³é€‰é¡¹
                'nocheckcertificate': True,  # ç»•è¿‡SSLè¯ä¹¦é—®é¢˜
                'extractor_retries': 2,  # å‡å°‘é‡è¯•æ¬¡æ•°ä»¥ä¾¿æ›´å¿«å¤±è´¥
                'fragment_retries': 3,
                'retries': 3,
                'file_access_retries': 2,
                'socket_timeout': 30,
                # ä½¿ç”¨å¤–éƒ¨ä¸‹è½½å™¨ï¼ˆå¦‚æœæœ‰ aria2ï¼‰
                # 'external_downloader': 'aria2c',
                # 'external_downloader_args': ['-x', '16', '-k', '1M'],
                # ç¦ç”¨è°ƒç”¨ä¸»é¡µ
                'no_call_home': True,
                'break_on_reject': False,  # é‡åˆ°è¢«é˜»æ­¢çš„æ ¼å¼ç»§ç»­å°è¯•å…¶ä»–æ ¼å¼
            })

            # æ£€æŸ¥æ˜¯å¦è®¾ç½®äº†ä»£ç†
            import os
            if os.environ.get('HTTP_PROXY') or os.environ.get('HTTPS_PROXY'):
                proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
                ydl_opts['proxy'] = proxy
                print(f"   â””â”€ ğŸŒ ä½¿ç”¨ä»£ç†: {proxy}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ YouTube cookies æ–‡ä»¶
            cookie_content = None
            cookie_file = Path(YOUTUBE_COOKIE_FILE)
            cookie_file_alt = Path(YOUTUBE_COOKIE_FILE_ALT)

            if cookie_file.exists():
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookie_content = f.read().strip()
                print(f"   â””â”€ ğŸª ä½¿ç”¨ Cookie æ–‡ä»¶: {YOUTUBE_COOKIE_FILE}")
            elif cookie_file_alt.exists():
                with open(cookie_file_alt, 'r', encoding='utf-8') as f:
                    cookie_content = f.read().strip()
                print(f"   â””â”€ ğŸª ä½¿ç”¨ Cookie æ–‡ä»¶: {YOUTUBE_COOKIE_FILE_ALT}")

            if cookie_content:
                # æ·»åŠ  cookies åˆ°è¯·æ±‚å¤´
                ydl_opts['http_headers']['Cookie'] = cookie_content
                ydl_opts['cookiefile'] = str(cookie_file if cookie_file.exists() else cookie_file_alt)
            else:
                # å°è¯•ä»æµè§ˆå™¨è·å– cookies
                browser_cookies = get_browser_cookies_youtube()
                if browser_cookies:
                    ydl_opts['http_headers']['Cookie'] = browser_cookies
                    print(f"   â””â”€ ğŸª ä½¿ç”¨æµè§ˆå™¨ Cookies")
                else:
                    print(f"   â””â”€ âš ï¸  æœªæ‰¾åˆ° Cookiesï¼Œå¯èƒ½é‡åˆ° 403 é”™è¯¯")

        # è‡ªå®šä¹‰headersä¼˜å…ˆ
        if headers:
            if 'http_headers' not in ydl_opts:
                ydl_opts['http_headers'] = {}
            ydl_opts['http_headers'].update(headers)

        # æ‰§è¡Œä¸‹è½½
        start_time = time.time()

        # è°ƒè¯•ä¿¡æ¯
        if platform == 'youtube':
            print(f"   â””â”€ å¼€å§‹è¿æ¥ YouTube...")

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)

            # è·å–å®é™…ä¸‹è½½çš„æ–‡ä»¶
            downloaded_file = Path(ydl.prepare_filename(info))
            if downloaded_file.exists():
                result['success'] = True
                result['output_file'] = str(downloaded_file)
            else:
                # å¯èƒ½æ–‡ä»¶åæœ‰å˜åŒ–ï¼Œå°è¯•æ‰¾æœ€æ–°æ–‡ä»¶
                files = list(output_dir.glob(f"{safe_title}.*"))
                if files:
                    latest = max(files, key=lambda f: f.stat().st_mtime)
                    # ç¡®ä¿æ˜¯æœ€è¿‘5åˆ†é’Ÿå†…åˆ›å»ºçš„
                    if time.time() - latest.stat().st_mtime < 300:
                        result['success'] = True
                        result['output_file'] = str(latest)

        elapsed = time.time() - start_time
        result['elapsed'] = elapsed

        if result['success']:
            file_size = Path(result['output_file']).stat().st_size / 1024 / 1024
            avg_speed = file_size / elapsed if elapsed > 0 else 0
            print(f"\r   â””â”€ âœ… å®Œæˆ! {elapsed:.1f}ç§’ | {file_size:.1f}MB | å¹³å‡ {avg_speed:.1f}MB/s{' ' * 20}")
        else:
            print(f"\r   â””â”€ âŒ ä¸‹è½½å¤±è´¥: æ–‡ä»¶æœªæ‰¾åˆ°{' ' * 30}")

    except yt_dlp.utils.DownloadError as e:
        error_msg = str(e)

        # YouTube 403 é”™è¯¯å¤„ç†
        if platform == 'youtube' and ('403' in error_msg or 'Forbidden' in error_msg):
            print(f"\r   â””â”€ âŒ YouTube ä¸‹è½½è¢«é˜»æ­¢ (403 Forbidden){' ' * 20}")
            print(f"   â””â”€ ğŸ’¡ è§£å†³æ–¹æ³•:")
            print(f"      1. ä½¿ç”¨æµè§ˆå™¨æ‰©å±• 'Get cookies.txt LOCALLY' å¯¼å‡º YouTube cookies")
            print(f"      2. å°† cookies ä¿å­˜ä¸º {YOUTUBE_COOKIE_FILE} æ”¾åœ¨å½“å‰ç›®å½•")
            print(f"      3. æˆ–ä½¿ç”¨ä»£ç†: set HTTPS_PROXY=http://127.0.0.1:7890")
            result['error'] = f"YouTube 403: éœ€è¦ä½¿ç”¨ cookies æˆ–ä»£ç†"
            result['elapsed'] = time.time() - start_time
            print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")
            return result

        # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ²¡æœ‰è§†é¢‘æ ¼å¼ï¼ˆå¯èƒ½æ˜¯å›¾æ–‡ï¼‰
        if 'No video formats found' in error_msg or 'No media found' in error_msg:
            print(f"\r   â””â”€ âš ï¸  æ— è§†é¢‘æ ¼å¼ï¼Œå°è¯•ä½œä¸ºå›¾æ–‡å¤„ç†...{' ' * 20}")
            # å°è¯•ä½œä¸ºå›¾æ–‡ä¸‹è½½
            start_img = time.time()
            img_result = download_images_from_note(url, title, output_dir, platform)
            result['elapsed'] = time.time() - start_time

            if img_result['success']:
                result['success'] = True
                result['output_file'] = str(output_dir / safe_title)
                result['count'] = img_result['count']
                result['is_normal'] = True  # æ ‡è®°ä¸ºå›¾æ–‡
                print(f"\r   â””â”€ âœ… å›¾æ–‡ä¸‹è½½æˆåŠŸ! {img_result['count']}å¼ å›¾ç‰‡ | {result['elapsed']:.1f}ç§’{' ' * 20}")
            else:
                result['error'] = f"è§†é¢‘å’Œå›¾æ–‡å‡å¤±è´¥: {img_result.get('error', 'æœªçŸ¥é”™è¯¯')[:60]}"
                result['elapsed'] = time.time() - start_time
                print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")
        else:
            result['error'] = f"ä¸‹è½½é”™è¯¯: {error_msg[:80]}"
            result['elapsed'] = time.time() - start_time
            print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")
    except Exception as e:
        result['error'] = f"é”™è¯¯: {str(e)[:80]}"
        result['elapsed'] = time.time() - start_time
        print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")

    return result


def batch_download(videos: list, author_name: str, output_base_dir: str = "downloaded_videos") -> list:
    """
    æ‰¹é‡ä¸‹è½½è§†é¢‘

    Args:
        videos: è§†é¢‘ä¿¡æ¯åˆ—è¡¨
        author_name: ä½œè€…åç§°ï¼ˆç”¨äºåˆ›å»ºæ–‡ä»¶å¤¹ï¼‰
        output_base_dir: åŸºç¡€è¾“å‡ºç›®å½•

    Returns:
        list: ä¸‹è½½ç»“æœåˆ—è¡¨
    """
    if not videos:
        print("âŒ æ²¡æœ‰è§†é¢‘å¯ä¸‹è½½")
        return []

    # åˆ›å»ºä½œè€…ç›®å½•
    author_dir = Path(output_base_dir) / sanitize_filename(author_name)
    author_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {author_dir}")

    results = []
    success_count = 0
    skip_count = 0
    fail_count = 0
    total_elapsed = 0
    total_images = 0
    total_videos = 0

    start_total = time.time()

    for i, video in enumerate(videos, 1):
        result = download_video(video, i, len(videos), author_dir)
        results.append(result)

        total_elapsed += result['elapsed']

        if result['success']:
            if 'skip_reason' in result:
                skip_count += 1
                # è·³è¿‡çš„å›¾æ–‡ä¹Ÿè¦ç»Ÿè®¡
                if result.get('skip_is_normal'):
                    total_images += result.get('skip_count', 0)
                else:
                    total_videos += 1
            else:
                success_count += 1
                # ç»Ÿè®¡å›¾æ–‡/è§†é¢‘æ•°é‡
                if result.get('is_normal'):
                    total_images += result.get('count', 0)
                else:
                    total_videos += 1
        else:
            fail_count += 1

        # é¿å…è¯·æ±‚è¿‡å¿«
        if i < len(videos):
            time.sleep(1)

    total_time = time.time() - start_total

    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"   æ€»è®¡: {len(videos)} | æˆåŠŸ: {success_count} | è·³è¿‡: {skip_count} | å¤±è´¥: {fail_count}")

    # è¯¦ç»†ç»Ÿè®¡
    if total_images > 0 or total_videos > 0:
        print(f"   ğŸ¬ è§†é¢‘: {total_videos} ä¸ª | ğŸ“· å›¾æ–‡: {total_images} å¼ å›¾ç‰‡")

    if success_count > 0:
        avg_time = total_elapsed / success_count if success_count > 0 else 0
        print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’ | å¹³å‡æ¯ä¸ª: {avg_time:.1f}ç§’")

    print("=" * 80)

    return results


def save_report(results: list, author_name: str, output_dir: str = "downloaded_videos"):
    """ä¿å­˜ä¸‹è½½æŠ¥å‘Š"""
    report_dir = Path(output_dir)
    report_dir.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨å›ºå®šçš„æŠ¥å‘Šæ–‡ä»¶åï¼Œæ‰€æœ‰æŠ¥å‘Šè¿½åŠ åˆ°åŒä¸€ä¸ªæ–‡ä»¶
    report_file = report_dir / f"{sanitize_filename(author_name)}_ä¸‹è½½æŠ¥å‘Š.txt"

    # ä½¿ç”¨UTF-8 BOMç¼–ç ï¼Œç¡®ä¿Windowsè®°äº‹æœ¬èƒ½æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
    # è¿½åŠ æ¨¡å¼ï¼Œæ‰€æœ‰æŠ¥å‘Šå†™å…¥åŒä¸€ä¸ªæ–‡ä»¶
    with open(report_file, 'a', encoding='utf-8-sig') as f:
        f.write(f"è§†é¢‘/å›¾æ–‡ä¸‹è½½æŠ¥å‘Š\n")
        f.write(f"{'='*60}\n")
        f.write(f"ä½œè€…: {author_name}\n")
        f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"{'='*60}\n\n")

        success = sum(1 for r in results if r['success'] and 'skip_reason' not in r)
        skip = sum(1 for r in results if r['success'] and 'skip_reason' in r)
        fail = sum(1 for r in results if not r['success'])
        total_time = sum(r.get('elapsed', 0) for r in results)

        # ç»Ÿè®¡å›¾æ–‡/è§†é¢‘
        total_images = sum(r.get('count', 0) for r in results if r.get('is_normal') and r['success'])
        total_videos = sum(1 for r in results if not r.get('is_normal') and r['success'] and 'skip_reason' not in r)

        f.write(f"æ€»è®¡: {len(results)} | æˆåŠŸ: {success} | è·³è¿‡: {skip} | å¤±è´¥: {fail}\n")
        f.write(f"ğŸ¬ è§†é¢‘: {total_videos} ä¸ª | ğŸ“· å›¾æ–‡: {total_images} å¼ å›¾ç‰‡\n")
        f.write(f"æ€»è€—æ—¶: {total_time:.1f}ç§’\n\n")
        f.write(f"{'='*60}\n\n")

        # è¯¦ç»†åˆ—è¡¨
        f.write(f"{'åºå·':<5} {'ç±»å‹':<6} {'çŠ¶æ€':<6} {'è€—æ—¶':<10} {'æ ‡é¢˜'}\n")
        f.write(f"{'-'*70}\n")

        for i, r in enumerate(results, 1):
            if r['success']:
                if 'skip_reason' in r:
                    status = "è·³è¿‡"
                else:
                    status = "æˆåŠŸ"
            else:
                status = "å¤±è´¥"

            # ç±»å‹æ˜¾ç¤º
            if r.get('is_normal'):
                type_str = "å›¾æ–‡"
                if r['success'] and 'skip_reason' not in r:
                    status = f"æˆåŠŸ({r.get('count', 0)}å¼ )"
            else:
                type_str = "è§†é¢‘"

            elapsed_str = f"{r.get('elapsed', 0):.1f}s" if r.get('elapsed', 0) > 0 else "--"

            title = r['title'][:40]
            f.write(f"{i:<5} {type_str:<6} {status:<8} {elapsed_str:<10} {title}\n")

        f.write(f"\n{'='*60}\n\n")

        # å¤±è´¥è¯¦æƒ…
        failed_results = [r for r in results if not r['success']]
        if failed_results:
            f.write(f"å¤±è´¥è¯¦æƒ…:\n\n")
            for r in failed_results:
                f.write(f"- {r['title']}\n")
                f.write(f"  é“¾æ¥: {r['url']}\n")
                f.write(f"  é”™è¯¯: {r.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n")

    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file.name}")


# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(
        description="ä»CSVæ–‡ä»¶æ‰¹é‡ä¸‹è½½è§†é¢‘",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

1. ä¸‹è½½å•ä¸ªCSVæ–‡ä»¶:
   python download_videos_from_csv.py -csv "MediaCrawler/xhs_videos_output/æ¨é›¨å¤-Yukun.csv"

2. ä¸‹è½½ç›®å½•ä¸‹æ‰€æœ‰CSV:
   python download_videos_from_csv.py -dir "MediaCrawler/xhs_videos_output"

3. åªä¸‹è½½videoç±»å‹:
   python download_videos_from_csv.py -csv "xxx.csv" --type video

4. æŒ‡å®šè¾“å‡ºç›®å½•:
   python download_videos_from_csv.py -csv "xxx.csv" -o "my_videos"

5. æµ‹è¯•ï¼ˆåªä¸‹è½½å‰3ä¸ªï¼‰:
   python download_videos_from_csv.py -csv "xxx.csv" --limit 3
        """
    )

    parser.add_argument('-csv', '--csv-file', help='CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-dir', '--directory', help='CSVæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆå¤„ç†æ‰€æœ‰CSVï¼‰')
    parser.add_argument('-o', '--output', default='downloaded_videos', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: downloaded_videosï¼‰')
    parser.add_argument('--type', choices=['video', 'normal'], help='ç­›é€‰è§†é¢‘ç±»å‹')
    parser.add_argument('--limit', type=int, help='é™åˆ¶ä¸‹è½½æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--yes', '-y', action='store_true', help='è·³è¿‡ç¡®è®¤ç›´æ¥å¼€å§‹ä¸‹è½½')

    args = parser.parse_args()

    # ç¡®å®šè¦å¤„ç†çš„CSVæ–‡ä»¶
    csv_files = []

    if args.csv_file:
        csv_files.append(args.csv_file)
    elif args.directory:
        dir_path = Path(args.directory)
        if dir_path.is_dir():
            csv_files = list(dir_path.glob('*.csv'))
            # æ’é™¤æŠ¥å‘Šæ–‡ä»¶
            csv_files = [f for f in csv_files if not f.name.startswith('_')]
        else:
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.directory}")
            return
    else:
        parser.print_help()
        return

    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return

    print(f"ğŸ“‚ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

    # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
    all_results = {}

    for csv_file in csv_files:
        csv_file = Path(csv_file)
        author_name = get_author_name_from_csv(str(csv_file))

        print("\n" + "=" * 80)
        print(f"ğŸ“ å¤„ç†ä½œè€…: {author_name}")
        print(f"   æ–‡ä»¶: {csv_file.name}")
        print("=" * 80)

        # è§£æCSV
        videos = parse_csv(str(csv_file), args.type)

        if not videos:
            print(f"âš ï¸  è·³è¿‡: æ²¡æœ‰è§†é¢‘")
            continue

        # é™åˆ¶æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
        if args.limit:
            videos = videos[:args.limit]
            print(f"âš ï¸  é™åˆ¶ä¸‹è½½æ•°é‡: {args.limit}")

        # æ˜¾ç¤ºè§†é¢‘åˆ—è¡¨
        show_video_list(videos)

        # ç¡®è®¤ä¸‹è½½
        if not args.yes:
            response = input("\næ˜¯å¦å¼€å§‹ä¸‹è½½? (y/n): ").strip().lower()
            if response != 'y':
                print("â­ï¸  è·³è¿‡æ­¤ä½œè€…")
                continue

        # æ‰¹é‡ä¸‹è½½
        results = batch_download(videos, author_name, args.output)
        all_results[author_name] = results

        # ä¿å­˜æŠ¥å‘Š
        save_report(results, author_name, args.output)

    # æ€»ä½“ç»Ÿè®¡
    if len(csv_files) > 1:
        print("\n" + "=" * 80)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
        print("=" * 80)

        total_videos = sum(len(r) for r in all_results.values())
        total_success = sum(sum(1 for v in r if v['success'] and 'skip_reason' not in v) for r in all_results.values())
        total_skip = sum(sum(1 for v in r if v['success'] and 'skip_reason' in v) for r in all_results.values())
        total_fail = sum(sum(1 for v in r if not v['success']) for r in all_results.values())

        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   ä½œè€…æ•°: {len(all_results)}")
        print(f"   æ€»è§†é¢‘: {total_videos}")
        print(f"   æˆåŠŸ: {total_success} | è·³è¿‡: {total_skip} | å¤±è´¥: {total_fail}")


if __name__ == "__main__":
    main()
