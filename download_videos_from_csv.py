#!/usr/bin/env python3
"""
ä»CSVæ–‡ä»¶æ‰¹é‡ä¸‹è½½è§†é¢‘

åŠŸèƒ½ï¼š
1. è¯»å– xhs_videos_output ç›®å½•ä¸‹çš„CSVæ–‡ä»¶
2. è‡ªåŠ¨è¯†åˆ«å°çº¢ä¹¦/Bç«™é“¾æ¥
3. ä¸‹è½½è§†é¢‘åˆ°ä»¥ä½œè€…å‘½åçš„æ–‡ä»¶å¤¹
4. æ–‡ä»¶åä½¿ç”¨è§†é¢‘æ ‡é¢˜
5. æ˜¾ç¤ºä¸‹è½½è¿›åº¦å’Œè€—æ—¶

ä½¿ç”¨ç¤ºä¾‹:
    # ä¸‹è½½å•ä¸ªCSVæ–‡ä»¶
    python download_videos_from_csv.py -csv "MediaCrawler/xhs_videos_output/æ¨é›¨å¤-Yukun.csv"

    # ä¸‹è½½ç›®å½•ä¸‹æ‰€æœ‰CSV
    python download_videos_from_csv.py -dir "MediaCrawler/xhs_videos_output"

    # åªä¸‹è½½æŒ‡å®šç±»å‹çš„è§†é¢‘
    python download_videos_from_csv.py -csv "xxx.csv" --type video
"""

import os
import sys
import csv
import re
import time
import threading
import argparse
from pathlib import Path
from datetime import datetime

import yt_dlp
import requests
from bs4 import BeautifulSoup

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# ==================== è¿›åº¦æ¡ ====================

class ProgressHook:
    """yt-dlpä¸‹è½½è¿›åº¦é’©å­"""

    def __init__(self):
        self.downloaded_bytes = 0
        self.total_bytes = 0
        self.speed = 0
        self.eta = 0
        self.filename = ""
        self.status = ""
        self._last_update = 0

    def __call__(self, d):
        if d['status'] == 'downloading':
            self.downloaded_bytes = d.get('downloaded_bytes', 0)
            self.total_bytes = d.get('total_bytes', 0) or d.get('total_bytes_estimate', 0)
            self.speed = d.get('speed', 0)
            self.eta = d.get('eta', 0)
            self.filename = d.get('filename', '')
            self.status = 'downloading'

            # é™åˆ¶æ›´æ–°é¢‘ç‡ï¼ˆæ¯0.5ç§’ï¼‰
            now = time.time()
            if now - self._last_update > 0.5:
                self._print_progress()
                self._last_update = now

        elif d['status'] == 'finished':
            self.status = 'finished'
            print(f"\r   â””â”€ ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨å¤„ç†...{' ' * 40}")

    def _print_progress(self):
        if self.total_bytes and self.total_bytes > 0:
            percent = self.downloaded_bytes / self.total_bytes * 100
            bar_length = 25
            filled = int(bar_length * percent / 100)
            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

            # é€Ÿåº¦æ˜¾ç¤º
            speed = self.speed or 0
            if speed > 0:
                if speed >= 1024 * 1024:
                    speed_str = f"{speed / 1024 / 1024:.1f}MB/s"
                elif speed >= 1024:
                    speed_str = f"{speed / 1024:.1f}KB/s"
                else:
                    speed_str = f"{speed:.0f}B/s"
            else:
                speed_str = "--"

            # ETAæ˜¾ç¤º
            eta = self.eta if self.eta and self.eta > 0 else 0
            if eta > 0:
                eta_str = f"{eta:.0f}s"
            else:
                eta_str = "--"

            # å·²ä¸‹è½½å¤§å°
            downloaded_mb = self.downloaded_bytes / 1024 / 1024
            total_mb = self.total_bytes / 1024 / 1024

            print(f"\r   â””â”€ [{bar}] {percent:5.1f}% | {downloaded_mb:.1f}/{total_mb:.1f}MB | {speed_str} | ETA: {eta_str}{' ' * 10}", end='', flush=True)


# ==================== å·¥å…·å‡½æ•° ====================

def sanitize_filename(name: str, max_length: int = 200) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    illegal_chars = r'[<>:"/\\|?*]'
    name = re.sub(illegal_chars, '_', name)

    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    name = ''.join(char for char in name if ord(char) >= 32)

    # å»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    name = name.strip('. ')

    # é™åˆ¶é•¿åº¦
    if len(name) > max_length:
        name = name[:max_length].rsplit(' ', 1)[0]  # å°è¯•åœ¨ç©ºæ ¼å¤„æˆªæ–­

    return name or "untitled"


def detect_platform(url: str) -> str:
    """æ£€æµ‹è§†é¢‘å¹³å°"""
    if 'bilibili.com' in url or 'b23.tv' in url:
        return 'bilibili'
    elif 'xiaohongshu.com' in url or 'xhslink.com' in url:
        return 'xiaohongshu'
    else:
        return 'unknown'


def get_author_name_from_csv(csv_path: str) -> str:
    """ä»CSVæ–‡ä»¶åæå–ä½œè€…å"""
    filename = Path(csv_path).stem
    # ç§»é™¤å¯èƒ½çš„æ—¶é—´æˆ³åç¼€
    if re.search(r'_\d{8}', filename):
        filename = re.sub(r'_\d{8}.*', '', filename)
    return filename


def parse_csv(csv_path: str, filter_type: str = None) -> list:
    """
    è§£æCSVæ–‡ä»¶ï¼Œæå–è§†é¢‘ä¿¡æ¯

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        filter_type: ç­›é€‰ç±»å‹ (video/normal)ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨

    Returns:
        list: è§†é¢‘ä¿¡æ¯åˆ—è¡¨ [{index, title, url, type, ...}, ...]
    """
    videos = []

    print(f"\nğŸ“– è¯»å–CSVæ–‡ä»¶: {Path(csv_path).name}")

    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            # æ£€æŸ¥åˆ—å
            if not reader.fieldnames:
                print("âŒ CSVæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return videos

            for row in reader:
                url = row.get('é“¾æ¥', '') or row.get('url', '')
                title = row.get('æ ‡é¢˜', '') or row.get('title', '')
                note_type = row.get('ç±»å‹', '') or row.get('type', '')
                index = row.get('åºå·', '') or row.get('index', '')

                # è·³è¿‡ç©ºURL
                if not url or not url.startswith('http'):
                    continue

                # ç±»å‹ç­›é€‰
                if filter_type and filter_type.lower() != note_type.lower():
                    continue

                videos.append({
                    'index': index,
                    'title': title or f"è§†é¢‘_{index}",
                    'url': url,
                    'type': note_type,
                    'row': row
                })

        print(f"âœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘é“¾æ¥")

    except Exception as e:
        print(f"âŒ è¯»å–CSVå¤±è´¥: {e}")

    return videos


def show_video_list(videos: list, show_count: int = 10):
    """
    æ˜¾ç¤ºå¾…ä¸‹è½½è§†é¢‘åˆ—è¡¨

    Args:
        videos: è§†é¢‘åˆ—è¡¨
        show_count: æ¯é¡µæ˜¾ç¤ºæ•°é‡
    """
    print("\n" + "=" * 80)
    print("ğŸ“‹ å¾…ä¸‹è½½è§†é¢‘åˆ—è¡¨")
    print("=" * 80)

    # ç»Ÿè®¡ç±»å‹
    video_count = sum(1 for v in videos if v.get('type', '').lower() == 'video')
    normal_count = sum(1 for v in videos if v.get('type', '').lower() == 'normal')
    unknown_count = len(videos) - video_count - normal_count

    for i, video in enumerate(videos, 1):
        platform = detect_platform(video['url'])
        platform_icon = {'xiaohongshu': 'ğŸ“•', 'bilibili': 'ğŸ“º', 'unknown': 'ğŸ“„'}.get(platform, 'ğŸ“„')

        # ç±»å‹å›¾æ ‡
        note_type = video.get('type', '').lower()
        if note_type == 'normal':
            type_icon = 'ğŸ“·'
            type_text = 'å›¾æ–‡'
        elif note_type == 'video':
            type_icon = 'ğŸ¬'
            type_text = 'è§†é¢‘'
        else:
            type_icon = 'â“'
            type_text = video['type'] or 'æœªçŸ¥'

        # åºå· | å¹³å° | ç±»å‹ | æ ‡é¢˜
        print(f"{i:3}. [{platform_icon}] [{type_icon}] {video['title'][:45]}... | {type_text}")

    print("=" * 80)
    print(f"æ€»è®¡: {len(videos)} | ğŸ¬è§†é¢‘: {video_count} | ğŸ“·å›¾æ–‡: {normal_count} | â“æœªçŸ¥: {unknown_count}")
    print("=" * 80)


# ==================== ä¸‹è½½æ ¸å¿ƒ ====================

def download_images_from_note(url: str, title: str, output_dir: Path, platform: str) -> dict:
    """
    ä¸‹è½½å°çº¢ä¹¦/Bç«™å›¾æ–‡å†…å®¹ä¸­çš„å›¾ç‰‡

    Args:
        url: å›¾æ–‡é“¾æ¥
        title: æ ‡é¢˜
        output_dir: è¾“å‡ºç›®å½•
        platform: å¹³å°åç§°

    Returns:
        dict: ä¸‹è½½ç»“æœ {success, files, count, error}
    """
    import json
    import re

    result = {
        'success': False,
        'files': [],
        'count': 0,
        'error': None
    }

    safe_title = sanitize_filename(title)
    note_dir = output_dir / safe_title
    note_dir.mkdir(parents=True, exist_ok=True)

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.xiaohongshu.com/' if platform == 'xiaohongshu' else 'https://www.bilibili.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }

    unique_images = []

    try:
        # ============ æ–¹æ³•1: å°çº¢ä¹¦ä¸“ç”¨ - ç›´æ¥è§£æé¡µé¢è·å–å›¾ç‰‡ ============
        if platform == 'xiaohongshu':
            print(f"   â””â”€ ğŸ“¡ è§£æå°çº¢ä¹¦é¡µé¢è·å–å›¾ç‰‡...")

            try:
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()

                html = response.text

                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°404
                if '/404?' in response.url or 'ä½ è®¿é—®çš„é¡µé¢ä¸è§äº†' in html:
                    print(f"   â””â”€ âš ï¸  é¡µé¢æ— æ³•è®¿é—®ï¼ˆåçˆ¬è™«ä¿æŠ¤æˆ–é“¾æ¥å¤±æ•ˆï¼‰")
                    # ç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•

                # æ”¹è¿›ç‰ˆï¼šä½¿ç”¨æ‹¬å·åŒ¹é…æ³•å®Œæ•´æå– imageList
                start_idx = html.find('window.__INITIAL_STATE__=')
                if start_idx >= 0:
                    start_idx += len('window.__INITIAL_STATE__=')
                    end_idx = html.find('</script>', start_idx)
                    json_str = html[start_idx:end_idx]

                    # æŸ¥æ‰¾ imageList æ•°ç»„ - ä½¿ç”¨è®¡æ•°å™¨åŒ¹é…å®Œæ•´çš„æ•°ç»„
                    list_start = json_str.find('"imageList"')
                    if list_start >= 0:
                        bracket_start = json_str.find('[', list_start)
                        if bracket_start >= 0:
                            # æ‰‹åŠ¨åŒ¹é…å¯¹åº”çš„ ]
                            depth = 0
                            i = bracket_start
                            while i < len(json_str):
                                if json_str[i] == '[':
                                    depth += 1
                                elif json_str[i] == ']':
                                    depth -= 1
                                    if depth == 0:
                                        bracket_end = i
                                        break
                                i += 1

                            list_content = json_str[bracket_start+1:bracket_end]

                            # åªæå– urlDefaultï¼ˆé»˜è®¤/åŸå›¾ï¼‰ï¼Œè·³è¿‡å…¶ä»–å˜ä½“
                            url_pattern = r'"urlDefault":"([^"]+)"'
                            for match in re.finditer(url_pattern, list_content):
                                img_url = match.group(1)
                                if img_url:
                                    # è§£ç  Unicode è½¬ä¹‰
                                    try:
                                        img_url = img_url.encode('utf-8').decode('unicode_escape')
                                    except:
                                        pass
                                    # ç¡®ä¿ https åè®®
                                    if img_url.startswith('http://'):
                                        img_url = 'https://' + img_url[7:]
                                    if 'xhscdn' in img_url:
                                        unique_images.append(img_url)

                # å¤‡ç”¨ï¼šå¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯• JSON è§£æ
                if not unique_images:
                    initial_state_pattern = r'window\.__INITIAL_STATE__\s*=\s*({.+?});'
                    match = re.search(initial_state_pattern, html)
                    if match:
                        try:
                            initial_state = json.loads(match.group(1))
                            image_list = initial_state.get('note', {}).get('noteDetail', {}).get('imageList', [])
                            if isinstance(image_list, list):
                                for img_obj in image_list:
                                    if isinstance(img_obj, dict):
                                        img_url = img_obj.get('urlDefault') or img_obj.get('url')
                                        if img_url:
                                            unique_images.append(img_url)
                        except json.JSONDecodeError:
                            pass

                # å¤‡ç”¨2: ä»æ•´ä¸ª HTML ä¸­æœç´¢ sns-webpic å›¾ç‰‡URL
                if not unique_images:
                    all_urls = re.findall(r'(https://sns-webpic[^\"\s\'<>]+)', html)
                    unique_urls = list(set(all_urls))
                    if unique_urls:
                        unique_images.extend(unique_urls[:10])

            except Exception as e:
                print(f"   â””â”€ âš ï¸  å°çº¢ä¹¦é¡µé¢è§£æå¤±è´¥: {str(e)[:40]}")

        # ============ æ–¹æ³•2: ä½¿ç”¨ yt-dlp æå–å›¾ç‰‡ä¿¡æ¯ ============
        if not unique_images:
            print(f"   â””â”€ ğŸ“¡ ä½¿ç”¨yt-dlpè·å–å›¾ç‰‡ä¿¡æ¯...")

            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'extract_flat': 'independent',
                'http_headers': headers,
            }

            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(url, download=False)

                    if info:
                        # thumbnails å­—æ®µ
                        if 'thumbnails' in info and info['thumbnails']:
                            seen_urls = set()
                            for thumb in info['thumbnails']:
                                img_url = thumb.get('url') or thumb.get('data', {}).get('url')
                                if img_url and img_url not in seen_urls:
                                    seen_urls.add(img_url)
                                    unique_images.append(img_url)

                        # pictures å­—æ®µ
                        elif 'pictures' in info and info['pictures']:
                            for pic in info['pictures']:
                                img_url = pic.get('url') or pic.get('data', {}).get('url_default')
                                if img_url:
                                    unique_images.append(img_url)

                        # images å­—æ®µ
                        elif 'images' in info and isinstance(info['images'], list):
                            unique_images.extend(info['images'])

            except Exception as e:
                print(f"   â””â”€ âš ï¸  yt-dlpæå–å¤±è´¥: {str(e)[:40]}")

        # ============ æ–¹æ³•3: é€šç”¨é¡µé¢è§£æ ============
        if not unique_images:
            print(f"   â””â”€ ğŸ“· å°è¯•é€šç”¨é¡µé¢è§£æ...")

            try:
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

                # ä» meta æ ‡ç­¾è·å–
                meta_og_image = soup.find('meta', property='og:image')
                if meta_og_image:
                    img_url = meta_og_image.get('content')
                    if img_url:
                        unique_images.append(img_url)

                # ä» twitter:image è·å–
                meta_twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
                if meta_twitter_image:
                    img_url = meta_twitter_image.get('content')
                    if img_url:
                        unique_images.append(img_url)

            except Exception as e:
                print(f"   â””â”€ âš ï¸  é€šç”¨è§£æå¤±è´¥: {str(e)[:40]}")

        if not unique_images:
            result['error'] = "æœªæ‰¾åˆ°å›¾ç‰‡"
            return result

        # æ¸…ç†å’Œå»é‡å›¾ç‰‡URL
        seen = set()
        cleaned_images = []
        for img in unique_images:
            if img and isinstance(img, str) and img.startswith('http') and img not in seen:
                # è½¬æ¢ä¸ºé«˜è´¨é‡URL
                if 'xhscdn.com' in img or 'xhslink' in img:
                    # ç§»é™¤å°ºå¯¸é™åˆ¶å‚æ•°è·å–åŸå›¾
                    img = img.split('?')[0]
                cleaned_images.append(img)
                seen.add(img)

        unique_images = cleaned_images

        if not unique_images:
            result['error'] = "å›¾ç‰‡URLä¸ºç©º"
            return result

        print(f"   â””â”€ ğŸ“· æ‰¾åˆ° {len(unique_images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")

        # ============ ä¸‹è½½å›¾ç‰‡ ============
        downloaded_files = []
        for i, img_url in enumerate(unique_images, 1):
            try:
                img_response = requests.get(img_url, headers=headers, timeout=30)
                img_response.raise_for_status()

                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                ext = '.jpg'
                content_type = img_response.headers.get('Content-Type', '')
                if 'png' in content_type:
                    ext = '.png'
                elif 'webp' in content_type:
                    ext = '.webp'
                elif 'jpeg' in content_type:
                    ext = '.jpg'

                img_filename = f"{safe_title}_{i:02d}{ext}"
                img_path = note_dir / img_filename

                with open(img_path, 'wb') as f:
                    f.write(img_response.content)

                downloaded_files.append(str(img_path))
                print(f"   â””â”€ [{i}/{len(unique_images)}] âœ… {img_filename}")

            except Exception as e:
                print(f"   â””â”€ [{i}/{len(unique_images)}] âŒ ä¸‹è½½å¤±è´¥: {str(e)[:40]}")

        if downloaded_files:
            result['success'] = True
            result['files'] = downloaded_files
            result['count'] = len(downloaded_files)
        else:
            result['error'] = "æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å¤±è´¥"

    except Exception as e:
        result['error'] = f"è·å–å›¾æ–‡å¤±è´¥: {str(e)}"

    return result


def download_video(video_info: dict, index: int, total: int, output_dir: Path, headers: dict = None) -> dict:
    """
    ä¸‹è½½å•ä¸ªè§†é¢‘æˆ–å›¾æ–‡

    Args:
        video_info: è§†é¢‘ä¿¡æ¯å­—å…¸
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°
        output_dir: è¾“å‡ºç›®å½•
        headers: è‡ªå®šä¹‰è¯·æ±‚å¤´

    Returns:
        dict: ä¸‹è½½ç»“æœ
    """
    url = video_info['url']
    title = video_info['title']
    note_type = video_info.get('type', 'video').lower()
    platform = detect_platform(url)

    # åˆ¤æ–­æ˜¯å¦ä¸ºå›¾æ–‡ç±»å‹
    is_normal = note_type == 'normal'

    result = {
        'url': url,
        'title': title,
        'platform': platform,
        'type': note_type,
        'success': False,
        'error': None,
        'output_file': None,
        'elapsed': 0,
        'is_normal': is_normal
    }

    # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
    safe_title = sanitize_filename(title)

    # ============ å›¾æ–‡ç±»å‹å¤„ç† ============
    if is_normal:
        note_dir = output_dir / safe_title

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if note_dir.exists():
            img_count = len(list(note_dir.glob('*.*')))
            result['success'] = True
            result['output_file'] = str(note_dir)
            result['skip_reason'] = f'å·²å­˜åœ¨({img_count}å¼ å›¾)'
            result['elapsed'] = 0
            result['count'] = img_count
            result['skip_is_normal'] = True  # æ ‡è®°è·³è¿‡çš„æ˜¯å›¾æ–‡
            result['skip_count'] = img_count

            print(f"\n[{index}/{total}] ğŸ“• {title[:50]}... | [å›¾æ–‡]")
            print(f"   â””â”€ â­ï¸  å·²å­˜åœ¨ ({img_count}å¼ å›¾ç‰‡)")
            return result

        # ä¸‹è½½å›¾æ–‡
        print(f"\n[{index}/{total}] ğŸ“• {title[:50]}... | [å›¾æ–‡]")
        print(f"   â””â”€ å¹³å°: {platform}")

        start_time = time.time()
        img_result = download_images_from_note(url, title, output_dir, platform)
        result['elapsed'] = time.time() - start_time

        if img_result['success']:
            result['success'] = True
            result['output_file'] = str(output_dir / safe_title)
            result['count'] = img_result['count']
            print(f"\r   â””â”€ âœ… å®Œæˆ! ä¸‹è½½äº† {img_result['count']} å¼ å›¾ç‰‡ | {result['elapsed']:.1f}ç§’{' ' * 20}")
        else:
            result['error'] = img_result.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")

        return result

    # ============ è§†é¢‘ç±»å‹å¤„ç† ============
    output_file = output_dir / f"{safe_title}.mp4"

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if output_file.exists():
        file_size = output_file.stat().st_size / 1024 / 1024
        result['success'] = True
        result['output_file'] = str(output_file)
        result['skip_reason'] = 'å·²å­˜åœ¨'
        result['elapsed'] = 0

        print(f"\n[{index}/{total}] {title[:50]}...")
        print(f"   â””â”€ â­ï¸  å·²å­˜åœ¨ ({file_size:.1f}MB)")
        return result

    # ä¸‹è½½ä¿¡æ¯
    print(f"\n[{index}/{total}] {title[:50]}...")
    print(f"   â””â”€ å¹³å°: {platform} | ç±»å‹: ğŸ¬è§†é¢‘")

    try:
        # åŸºç¡€é…ç½®
        progress_hook = ProgressHook()

        ydl_opts = {
            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
            'outtmpl': str(output_dir / f"{safe_title}.%(ext)s"),
            'quiet': True,
            'no_warnings': True,
            'progress_hooks': [progress_hook],
            'concurrentfragments': 4,  # æµå¼ä¸‹è½½
        }

        # å°çº¢ä¹¦ç‰¹æ®Šå¤„ç†ï¼šéœ€è¦Refererå’ŒCookie
        if platform == 'xiaohongshu':
            ydl_opts.update({
                'http_headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://www.xiaohongshu.com/',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                }
            })

        # Bç«™ç‰¹æ®Šå¤„ç†
        elif platform == 'bilibili':
            ydl_opts.update({
                'http_headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://www.bilibili.com/',
                }
            })

        # è‡ªå®šä¹‰headersä¼˜å…ˆ
        if headers:
            if 'http_headers' not in ydl_opts:
                ydl_opts['http_headers'] = {}
            ydl_opts['http_headers'].update(headers)

        # æ‰§è¡Œä¸‹è½½
        start_time = time.time()

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)

            # è·å–å®é™…ä¸‹è½½çš„æ–‡ä»¶
            downloaded_file = Path(ydl.prepare_filename(info))
            if downloaded_file.exists():
                result['success'] = True
                result['output_file'] = str(downloaded_file)
            else:
                # å¯èƒ½æ–‡ä»¶åæœ‰å˜åŒ–ï¼Œå°è¯•æ‰¾æœ€æ–°æ–‡ä»¶
                files = list(output_dir.glob(f"{safe_title}.*"))
                if files:
                    latest = max(files, key=lambda f: f.stat().st_mtime)
                    # ç¡®ä¿æ˜¯æœ€è¿‘5åˆ†é’Ÿå†…åˆ›å»ºçš„
                    if time.time() - latest.stat().st_mtime < 300:
                        result['success'] = True
                        result['output_file'] = str(latest)

        elapsed = time.time() - start_time
        result['elapsed'] = elapsed

        if result['success']:
            file_size = Path(result['output_file']).stat().st_size / 1024 / 1024
            avg_speed = file_size / elapsed if elapsed > 0 else 0
            print(f"\r   â””â”€ âœ… å®Œæˆ! {elapsed:.1f}ç§’ | {file_size:.1f}MB | å¹³å‡ {avg_speed:.1f}MB/s{' ' * 20}")
        else:
            print(f"\r   â””â”€ âŒ ä¸‹è½½å¤±è´¥: æ–‡ä»¶æœªæ‰¾åˆ°{' ' * 30}")

    except yt_dlp.utils.DownloadError as e:
        error_msg = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ²¡æœ‰è§†é¢‘æ ¼å¼ï¼ˆå¯èƒ½æ˜¯å›¾æ–‡ï¼‰
        if 'No video formats found' in error_msg or 'No media found' in error_msg:
            print(f"\r   â””â”€ âš ï¸  æ— è§†é¢‘æ ¼å¼ï¼Œå°è¯•ä½œä¸ºå›¾æ–‡å¤„ç†...{' ' * 20}")
            # å°è¯•ä½œä¸ºå›¾æ–‡ä¸‹è½½
            start_img = time.time()
            img_result = download_images_from_note(url, title, output_dir, platform)
            result['elapsed'] = time.time() - start_time

            if img_result['success']:
                result['success'] = True
                result['output_file'] = str(output_dir / safe_title)
                result['count'] = img_result['count']
                result['is_normal'] = True  # æ ‡è®°ä¸ºå›¾æ–‡
                print(f"\r   â””â”€ âœ… å›¾æ–‡ä¸‹è½½æˆåŠŸ! {img_result['count']}å¼ å›¾ç‰‡ | {result['elapsed']:.1f}ç§’{' ' * 20}")
            else:
                result['error'] = f"è§†é¢‘å’Œå›¾æ–‡å‡å¤±è´¥: {img_result.get('error', 'æœªçŸ¥é”™è¯¯')[:60]}"
                result['elapsed'] = time.time() - start_time
                print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")
        else:
            result['error'] = f"ä¸‹è½½é”™è¯¯: {error_msg[:80]}"
            result['elapsed'] = time.time() - start_time
            print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")
    except Exception as e:
        result['error'] = f"é”™è¯¯: {str(e)[:80]}"
        result['elapsed'] = time.time() - start_time
        print(f"\r   â””â”€ âŒ {result['error'][:60]}{' ' * 20}")

    return result


def batch_download(videos: list, author_name: str, output_base_dir: str = "downloaded_videos") -> list:
    """
    æ‰¹é‡ä¸‹è½½è§†é¢‘

    Args:
        videos: è§†é¢‘ä¿¡æ¯åˆ—è¡¨
        author_name: ä½œè€…åç§°ï¼ˆç”¨äºåˆ›å»ºæ–‡ä»¶å¤¹ï¼‰
        output_base_dir: åŸºç¡€è¾“å‡ºç›®å½•

    Returns:
        list: ä¸‹è½½ç»“æœåˆ—è¡¨
    """
    if not videos:
        print("âŒ æ²¡æœ‰è§†é¢‘å¯ä¸‹è½½")
        return []

    # åˆ›å»ºä½œè€…ç›®å½•
    author_dir = Path(output_base_dir) / sanitize_filename(author_name)
    author_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {author_dir}")

    results = []
    success_count = 0
    skip_count = 0
    fail_count = 0
    total_elapsed = 0
    total_images = 0
    total_videos = 0

    start_total = time.time()

    for i, video in enumerate(videos, 1):
        result = download_video(video, i, len(videos), author_dir)
        results.append(result)

        total_elapsed += result['elapsed']

        if result['success']:
            if 'skip_reason' in result:
                skip_count += 1
                # è·³è¿‡çš„å›¾æ–‡ä¹Ÿè¦ç»Ÿè®¡
                if result.get('skip_is_normal'):
                    total_images += result.get('skip_count', 0)
                else:
                    total_videos += 1
            else:
                success_count += 1
                # ç»Ÿè®¡å›¾æ–‡/è§†é¢‘æ•°é‡
                if result.get('is_normal'):
                    total_images += result.get('count', 0)
                else:
                    total_videos += 1
        else:
            fail_count += 1

        # é¿å…è¯·æ±‚è¿‡å¿«
        if i < len(videos):
            time.sleep(1)

    total_time = time.time() - start_total

    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"   æ€»è®¡: {len(videos)} | æˆåŠŸ: {success_count} | è·³è¿‡: {skip_count} | å¤±è´¥: {fail_count}")

    # è¯¦ç»†ç»Ÿè®¡
    if total_images > 0 or total_videos > 0:
        print(f"   ğŸ¬ è§†é¢‘: {total_videos} ä¸ª | ğŸ“· å›¾æ–‡: {total_images} å¼ å›¾ç‰‡")

    if success_count > 0:
        avg_time = total_elapsed / success_count if success_count > 0 else 0
        print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’ | å¹³å‡æ¯ä¸ª: {avg_time:.1f}ç§’")

    print("=" * 80)

    return results


def save_report(results: list, author_name: str, output_dir: str = "downloaded_videos"):
    """ä¿å­˜ä¸‹è½½æŠ¥å‘Š"""
    report_dir = Path(output_dir)
    report_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = report_dir / f"{sanitize_filename(author_name)}_ä¸‹è½½æŠ¥å‘Š_{timestamp}.txt"

    # ä½¿ç”¨UTF-8 BOMç¼–ç ï¼Œç¡®ä¿Windowsè®°äº‹æœ¬èƒ½æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
    with open(report_file, 'w', encoding='utf-8-sig') as f:
        f.write(f"è§†é¢‘/å›¾æ–‡ä¸‹è½½æŠ¥å‘Š\n")
        f.write(f"{'='*60}\n")
        f.write(f"ä½œè€…: {author_name}\n")
        f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"{'='*60}\n\n")

        success = sum(1 for r in results if r['success'] and 'skip_reason' not in r)
        skip = sum(1 for r in results if r['success'] and 'skip_reason' in r)
        fail = sum(1 for r in results if not r['success'])
        total_time = sum(r.get('elapsed', 0) for r in results)

        # ç»Ÿè®¡å›¾æ–‡/è§†é¢‘
        total_images = sum(r.get('count', 0) for r in results if r.get('is_normal') and r['success'])
        total_videos = sum(1 for r in results if not r.get('is_normal') and r['success'] and 'skip_reason' not in r)

        f.write(f"æ€»è®¡: {len(results)} | æˆåŠŸ: {success} | è·³è¿‡: {skip} | å¤±è´¥: {fail}\n")
        f.write(f"ğŸ¬ è§†é¢‘: {total_videos} ä¸ª | ğŸ“· å›¾æ–‡: {total_images} å¼ å›¾ç‰‡\n")
        f.write(f"æ€»è€—æ—¶: {total_time:.1f}ç§’\n\n")
        f.write(f"{'='*60}\n\n")

        # è¯¦ç»†åˆ—è¡¨
        f.write(f"{'åºå·':<5} {'ç±»å‹':<6} {'çŠ¶æ€':<6} {'è€—æ—¶':<10} {'æ ‡é¢˜'}\n")
        f.write(f"{'-'*70}\n")

        for i, r in enumerate(results, 1):
            if r['success']:
                if 'skip_reason' in r:
                    status = "è·³è¿‡"
                else:
                    status = "æˆåŠŸ"
            else:
                status = "å¤±è´¥"

            # ç±»å‹æ˜¾ç¤º
            if r.get('is_normal'):
                type_str = "å›¾æ–‡"
                if r['success'] and 'skip_reason' not in r:
                    status = f"æˆåŠŸ({r.get('count', 0)}å¼ )"
            else:
                type_str = "è§†é¢‘"

            elapsed_str = f"{r.get('elapsed', 0):.1f}s" if r.get('elapsed', 0) > 0 else "--"

            title = r['title'][:40]
            f.write(f"{i:<5} {type_str:<6} {status:<8} {elapsed_str:<10} {title}\n")

        f.write(f"\n{'='*60}\n\n")

        # å¤±è´¥è¯¦æƒ…
        failed_results = [r for r in results if not r['success']]
        if failed_results:
            f.write(f"å¤±è´¥è¯¦æƒ…:\n\n")
            for r in failed_results:
                f.write(f"- {r['title']}\n")
                f.write(f"  é“¾æ¥: {r['url']}\n")
                f.write(f"  é”™è¯¯: {r.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n")

    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file.name}")


# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(
        description="ä»CSVæ–‡ä»¶æ‰¹é‡ä¸‹è½½è§†é¢‘",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

1. ä¸‹è½½å•ä¸ªCSVæ–‡ä»¶:
   python download_videos_from_csv.py -csv "MediaCrawler/xhs_videos_output/æ¨é›¨å¤-Yukun.csv"

2. ä¸‹è½½ç›®å½•ä¸‹æ‰€æœ‰CSV:
   python download_videos_from_csv.py -dir "MediaCrawler/xhs_videos_output"

3. åªä¸‹è½½videoç±»å‹:
   python download_videos_from_csv.py -csv "xxx.csv" --type video

4. æŒ‡å®šè¾“å‡ºç›®å½•:
   python download_videos_from_csv.py -csv "xxx.csv" -o "my_videos"

5. æµ‹è¯•ï¼ˆåªä¸‹è½½å‰3ä¸ªï¼‰:
   python download_videos_from_csv.py -csv "xxx.csv" --limit 3
        """
    )

    parser.add_argument('-csv', '--csv-file', help='CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-dir', '--directory', help='CSVæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆå¤„ç†æ‰€æœ‰CSVï¼‰')
    parser.add_argument('-o', '--output', default='downloaded_videos', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: downloaded_videosï¼‰')
    parser.add_argument('--type', choices=['video', 'normal'], help='ç­›é€‰è§†é¢‘ç±»å‹')
    parser.add_argument('--limit', type=int, help='é™åˆ¶ä¸‹è½½æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--yes', '-y', action='store_true', help='è·³è¿‡ç¡®è®¤ç›´æ¥å¼€å§‹ä¸‹è½½')

    args = parser.parse_args()

    # ç¡®å®šè¦å¤„ç†çš„CSVæ–‡ä»¶
    csv_files = []

    if args.csv_file:
        csv_files.append(args.csv_file)
    elif args.directory:
        dir_path = Path(args.directory)
        if dir_path.is_dir():
            csv_files = list(dir_path.glob('*.csv'))
            # æ’é™¤æŠ¥å‘Šæ–‡ä»¶
            csv_files = [f for f in csv_files if not f.name.startswith('_')]
        else:
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.directory}")
            return
    else:
        parser.print_help()
        return

    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return

    print(f"ğŸ“‚ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

    # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
    all_results = {}

    for csv_file in csv_files:
        csv_file = Path(csv_file)
        author_name = get_author_name_from_csv(str(csv_file))

        print("\n" + "=" * 80)
        print(f"ğŸ“ å¤„ç†ä½œè€…: {author_name}")
        print(f"   æ–‡ä»¶: {csv_file.name}")
        print("=" * 80)

        # è§£æCSV
        videos = parse_csv(str(csv_file), args.type)

        if not videos:
            print(f"âš ï¸  è·³è¿‡: æ²¡æœ‰è§†é¢‘")
            continue

        # é™åˆ¶æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
        if args.limit:
            videos = videos[:args.limit]
            print(f"âš ï¸  é™åˆ¶ä¸‹è½½æ•°é‡: {args.limit}")

        # æ˜¾ç¤ºè§†é¢‘åˆ—è¡¨
        show_video_list(videos)

        # ç¡®è®¤ä¸‹è½½
        if not args.yes:
            response = input("\næ˜¯å¦å¼€å§‹ä¸‹è½½? (y/n): ").strip().lower()
            if response != 'y':
                print("â­ï¸  è·³è¿‡æ­¤ä½œè€…")
                continue

        # æ‰¹é‡ä¸‹è½½
        results = batch_download(videos, author_name, args.output)
        all_results[author_name] = results

        # ä¿å­˜æŠ¥å‘Š
        save_report(results, author_name, args.output)

    # æ€»ä½“ç»Ÿè®¡
    if len(csv_files) > 1:
        print("\n" + "=" * 80)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
        print("=" * 80)

        total_videos = sum(len(r) for r in all_results.values())
        total_success = sum(sum(1 for v in r if v['success'] and 'skip_reason' not in v) for r in all_results.values())
        total_skip = sum(sum(1 for v in r if v['success'] and 'skip_reason' in v) for r in all_results.values())
        total_fail = sum(sum(1 for v in r if not v['success']) for r in all_results.values())

        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   ä½œè€…æ•°: {len(all_results)}")
        print(f"   æ€»è§†é¢‘: {total_videos}")
        print(f"   æˆåŠŸ: {total_success} | è·³è¿‡: {total_skip} | å¤±è´¥: {total_fail}")


if __name__ == "__main__":
    main()
