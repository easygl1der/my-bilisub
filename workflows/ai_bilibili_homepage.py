#!/usr/bin/env python3
"""
AIè‡ªåŠ¨åˆ·Bç«™é¦–é¡µå¹¶æ€»ç»“

ä¸€é”®å®Œæˆï¼š
1. åˆ·æ–°Bç«™é¦–é¡µæ¨èï¼ˆè‡ªå®šä¹‰æ¬¡æ•°ï¼‰
2. é‡‡é›†è§†é¢‘ä¿¡æ¯å¹¶å¯¼å‡ºCSV
3. æ‰¹é‡æå–å†…ç½®å­—å¹•
4. AIç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆæ¨é€è¶‹åŠ¿+è¯¦ç»†åˆ†ç±»ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
    # é»˜è®¤é…ç½®ï¼ˆåˆ·æ–°3æ¬¡ï¼Œæœ€å¤š50ä¸ªè§†é¢‘ï¼‰
    python ai_bilibili_homepage.py

    # ä»…é‡‡é›†ï¼Œç”ŸæˆCSV
    python ai_bilibili_homepage.py --mode scrape

    # é‡‡é›†+æå–å­—å¹•
    python ai_bilibili_homepage.py --mode scrape+subtitle

    # å®Œæ•´æµç¨‹ï¼ˆé‡‡é›†+å­—å¹•+AIï¼‰
    python ai_bilibili_homepage.py --mode full --model flash-lite

    # è‡ªå®šä¹‰åˆ·æ–°æ¬¡æ•°å’Œè§†é¢‘æ•°
    python ai_bilibili_homepage.py --refresh-count 5 --max-videos 100 --mode full

    # ä»å·²æœ‰CSVå¼€å§‹æå–å­—å¹•
    python ai_bilibili_homepage.py --csv homepage_videos_2025-02-23.csv --mode scrape+subtitle

    # ä»…å¯¹å·²æœ‰å­—å¹•ç”ŸæˆAIæ‘˜è¦
    python ai_bilibili_homepage.py --csv homepage_videos_2025-02-23.csv --mode summary-only
"""

import argparse
import asyncio
import sys
import csv
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
import re

# Windowsç¼–ç ä¿®å¤ - å§‹ç»ˆå¯ç”¨UTF-8è¾“å‡º
if sys.platform == 'win32':
    try:
        import io
        # æ— è®ºæ˜¯å¦åœ¨TTYä¸­éƒ½å¼ºåˆ¶ä½¿ç”¨UTF-8
        if hasattr(sys.stdout, 'buffer'):
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        if hasattr(sys.stderr, 'buffer'):
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except (ValueError, AttributeError):
        pass

from playwright.async_api import async_playwright
import httpx
from bs4 import BeautifulSoup
import time
import os

# å»¶è¿Ÿå¯¼å…¥ Gemini APIï¼ˆä»…åœ¨éœ€è¦æ—¶å¯¼å…¥ï¼‰
_gemini_available = False
try:
    from google import genai
    _gemini_available = True
except ImportError:
    try:
        import google.generativeai as genai
        _gemini_available = True
    except ImportError:
        pass

# å»¶è¿Ÿå¯¼å…¥ bilibili_apiï¼ˆä»…åœ¨éœ€è¦æ—¶å¯¼å…¥ï¼‰
_bilibili_api_available = False
try:
    from bilibili_api import video, Credential
    import aiohttp
    _bilibili_api_available = True
except ImportError:
    pass


# ==================== è·¯å¾„é…ç½® ====================
PROJECT_DIR = Path(__file__).parent.parent  # è·å–æ ¹ç›®å½•
MEDIA_CRAWLER_DIR = PROJECT_DIR / "MediaCrawler"
SUBTITLE_OUTPUT = MEDIA_CRAWLER_DIR / "bilibili_subtitles"


# ==================== Cookie è¯»å– ====================
def read_bilibili_cookie():
    """ä» config/cookies.txt è¯»å– Bilibili Cookie"""
    cookie_file = PROJECT_DIR / "config" / "cookies.txt"
    if not cookie_file.exists():
        print("Cookieæ–‡ä»¶ä¸å­˜åœ¨")
        return ""

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£æ [bilibili] éƒ¨åˆ†
    in_bilibili_section = False
    cookies = {}
    for line in content.split('\n'):
        line = line.strip()
        if line == '[bilibili]':
            in_bilibili_section = True
            continue
        elif line.startswith('['):
            in_bilibili_section = False
            continue
        elif in_bilibili_section and '=' in line and not line.startswith('#'):
            key, value = line.split('=', 1)
            cookies[key.strip()] = value.strip()

    # ä¼˜å…ˆä½¿ç”¨ bilibili_full
    if 'bilibili_full' in cookies:
        return cookies['bilibili_full']

    # å¦åˆ™æ„å»ºCookieå­—ç¬¦ä¸²
    return '; '.join([f"{k}={v}" for k, v in cookies.items() if not k.endswith('_full')])


# ==================== è·å–å…³æ³¨åˆ—è¡¨ ====================
async def get_following_list(cookie_str: str) -> set:
    """
    è·å–ç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨ï¼ˆUPä¸»UIDé›†åˆï¼‰

    Returns:
        set: å·²å…³æ³¨UPä¸»çš„UIDé›†åˆ
    """
    following_uids = set()

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Referer": "https://www.bilibili.com",
        "Cookie": cookie_str,
        "Accept": "application/json",
    }

    try:
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            # é¦–å…ˆè·å–ç”¨æˆ·è‡ªå·±çš„UID
            nav_url = "https://api.bilibili.com/x/web-interface/nav"
            nav_response = await client.get(nav_url, headers=headers)

            # è°ƒè¯•ï¼šæ˜¾ç¤ºå“åº”çŠ¶æ€
            print(f"  å¯¼èˆªAPIå“åº”çŠ¶æ€: {nav_response.status_code}")

            if nav_response.status_code != 200:
                print(f"âš ï¸  å¯¼èˆªAPIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {nav_response.status_code}")
                return following_uids

            try:
                nav_data = nav_response.json()
            except Exception as e:
                print(f"âš ï¸  APIå“åº”è§£æå¤±è´¥: {e}")
                return following_uids

            if nav_data.get("code") != 0:
                print(f"âš ï¸  APIè¿”å›é”™è¯¯: code={nav_data.get('code')}, message={nav_data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return following_uids

            user_mid = nav_data.get("data", {}).get("mid")
            if not user_mid:
                print("âš ï¸  æœªç™»å½•æˆ–æ— æ³•è·å–ç”¨æˆ·IDï¼Œè·³è¿‡å…³æ³¨åˆ—è¡¨è·å–")
                return following_uids

            print(f"ğŸ” è·å–å…³æ³¨åˆ—è¡¨ (ç”¨æˆ·ID: {user_mid})...")

            # è·å–å…³æ³¨åˆ—è¡¨ï¼ˆåˆ†é¡µè·å–ï¼‰
            page = 1
            page_size = 50  # æ¯é¡µ50ä¸ª

            while page <= 10:  # æœ€å¤šè·å–10é¡µï¼ˆ500ä¸ªå…³æ³¨ï¼‰
                follow_url = f"https://api.bilibili.com/x/relation/followings?vmid={user_mid}&pn={page}&ps={page_size}&order=desc"

                response = await client.get(follow_url, headers=headers)

                if response.status_code != 200:
                    print(f"  ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    break

                data = response.json()

                if data.get("code") == 0:
                    followings = data.get("data", {}).get("list", [])

                    if not followings:
                        print(f"  ç¬¬{page}é¡µ: æ²¡æœ‰æ›´å¤šå…³æ³¨")
                        break

                    for item in followings:
                        mid = item.get("mid")
                        if mid:
                            # ç¡®ä¿ UID æ˜¯å­—ç¬¦ä¸²ç±»å‹
                            following_uids.add(str(mid))

                    print(f"  ç¬¬{page}é¡µ: å·²è·å– {len(following_uids)} ä¸ªå…³æ³¨")

                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š
                    total = data.get("data", {}).get("total", 0)
                    if len(following_uids) >= total:
                        break

                    page += 1
                else:
                    print(f"  è·å–å…³æ³¨åˆ—è¡¨ç¬¬{page}é¡µå¤±è´¥: code={data.get('code')}, message={data.get('message')}")
                    break

            print(f"âœ… å…³æ³¨åˆ—è¡¨è·å–å®Œæˆï¼Œå…± {len(following_uids)} ä¸ªå·²å…³æ³¨UPä¸»")

            # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰5ä¸ªå…³æ³¨çš„ UID
            if len(following_uids) > 0:
                sample_list = list(following_uids)[:5]
                print(f"  ç¤ºä¾‹UID: {', '.join(sample_list)}")

    except Exception as e:
        print(f"âš ï¸  è·å–å…³æ³¨åˆ—è¡¨å¤±è´¥: {e}")

    return following_uids


# ==================== ç™»å½•éªŒè¯ ====================
async def test_login(cookie_str):
    """æµ‹è¯• Cookie æ˜¯å¦æœ‰æ•ˆ"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Referer": "https://www.bilibili.com",
        "Cookie": cookie_str
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "https://www.bilibili.com/x/web-interface/nav",
                headers=headers
            )
            data = response.json()

            if data.get("code") == 0 and data.get("data", {}).get("isLogin"):
                user_data = data.get("data", {})
                return True, user_data.get('uname', ''), user_data.get('mid', '')
            else:
                return False, '', ''
    except Exception as e:
        print(f"ç™»å½•æµ‹è¯•å¤±è´¥: {e}")
        return False, '', ''


# ==================== è§†é¢‘å¡ç‰‡è§£æ ====================
def parse_video_cards(page_content, following_uids: set = None):
    """
    ä»é¡µé¢å†…å®¹è§£æè§†é¢‘å¡ç‰‡

    Args:
        page_content: é¡µé¢HTMLå†…å®¹
        following_uids: å·²å…³æ³¨UPä¸»çš„UIDé›†åˆï¼ˆç”¨äºæ ‡è®°ï¼‰
    """
    soup = BeautifulSoup(page_content, 'html.parser')

    videos = []
    # æŸ¥æ‰¾è§†é¢‘å¡ç‰‡
    video_cards = soup.select('.bili-video-card')

    for card in video_cards:
        # è·å–BVå·
        video_link = card.select_one('a[href*="/video/BV"]')
        if not video_link:
            continue

        href = video_link.get('href', '')
        if 'BV' in href:
            bvid = href.split('BV')[1].split('?')[0].split('/')[0]
            bvid = 'BV' + bvid
        else:
            continue

        # è·å–æ ‡é¢˜
        title_elem = card.select_one('.bili-video-card__info--tit')
        if not title_elem:
            title_elem = card.select_one('a[href*="/video/BV"]')

        if title_elem:
            title = title_elem.get('title', '') or title_elem.get_text(strip=True)
        else:
            title = ""

        # è·å–UPä¸»ä¿¡æ¯
        uploader_elem = card.select_one('.bili-video-card__info--author')
        uploader = uploader_elem.get_text(strip=True) if uploader_elem else ""

        # è·å–UPä¸»é“¾æ¥
        uploader_link = card.select_one('a[href*="space.bilibili.com"]')
        uploader_url = ""
        uploader_uid = ""
        is_following = False
        if uploader_link:
            uploader_url = uploader_link.get('href', '')
            if uploader_url.startswith('//'):
                uploader_url = 'https:' + uploader_url

            # æå–UID
            if "space.bilibili.com/" in uploader_url:
                uid_part = uploader_url.split("space.bilibili.com/")[-1].split("?")[0].split("/")[0]
                uploader_uid = uid_part

                # æ£€æŸ¥æ˜¯å¦å·²å…³æ³¨
                if following_uids and uploader_uid in following_uids:
                    is_following = True

        video_info = {
            "bvid": bvid,
            "title": title,
            "uploader": uploader,
            "uploader_url": uploader_url,
            "uploader_uid": uploader_uid,
            "video_url": f"https://www.bilibili.com/video/{bvid}",
            "is_following": is_following,  # TODO: å…³æ³¨æ ‡æ³¨åŠŸèƒ½å¾…å®Œå–„
        }
        videos.append(video_info)

    return videos


# ==================== æ­¥éª¤1: é‡‡é›†é¦–é¡µæ¨è ====================
async def scrape_homepage_recommend(
    cookie_str: str,
    refresh_count: int = 3,
    max_videos: int = 50
) -> List[Dict]:
    """
    é‡‡é›†Bç«™é¦–é¡µæ¨èè§†é¢‘

    Args:
        cookie_str: Bç«™Cookie
        refresh_count: åˆ·æ–°æ¬¡æ•°
        max_videos: æœ€å¤§è§†é¢‘æ•°

    Returns:
        è§†é¢‘åˆ—è¡¨ï¼Œæ¯ä¸ªè§†é¢‘åŒ…å«bvidã€titleã€uploaderã€uploader_urlã€uploader_uidã€video_urlã€refresh_batchã€is_following
    """
    print("\n" + "=" * 70)
    print("ğŸ“‹ æ­¥éª¤ 1/3: é‡‡é›†é¦–é¡µæ¨è")
    print("=" * 70)

    # æµ‹è¯•ç™»å½•
    print("ğŸ” æµ‹è¯•ç™»å½•çŠ¶æ€...")
    is_logged_in, username, user_id = await test_login(cookie_str)

    if is_logged_in:
        print(f"âœ… ç™»å½•æˆåŠŸï¼")
        if username:
            print(f"   ç”¨æˆ·å: {username}")
        if user_id:
            print(f"   ç”¨æˆ·ID: {user_id}")
    else:
        print("âš ï¸ ç™»å½•å¤±è´¥ï¼šCookieå¯èƒ½å·²è¿‡æœŸï¼Œç»§ç»­å°è¯•é‡‡é›†...")

    # è·å–å…³æ³¨åˆ—è¡¨
    following_uids = await get_following_list(cookie_str)

    print()

    # å¯åŠ¨æµè§ˆå™¨
    print("å¯åŠ¨æµè§ˆå™¨...")

    all_videos = []
    refresh_times = []  # è®°å½•æ¯æ¬¡åˆ·æ–°çš„æ—¶é—´æˆ³

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()

        # è®¾ç½® Cookie
        cookies_list = []
        for cookie_pair in cookie_str.split(';'):
            if '=' in cookie_pair:
                name, value = cookie_pair.strip().split('=', 1)
                cookies_list.append({
                    'name': name,
                    'value': value,
                    'domain': '.bilibili.com',
                    'path': '/'
                })

        await context.add_cookies(cookies_list)

        page = await context.new_page()

        # é‡‡é›†é¦–é¡µæ¨è
        for i in range(refresh_count):
            batch_num = i + 1
            print(f"ç¬¬ {batch_num}/{refresh_count} æ¬¡åˆ·æ–°...")

            # è®°å½•æ—¶é—´æˆ³
            batch_time = datetime.now()
            refresh_times.append(batch_time)

            await page.goto("https://www.bilibili.com")
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ™ºèƒ½ç­‰å¾…ï¼Œç­‰å¾…å…³é”®å…ƒç´ åŠ è½½å®Œæˆ
            await page.wait_for_selector('.bili-video-card', timeout=15000)
            await page.wait_for_load_state('networkidle', timeout=10000)
            await asyncio.sleep(1)  # çŸ­æš‚ç¼“å†²

            # è·å–é¡µé¢å†…å®¹
            content = await page.content()

            # è§£æè§†é¢‘ï¼ˆä¼ å…¥å…³æ³¨åˆ—è¡¨ï¼‰
            videos = parse_video_cards(content, following_uids)

            # æ·»åŠ åˆ·æ–°æ‰¹æ¬¡ä¿¡æ¯
            for video in videos:
                video['refresh_batch'] = batch_num
                video['refresh_time'] = batch_time.strftime('%Y-%m-%d %H:%M:%S')

            # å»é‡ï¼ˆæŒ‰BVå·ï¼‰
            seen_bvids = {v['bvid'] for v in all_videos}
            new_videos = [v for v in videos if v['bvid'] not in seen_bvids]
            new_count = len(new_videos)

            print(f"  æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘ï¼ˆæ–°å¢ {new_count} ä¸ªï¼‰")

            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_videos.extend(new_videos)

            if len(all_videos) >= max_videos:
                print(f"  å·²è¾¾åˆ°æœ€å¤§è§†é¢‘æ•°é™åˆ¶ ({max_videos})ï¼Œåœæ­¢åˆ·æ–°")
                break

            # æ»šåŠ¨é¡µé¢è§¦å‘åŠ è½½ï¼ˆä¸ºä¸‹ä¸€æ¬¡åˆ·æ–°åšå‡†å¤‡ï¼‰
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            # ä¼˜åŒ–ï¼šæ™ºèƒ½æ»šåŠ¨æ£€æµ‹
            await page.wait_for_function("document.body.scrollHeight > 0", timeout=5000)

        await browser.close()

    print()
    print(f"âœ… é‡‡é›†å®Œæˆï¼")
    print(f"   æ€»è§†é¢‘æ•°: {len(all_videos)} ä¸ª")
    print(f"   åˆ·æ–°æ‰¹æ¬¡: {len(refresh_times)} æ¬¡")

    return all_videos


# ==================== CSV å¯¼å‡º ====================
def export_to_csv(videos: List[Dict], output_path: Path):
    """
    å°†è§†é¢‘åˆ—è¡¨å¯¼å‡ºä¸ºCSVæ–‡ä»¶

    Args:
        videos: è§†é¢‘åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if not videos:
        print("âš ï¸ æ²¡æœ‰è§†é¢‘æ•°æ®å¯å¯¼å‡º")
        return False

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # CSVå­—æ®µ
    fieldnames = [
        'åºå·',
        'BVå·',
        'æ ‡é¢˜',
        'UPä¸»',
        'UPä¸»_UID',
        'UPä¸»ä¸»é¡µ',
        'è§†é¢‘é“¾æ¥',
        'å­—å¹•çŠ¶æ€',
        'åˆ·æ–°æ‰¹æ¬¡',
        'åˆ·æ–°æ—¶é—´',
        'æ˜¯å¦å…³æ³¨'
    ]

    with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for i, video in enumerate(videos, 1):
            writer.writerow({
                'åºå·': i,
                'BVå·': video.get('bvid', ''),
                'æ ‡é¢˜': video.get('title', ''),
                'UPä¸»': video.get('uploader', ''),
                'UPä¸»_UID': video.get('uploader_uid', ''),
                'UPä¸»ä¸»é¡µ': video.get('uploader_url', ''),
                'è§†é¢‘é“¾æ¥': video.get('video_url', ''),
                'å­—å¹•çŠ¶æ€': 'å¾…æå–',
                'åˆ·æ–°æ‰¹æ¬¡': video.get('refresh_batch', ''),
                'åˆ·æ–°æ—¶é—´': video.get('refresh_time', ''),
                'æ˜¯å¦å…³æ³¨': 'æ˜¯' if video.get('is_following', False) else 'å¦'
            })

    print(f"   å·²ä¿å­˜: {output_path}")
    return True


# ==================== JSON å¯¼å‡º ====================
def export_to_json(videos: List[Dict], output_path: Path):
    """
    å°†è§†é¢‘åˆ—è¡¨å¯¼å‡ºä¸ºJSONæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•å’ŒAIåˆ†æï¼‰

    Args:
        videos: è§†é¢‘åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if not videos:
        return False

    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(videos, f, ensure_ascii=False, indent=2)

    print(f"   å·²ä¿å­˜: {output_path}")
    return True


# ==================== æ­¥éª¤2: å­—å¹•æå– ====================
def load_cookies_for_bilibili_api() -> dict:
    """ä» config/cookies.txt åŠ è½½ cookiesï¼ˆç”¨äº bilibili_apiï¼‰"""
    cookie_file = PROJECT_DIR / "config" / "cookies.txt"
    cookies = {}

    if not cookie_file.exists():
        return cookies

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£æ [bilibili] éƒ¨åˆ†
    in_bilibili_section = False
    for line in content.split('\n'):
        line = line.strip()
        if line == '[bilibili]':
            in_bilibili_section = True
            continue
        elif line.startswith('['):
            in_bilibili_section = False
            continue
        elif in_bilibili_section and '=' in line and not line.startswith('#'):
            key, value = line.split('=', 1)
            cookies[key.strip()] = value.strip()

    return cookies


def get_credential():
    """è·å– bilibili_api è®¤è¯å‡­æ®"""
    cookies = load_cookies_for_bilibili_api()
    sessdata = cookies.get("SESSDATA", "")
    bili_jct = cookies.get("bili_jct", "")
    buvid3 = cookies.get("buvid3", "")

    if not sessdata:
        return None

    return Credential(
        sessdata=sessdata,
        bili_jct=bili_jct,
        buvid3=buvid3
    )


def format_srt_time(seconds: float) -> str:
    """å°†ç§’æ•°è½¬æ¢ä¸º SRT æ—¶é—´ç æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


async def fetch_subtitle_srt(bvid: str, title: str, output_dir: Path) -> dict:
    """
    è·å–å•ä¸ªè§†é¢‘çš„ SRT å­—å¹•ï¼ˆä»…å†…ç½®å­—å¹•ï¼‰

    è¿”å›:
        {
            'success': bool,
            'srt_path': str or None,
            'error': str or None
        }
    """
    result = {'success': False, 'srt_path': None, 'error': None}

    if not _bilibili_api_available:
        result['error'] = 'bilibili_api æœªå®‰è£…'
        return result

    try:
        credential = get_credential()
        v = video.Video(bvid=bvid, credential=credential)
        output_dir.mkdir(parents=True, exist_ok=True)

        # è·å–è§†é¢‘ä¿¡æ¯
        info = await v.get_info()
        cid = info["cid"]

        # è·å–å­—å¹•åˆ—è¡¨
        player_info = await v.get_player_info(cid=cid)
        subtitles = player_info.get("subtitle", {}).get("subtitles", [])

        if not subtitles:
            result['error'] = 'æ— å­—å¹•'
            return result

        # ä¸‹è½½ç¬¬ä¸€æ¡å­—å¹•ï¼ˆé€šå¸¸æ˜¯ä¸­æ–‡ï¼‰
        sub = subtitles[0]
        url = "https:" + sub["subtitle_url"]

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                data = await resp.json(content_type=None)

        # æ¸…ç†æ–‡ä»¶å
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
        srt_path = output_dir / f"{bvid}_{safe_title}.srt"

        # ä¿å­˜ SRT
        with open(srt_path, "w", encoding="utf-8") as f:
            for i, item in enumerate(data.get("body", []), 1):
                start_time = format_srt_time(item['from'])
                end_time = format_srt_time(item['to'])
                f.write(f"{i}\n{start_time} --> {end_time}\n{item['content']}\n\n")

        result['success'] = True
        result['srt_path'] = str(srt_path)

    except Exception as e:
        result['error'] = str(e)

    return result


def read_csv_videos(csv_path: Path) -> List[Dict]:
    """è¯»å– CSV æ–‡ä»¶ï¼Œè¿”å›è§†é¢‘åˆ—è¡¨"""
    videos = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            videos.append(row)

    return videos


def write_csv_status(csv_path: Path, videos: List[Dict]):
    """å†™å› CSV æ–‡ä»¶ï¼Œæ›´æ–°å­—å¹•çŠ¶æ€"""
    if not videos:
        return

    # è¯»å–åŸå§‹CSVçš„fieldnamesï¼Œç¡®ä¿åªå†™å…¥åŸå§‹å­—æ®µ
    original_fieldnames = []
    if csv_path.exists():
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            original_fieldnames = reader.fieldnames or []

    # å¦‚æœæ²¡æœ‰åŸå§‹å­—æ®µåï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªvideoçš„å­—æ®µï¼ˆä½†æ’é™¤'å­—å¹•è·¯å¾„'ï¼‰
    if not original_fieldnames:
        original_fieldnames = [k for k in videos[0].keys() if k != 'å­—å¹•è·¯å¾„']

    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=original_fieldnames)
        writer.writeheader()

        # åªå†™å…¥åŸå§‹å­—æ®µä¸­å­˜åœ¨çš„å€¼
        for video in videos:
            row = {k: video.get(k, '') for k in original_fieldnames}
            writer.writerow(row)


async def extract_subtitles_from_csv(
    csv_path: Path,
    subtitle_dir: Path,
    limit: int = None,
    max_concurrent: int = 5
):
    """
    ä»CSVæ–‡ä»¶æ‰¹é‡æå–å­—å¹•ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        subtitle_dir: å­—å¹•è¾“å‡ºç›®å½•
        limit: é™åˆ¶å¤„ç†è§†é¢‘æ•°é‡
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤5ï¼‰
    """
    print("\n" + "=" * 70)
    print("ğŸ“ æ­¥éª¤ 2/3: æ‰¹é‡æå–å­—å¹•ï¼ˆå†…ç½®å­—å¹•ä¼˜å…ˆï¼‰")
    print("=" * 70)

    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return False

    print(f"ğŸ“„ CSVæ–‡ä»¶: {csv_path}")
    videos = read_csv_videos(csv_path)

    if not videos:
        print("âŒ CSVæ–‡ä»¶ä¸ºç©º")
        return False

    if limit:
        videos = videos[:limit]
        print(f"ğŸ”¢ é™åˆ¶æ•°é‡: {limit}")

    print(f"ğŸ“Š æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
    print(f"âš¡ å¹¶å‘æ•°: {max_concurrent}")
    print()

    # åˆ›å»ºå­—å¹•è¾“å‡ºç›®å½•
    subtitle_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ å­—å¹•ä¿å­˜ç›®å½•: {subtitle_dir}")
    print()

    # ç»Ÿè®¡
    success_count = 0
    no_subtitle_count = 0
    fail_count = 0
    skipped_count = 0

    # æ€»è®¡æ—¶
    total_start_time = time.time()

    # è¿‡æ»¤éœ€è¦å¤„ç†çš„è§†é¢‘
    pending_tasks = []
    for i, video_data in enumerate(videos):
        bvid = video_data.get('BVå·', '')

        if not bvid:
            no_subtitle_count += 1
            continue

        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        current_status = video_data.get('å­—å¹•çŠ¶æ€', '').strip()
        if current_status in ['å·²æå–', 'æ— å­—å¹•']:
            skipped_count += 1
            continue

        # æ·»åŠ å¾…å¤„ç†ä»»åŠ¡
        pending_tasks.append((i, video_data))

    print(f"ğŸ“‹ å¾…å¤„ç†è§†é¢‘: {len(pending_tasks)} ä¸ªï¼ˆå·²è·³è¿‡ {skipped_count} ä¸ªï¼‰")
    print()

    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_video(index: int, video_data: dict):
        """å¤„ç†å•ä¸ªè§†é¢‘çš„åŒ…è£…å‡½æ•°ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        async with semaphore:
            bvid = video_data.get('BVå·', '')
            title = video_data.get('æ ‡é¢˜', 'æœªå‘½å')

            print(f"[{len(pending_tasks) - pending_tasks.count(None)}/{len(pending_tasks)}] {title[:40]}...", end='\r')

            # è·å–å­—å¹•
            result = await fetch_subtitle_srt(bvid, title, subtitle_dir)

            if result['success']:
                print(f"  âœ… [{title[:30]}]")
                video_data['å­—å¹•çŠ¶æ€'] = 'å·²æå–'
                video_data['å­—å¹•è·¯å¾„'] = result['srt_path']
                return 'success'
            elif result['error'] == 'æ— å­—å¹•':
                print(f"  âš ï¸  [{title[:30]}] - æ— å­—å¹•")
                video_data['å­—å¹•çŠ¶æ€'] = 'æ— å­—å¹•'
                return 'no_subtitle'
            else:
                print(f"  âŒ [{title[:30]}] - {result['error'][:30]}")
                video_data['å­—å¹•çŠ¶æ€'] = 'æå–å¤±è´¥'
                return 'fail'

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    tasks = [process_video(i, video_data) for i, video_data in pending_tasks]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ç»Ÿè®¡ç»“æœ
    for result in results:
        if isinstance(result, Exception):
            fail_count += 1
        elif result == 'success':
            success_count += 1
        elif result == 'no_subtitle':
            no_subtitle_count += 1
        elif result == 'fail':
            fail_count += 1

    # æœ€ç»ˆä¿å­˜
    write_csv_status(csv_path, videos)

    # æ€»è€—æ—¶
    total_elapsed = time.time() - total_start_time
    speed = len(pending_tasks) / total_elapsed if total_elapsed > 0 else 0

    print()
    print("=" * 70)
    print("âœ… å­—å¹•æå–å®Œæˆï¼")
    print(f"   æˆåŠŸ: {success_count} ä¸ª")
    print(f"   æ— å­—å¹•: {no_subtitle_count} ä¸ª")
    print(f"   å¤±è´¥: {fail_count} ä¸ª")
    print(f"   è·³è¿‡: {skipped_count} ä¸ª")
    print(f"   æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
    print(f"   é€Ÿåº¦: {speed:.2f} ä¸ª/ç§’")
    print("=" * 70)

    return success_count > 0


# ==================== æ­¥éª¤3: AIåˆ†ææŠ¥å‘Šç”Ÿæˆ ====================
def get_gemini_api_key() -> str:
    """è·å– Gemini API Key"""
    # 1. ç¯å¢ƒå˜é‡
    api_key = os.environ.get('GEMINI_API_KEY')
    if api_key:
        return api_key

    # 2. é…ç½®æ–‡ä»¶
    try:
        sys.path.insert(0, str(PROJECT_DIR))
        from config_api import API_CONFIG
        api_key = API_CONFIG.get('gemini', {}).get('api_key')
        if api_key:
            return api_key
    except ImportError:
        pass

    return None


class GeminiClient:
    """ç®€åŒ–çš„ Gemini API å®¢æˆ·ç«¯"""

    def __init__(self, model: str = 'flash', api_key: str = None):
        self.api_key = api_key or get_gemini_api_key()
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°ï¼Œä¸åšæ‹¼æ¥
        self.model_name = f"gemini-2.5-{model}" if model != 'flash' else 'gemini-2.5-flash'

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° Gemini API Keyï¼Œè¯·åœ¨ config_api.py ä¸­é…ç½®æˆ–è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")

        # é…ç½®å®¢æˆ·ç«¯
        try:
            from google import genai
            self.client = genai.Client(api_key=self.api_key)
            self.use_new_sdk = True
        except ImportError:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self.use_new_sdk = False

    def generate_content(self, prompt: str, max_retries: int = 3) -> Dict:
        """ç”Ÿæˆå†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        import time

        for attempt in range(max_retries):
            try:
                if self.use_new_sdk:
                    response = self.client.models.generate_content(
                        model=self.model_name,
                        contents=prompt
                    )
                    text = response.text
                else:
                    import google.generativeai as genai
                    model = genai.GenerativeModel(self.model_name)
                    response = model.generate_content(prompt)
                    text = response.text

                return {
                    'text': text.strip() if text else '',
                    'success': True
                }
            except Exception as e:
                error_msg = str(e)
                # ç½‘ç»œé”™è¯¯æˆ–ä¸´æ—¶æ€§é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
                is_retryable = any(keyword in error_msg.lower() for keyword in [
                    'server disconnected', 'network', 'timeout', 'connection',
                    'temporarily unavailable', 'rate limit', '500', '503'
                ])

                if is_retryable and attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 2  # æŒ‡æ•°é€€é¿: 2, 4, 8ç§’
                    print(f"   âš ï¸  APIè°ƒç”¨å¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {error_msg[:100]}")
                    print(f"   ğŸ”„ {wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    return {
                        'text': '',
                        'success': False,
                        'error': error_msg,
                        'retries': attempt + 1
                    }


def generate_fallback_analysis(videos: List[Dict], batch_stats: Dict) -> tuple:
    """ç”ŸæˆåŸºç¡€ç»Ÿè®¡åˆ†æï¼ˆå½“AI APIä¸å¯ç”¨æ—¶ï¼‰"""

    # ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç»Ÿè®¡
    trend_lines = []
    trend_lines.append("## åˆ·æ–°è®°å½•æ€»è§ˆ\n")
    trend_lines.append("| æ‰¹æ¬¡ | æ—¶é—´ | æ–°å¢è§†é¢‘æ•° | ç´¯è®¡è§†é¢‘æ•° |\n")
    trend_lines.append("|------|------|------------|------------|\n")

    cumulative = 0
    for batch_num in sorted(batch_stats.keys()):
        batch_videos = batch_stats[batch_num]
        count = len(batch_videos)
        cumulative += count
        # è·å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„æ—¶é—´
        time_str = batch_videos[0].get('åˆ·æ–°æ—¶é—´', 'æœªçŸ¥') if batch_videos else 'æœªçŸ¥'
        trend_lines.append(f"| {batch_num} | {time_str} | {count} | {cumulative} |\n")

    # ç»Ÿè®¡å…³æ³¨çš„UPä¸»
    followed_count = sum(1 for v in videos if v.get('æ˜¯å¦å…³æ³¨') == 'æ˜¯')
    trend_lines.append(f"\n**ç»Ÿè®¡æ‘˜è¦**:\n")
    trend_lines.append(f"- æ€»è§†é¢‘æ•°: {len(videos)}\n")
    trend_lines.append(f"- åˆ·æ–°æ‰¹æ¬¡: {len(batch_stats)}\n")
    trend_lines.append(f"- å·²å…³æ³¨UPä¸»: {followed_count} ä¸ª ({followed_count/len(videos)*100:.1f}%)\n")

    # UPä¸»é¢‘ç‡ç»Ÿè®¡
    uploader_counts = {}
    for v in videos:
        uploader = v.get('UPä¸»', 'æœªçŸ¥')
        uploader_counts[uploader] = uploader_counts.get(uploader, 0) + 1

    if uploader_counts:
        trend_lines.append(f"\n**UPä¸»å‡ºç°é¢‘ç‡TOP5**:\n")
        for uploader, count in sorted(uploader_counts.items(), key=lambda x: -x[1])[:5]:
            trend_lines.append(f"- {uploader}: {count}ä¸ªè§†é¢‘\n")

    trend_analysis = ''.join(trend_lines)

    # ç¬¬äºŒéƒ¨åˆ†ï¼šæŒ‰æ‰¹æ¬¡åˆ—å‡ºè§†é¢‘
    detail_lines = []
    detail_lines.append("## ç›®å½•\n")
    detail_lines.append("| æ‰¹æ¬¡ | è§†é¢‘æ•°é‡ | é¡µç  |\n")
    detail_lines.append("|------|----------|------|\n")

    for batch_num in sorted(batch_stats.keys()):
        count = len(batch_stats[batch_num])
        detail_lines.append(f"| {batch_num} | {count} | [è·³è½¬](#ç¬¬{batch_num}æ¬¡åˆ·æ–°) |\n")

    detail_lines.append("\n---\n\n")

    for batch_num in sorted(batch_stats.keys()):
        batch_videos = batch_stats[batch_num]
        detail_lines.append(f"## ç¬¬{batch_num}æ¬¡åˆ·æ–° ({len(batch_videos)}ä¸ªè§†é¢‘)\n\n")

        for i, video in enumerate(batch_videos, 1):
            title = video.get('æ ‡é¢˜', '')
            bvid = video.get('BVå·', '')
            uploader = video.get('UPä¸»', '')
            uploader_uid = video.get('UPä¸»_UID', '')
            uploader_url = video.get('UPä¸»ä¸»é¡µ', '')
            video_url = video.get('è§†é¢‘é“¾æ¥', '')

            detail_lines.append(f"### {i}. {title}\n")
            detail_lines.append(f"- **BVå·**: {bvid}\n")
            detail_lines.append(f"- **UPä¸»**: {uploader} (UID: {uploader_uid})\n")
            detail_lines.append(f"- **UPä¸»ä¸»é¡µ**: {uploader_url}\n")
            detail_lines.append(f"- **è§†é¢‘é“¾æ¥**: {video_url}\n")

            is_following = video.get('æ˜¯å¦å…³æ³¨', 'å¦')
            if is_following == 'æ˜¯':
                detail_lines.append(f"- **çŠ¶æ€**: âœ… å·²å…³æ³¨\n")

            detail_lines.append("\n")

    detail_analysis = ''.join(detail_lines)

    return trend_analysis, detail_analysis


def generate_ai_analysis_report(
    csv_path: Path,
    subtitle_dir: Path,
    model: str = 'flash-lite'
) -> bool:
    """
    ç”ŸæˆAIåˆ†ææŠ¥å‘Šï¼ˆä¸¤éƒ¨åˆ†ç»“æ„ï¼‰

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        subtitle_dir: å­—å¹•ç›®å½•è·¯å¾„
        model: Geminiæ¨¡å‹åç§°

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    print("\n" + "=" * 70)
    print("ğŸ¤– æ­¥éª¤ 3/3: ç”ŸæˆAIåˆ†ææŠ¥å‘Š")
    print("=" * 70)

    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return False

    # è¯»å–è§†é¢‘æ•°æ®
    videos = read_csv_videos(csv_path)
    if not videos:
        print("âŒ CSVæ–‡ä»¶ä¸ºç©º")
        return False

    print(f"ğŸ“Š åˆ†æ {len(videos)} ä¸ªè§†é¢‘")
    print(f"ğŸ¤– æ¨¡å‹: {model}")
    print()

    # åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
    try:
        gemini_client = GeminiClient(model=model)
    except ValueError as e:
        print(f"âŒ {e}")
        print()
        print("è¯·é…ç½® Gemini API Key:")
        print("1. åˆ›å»º config_api.py æ–‡ä»¶")
        print("2. æ·»åŠ å†…å®¹: API_CONFIG = {'gemini': {'api_key': 'your_api_key'}}")
        print("   æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export GEMINI_API_KEY=your_api_key")
        return False

    # ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨é€è¶‹åŠ¿åˆ†æ ====================
    print("ğŸ“Š ç”Ÿæˆç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨é€è¶‹åŠ¿åˆ†æ...")

    # æŒ‰åˆ·æ–°æ‰¹æ¬¡ç»Ÿè®¡
    batch_stats = {}
    for video in videos:
        batch = video.get('åˆ·æ–°æ‰¹æ¬¡', '1')
        if batch not in batch_stats:
            batch_stats[batch] = []
        batch_stats[batch].append(video)

    # æ„å»ºæ¨é€è¶‹åŠ¿åˆ†æçš„ Prompt
    trend_prompt = f"""ä½ æ˜¯ä¸€ä¸ªBç«™æ¨èç®—æ³•åˆ†æä¸“å®¶ã€‚æˆ‘æœ‰ä»¥ä¸‹æ•°æ®ï¼š

æˆ‘åˆ·æ–°äº†Bç«™é¦–é¡µ {len(batch_stats)} æ¬¡ï¼Œæ¯æ¬¡åˆ·æ–°è·å–çš„è§†é¢‘ä¿¡æ¯å¦‚ä¸‹ï¼š

"""

    for batch_num in sorted(batch_stats.keys()):
        batch_videos = batch_stats[batch_num]
        trend_prompt += f"\nç¬¬{batch_num}æ¬¡åˆ·æ–° ({len(batch_videos)}ä¸ªè§†é¢‘):\n"
        for i, video in enumerate(batch_videos[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            title = video.get('æ ‡é¢˜', '')[:50]
            uploader = video.get('UPä¸»', '')
            trend_prompt += f"  {i}. {title} - UPä¸»: {uploader}\n"
        if len(batch_videos) > 10:
            trend_prompt += f"  ... è¿˜æœ‰ {len(batch_videos) - 10} ä¸ªè§†é¢‘\n"

    trend_prompt += """

è¯·åˆ†æï¼š
1. æ¯æ¬¡åˆ·æ–°çš„è§†é¢‘ä¸»é¢˜åˆ†å¸ƒå’Œé£æ ¼ç‰¹ç‚¹
2. ç®—æ³•æ¨é€çš„è¶‹åŠ¿å˜åŒ–
3. æ¨æµ‹ç”¨æˆ·çš„å…´è¶£åå¥½å’Œç®—æ³•çš„æ¨èé€»è¾‘

è¾“å‡ºæ ¼å¼ï¼ˆä½¿ç”¨Markdownï¼‰ï¼š
## åˆ·æ–°è®°å½•æ€»è§ˆ
[ç”¨è¡¨æ ¼æ˜¾ç¤ºæ‰¹æ¬¡ã€è§†é¢‘æ•°ã€ä¸»è¦ä¸»é¢˜]

## å„æ‰¹æ¬¡è§†é¢‘ä¸»é¢˜åˆ†å¸ƒ
[åˆ—å‡ºæ¯ä¸ªæ‰¹æ¬¡çš„ä¸»é¢˜åˆ†ç±»]

## ç®—æ³•æ¨é€è¶‹åŠ¿åˆ†æ
[åˆ†ææ¨é€è¶‹åŠ¿å’Œç®—æ³•é€»è¾‘]
"""

    # ç”Ÿæˆç¬¬ä¸€éƒ¨åˆ†ï¼ˆå¸¦é‡è¯•ï¼‰
    print("   è°ƒç”¨ Gemini API...")
    trend_result = gemini_client.generate_content(trend_prompt)

    if not trend_result['success']:
        retries = trend_result.get('retries', 1)
        print(f"âŒ æ¨é€è¶‹åŠ¿åˆ†æç”Ÿæˆå¤±è´¥ (å·²é‡è¯•{retries}æ¬¡): {trend_result.get('error', 'Unknown error')[:150]}")
        print("   ğŸ“Š ä½¿ç”¨åŸºç¡€ç»Ÿè®¡åˆ†æ...")
        # ç”ŸæˆåŸºç¡€ç»Ÿè®¡ä½œä¸ºç¬¬ä¸€éƒ¨åˆ†
        trend_analysis, _ = generate_fallback_analysis(videos, batch_stats)
        trend_analysis = "## âš ï¸ æ³¨æ„ï¼šç”±äºç½‘ç»œé—®é¢˜ï¼ŒAIåˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼Œä»¥ä¸‹ä¸ºåŸºç¡€ç»Ÿè®¡åˆ†æ\n\n" + trend_analysis
    else:
        trend_analysis = trend_result['text']
        print("âœ… æ¨é€è¶‹åŠ¿åˆ†æå®Œæˆ")
        # æ˜¾ç¤ºé¢„è§ˆ
        if len(trend_analysis) > 200:
            print(f"   é¢„è§ˆ: {trend_analysis[:200]}...")

    # ==================== å¹¶è¡Œç”Ÿæˆä¸¤éƒ¨åˆ†åˆ†æ ====================
    print()
    print("âš¡ å¹¶è¡Œç”Ÿæˆä¸¤éƒ¨åˆ†åˆ†æ...")

    # å‡†å¤‡ç¬¬äºŒéƒ¨åˆ†çš„promptï¼ˆåœ¨å¹¶è¡Œå‰æ„å»ºï¼‰
    # æ£€æŸ¥å­—å¹•æ–‡ä»¶
    subtitle_files = list(subtitle_dir.glob("*.srt")) if subtitle_dir.exists() else []
    has_subtitles = len(subtitle_files) > 0

    if has_subtitles:
        print(f"   æ‰¾åˆ° {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶")
    else:
        print("   âš ï¸  æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶ï¼Œå°†åŸºäºæ ‡é¢˜ç”Ÿæˆåˆ†æ")

    # æ„å»ºè¯¦ç»†åˆ†ç±»åˆ†æçš„ Prompt
    detail_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚æˆ‘æœ‰ä»¥ä¸‹è§†é¢‘æ•°æ®ï¼š

"""

    # è¯»å–å­—å¹•å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    subtitle_contents = {}
    if has_subtitles:
        for srt_file in subtitle_files[:20]:  # æœ€å¤šå¤„ç†20ä¸ªå­—å¹•
            bvid = srt_file.stem.split('_')[0]
            try:
                with open(srt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # åªå–å‰2000å­—ç¬¦ä½œä¸ºæ‘˜è¦
                    subtitle_contents[bvid] = content[:2000]
            except:
                pass

    # æŒ‰æ‰¹æ¬¡åˆ†ç»„è§†é¢‘
    for batch_num in sorted(batch_stats.keys()):
        batch_videos = batch_stats[batch_num]
        detail_prompt += f"\n### ç¬¬{batch_num}æ¬¡åˆ·æ–° ({len(batch_videos)}ä¸ªè§†é¢‘)\n\n"

        for video in batch_videos:
            bvid = video.get('BVå·', '')
            title = video.get('æ ‡é¢˜', '')
            uploader = video.get('UPä¸»', '')
            uploader_uid = video.get('UPä¸»_UID', '')
            uploader_url = video.get('UPä¸»ä¸»é¡µ', '')
            video_url = video.get('è§†é¢‘é“¾æ¥', '')

            detail_prompt += f"**{title}**\n"
            detail_prompt += f"- BVå·: {bvid}\n"
            detail_prompt += f"- UPä¸»: {uploader} (UID: {uploader_uid})\n"
            detail_prompt += f"- UPä¸»ä¸»é¡µ: {uploader_url}\n"
            detail_prompt += f"- è§†é¢‘é“¾æ¥: {video_url}\n"

            # æ·»åŠ å­—å¹•æ‘˜è¦
            if bvid in subtitle_contents:
                detail_prompt += f"- å­—å¹•æ‘˜è¦: {subtitle_contents[bvid][:500]}...\n"

            detail_prompt += "\n"

    detail_prompt += """

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ç›®å½•
| åºå· | ä¸»é¢˜åˆ†ç±» | è§†é¢‘æ•°é‡ | é¡µç  |
|------|----------|----------|------|
| 1 | ä¸»é¢˜å | æ•°é‡ | [è·³è½¬](#ä¸»é¢˜å) |

---

## ä¸»é¢˜å (Nä¸ªè§†é¢‘)

### 1. è§†é¢‘æ ‡é¢˜
- **BVå·**: BV1xxx
- **UPä¸»**: åç§° (UID: xxx)
- **UPä¸»ä¸»é¡µ**: https://space.bilibili.com/xxx
- **è§†é¢‘é“¾æ¥**: https://www.bilibili.com/video/BV1xxx
"""

    if has_subtitles:
        detail_prompt += "- **å­—å¹•æ‘˜è¦**: [åŸºäºå­—å¹•å†…å®¹ç”Ÿæˆ200-300å­—æ‘˜è¦]\n"

    detail_prompt += "- **æ¨èæ‰¹æ¬¡**: ç¬¬Xæ¬¡åˆ·æ–°\n"

    # åˆ›å»ºå¼‚æ­¥å‡½æ•°å¹¶è¡Œæ‰§è¡Œä¸¤æ¬¡APIè°ƒç”¨
    async def generate_both_parts():
        """å¹¶è¡Œç”Ÿæˆä¸¤éƒ¨åˆ†åˆ†æ"""
        import asyncio as _asyncio

        async def get_trend():
            return gemini_client.generate_content(trend_prompt)

        async def get_detail():
            return gemini_client.generate_content(detail_prompt)

        # å¹¶è¡Œæ‰§è¡Œ
        results = await _asyncio.gather(get_trend(), get_detail(), return_exceptions=True)
        return results

    # æ‰§è¡Œå¹¶è¡Œè°ƒç”¨
    print("   è°ƒç”¨ Gemini API (å¹¶è¡Œå¤„ç†è¶‹åŠ¿+è¯¦ç»†åˆ†æ)...")
    results = asyncio.run(generate_both_parts())
    trend_result = results[0] if not isinstance(results[0], Exception) else {'success': False, 'error': str(results[0])}
    detail_result = results[1] if not isinstance(results[1], Exception) else {'success': False, 'error': str(results[1])}

    # å¤„ç†è¶‹åŠ¿åˆ†æç»“æœ
    if not trend_result['success']:
        retries = trend_result.get('retries', 1)
        print(f"âŒ æ¨é€è¶‹åŠ¿åˆ†æç”Ÿæˆå¤±è´¥ (å·²é‡è¯•{retries}æ¬¡): {trend_result.get('error', 'Unknown error')[:150]}")
        print("   ğŸ“Š ä½¿ç”¨åŸºç¡€ç»Ÿè®¡åˆ†æ...")
        # ç”ŸæˆåŸºç¡€ç»Ÿè®¡ä½œä¸ºç¬¬ä¸€éƒ¨åˆ†
        trend_analysis, _ = generate_fallback_analysis(videos, batch_stats)
        trend_analysis = "## âš ï¸ æ³¨æ„ï¼šç”±äºç½‘ç»œé—®é¢˜ï¼ŒAIåˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼Œä»¥ä¸‹ä¸ºåŸºç¡€ç»Ÿè®¡åˆ†æ\n\n" + trend_analysis
    else:
        trend_analysis = trend_result['text']
        print("âœ… æ¨é€è¶‹åŠ¿åˆ†æå®Œæˆ")
        # æ˜¾ç¤ºé¢„è§ˆ
        if len(trend_analysis) > 200:
            print(f"   é¢„è§ˆ: {trend_analysis[:200]}...")

    # å¤„ç†è¯¦ç»†åˆ†æç»“æœ
    print()
    if not detail_result['success']:
        retries = detail_result.get('retries', 1)
        print(f"âŒ è¯¦ç»†åˆ†ç±»åˆ†æç”Ÿæˆå¤±è´¥ (å·²é‡è¯•{retries}æ¬¡): {detail_result.get('error', 'Unknown error')[:150]}")
        print("   ğŸ“Š ä½¿ç”¨åŸºç¡€ç»Ÿè®¡åˆ†æ...")
        # ç”ŸæˆåŸºç¡€åˆ—è¡¨ä½œä¸ºç¬¬äºŒéƒ¨åˆ†
        _, detail_analysis = generate_fallback_analysis(videos, batch_stats)
        detail_analysis = "## âš ï¸ æ³¨æ„ï¼šç”±äºç½‘ç»œé—®é¢˜ï¼ŒAIåˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼Œä»¥ä¸‹ä¸ºåŸºç¡€è§†é¢‘åˆ—è¡¨\n\n" + detail_analysis
    else:
        detail_analysis = detail_result['text']
        print("âœ… è¯¦ç»†åˆ†ç±»åˆ†æå®Œæˆ")
        # æ˜¾ç¤ºé¢„è§ˆ
        if len(detail_analysis) > 200:
            print(f"   é¢„è§ˆ: {detail_analysis[:200]}...")

    # ==================== ä¿å­˜æŠ¥å‘Š ====================
    date_str = datetime.now().strftime('%Y-%m-%d')
    report_path = SUBTITLE_OUTPUT / f"homepage_{date_str}_AIæ€»ç»“.md"

    report_content = f"""# Bç«™é¦–é¡µæ¨èAIåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**é‡‡é›†è§†é¢‘æ•°**: {len(videos)}
**åˆ·æ–°æ‰¹æ¬¡**: {len(batch_stats)}

---

"""

    report_content += trend_analysis
    report_content += "\n\n---\n\n"
    report_content += detail_analysis

    # ä¿å­˜æŠ¥å‘Š
    SUBTITLE_OUTPUT.mkdir(parents=True, exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print()
    print("=" * 70)
    print("âœ… AIåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"   æŠ¥å‘Šè·¯å¾„: {report_path}")
    print("=" * 70)

    return True


# ==================== ä¸»ç¨‹åº ====================
def main():
    parser = argparse.ArgumentParser(
        description="AIè‡ªåŠ¨åˆ·Bç«™é¦–é¡µå¹¶æ€»ç»“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # é»˜è®¤é…ç½®ï¼ˆåˆ·æ–°3æ¬¡ï¼Œæœ€å¤š50ä¸ªè§†é¢‘ï¼‰
  python ai_bilibili_homepage.py

  # ä»…é‡‡é›†ï¼Œç”ŸæˆCSV
  python ai_bilibili_homepage.py --mode scrape

  # é‡‡é›†+æå–å­—å¹•
  python ai_bilibili_homepage.py --mode scrape+subtitle

  # å®Œæ•´æµç¨‹ï¼ˆé‡‡é›†+å­—å¹•+AIï¼‰
  python ai_bilibili_homepage.py --mode full --model flash-lite

  # è‡ªå®šä¹‰åˆ·æ–°æ¬¡æ•°å’Œè§†é¢‘æ•°
  python ai_bilibili_homepage.py --refresh-count 5 --max-videos 100 --mode full

  # ä»å·²æœ‰CSVå¼€å§‹æå–å­—å¹•
  python ai_bilibili_homepage.py --csv homepage_videos_2025-02-23.csv --mode scrape+subtitle

  # ä»…å¯¹å·²æœ‰å­—å¹•ç”ŸæˆAIæ‘˜è¦
  python ai_bilibili_homepage.py --csv homepage_videos_2025-02-23.csv --mode summary-only
        """
    )

    parser.add_argument("--mode", "-m",
                        choices=['scrape', 'scrape+subtitle', 'full', 'summary-only'],
                        default='full',
                        help="å¤„ç†æ¨¡å¼ï¼šscrape(ä»…é‡‡é›†) | scrape+subtitle(é‡‡é›†+å­—å¹•) | full(å®Œæ•´æµç¨‹) | summary-only(ä»…AIæ‘˜è¦)")
    parser.add_argument("--refresh-count", "-r", type=int, default=3,
                        help="åˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰")
    parser.add_argument("--max-videos", "-n", type=int, default=50,
                        help="æœ€å¤§è§†é¢‘æ•°ï¼ˆé»˜è®¤ï¼š50ï¼‰")
    parser.add_argument("--csv", "-c",
                        help="ä½¿ç”¨å·²æœ‰çš„CSVæ–‡ä»¶ï¼ˆè·³è¿‡é‡‡é›†æ­¥éª¤ï¼‰")
    parser.add_argument("--model",
                        choices=['flash', 'flash-lite', 'pro'],
                        default='flash-lite',
                        help="Geminiæ¨¡å‹ï¼ˆé»˜è®¤: flash-liteï¼‰")
    parser.add_argument("--jobs", "-j", type=int, default=3,
                        help="å¹¶å‘å¤„ç†æ•°ï¼ˆé»˜è®¤: 3ï¼‰")

    args = parser.parse_args()

    # ç”Ÿæˆæ–‡ä»¶å
    date_str = datetime.now().strftime('%Y-%m-%d')
    if args.csv:
        csv_path = Path(args.csv)
    else:
        csv_path = PROJECT_DIR / f"homepage_videos_{date_str}.csv"

    json_path = PROJECT_DIR / f"homepage_videos_{date_str}.json"

    print("\n" + "=" * 70)
    print("ğŸ¤– AIè‡ªåŠ¨åˆ·Bç«™å¹¶æ€»ç»“")
    print("=" * 70)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print(f"ğŸ“‹ é…ç½®:")
    print(f"   åˆ·æ–°æ¬¡æ•°: {args.refresh_count}")
    print(f"   æœ€å¤§è§†é¢‘æ•°: {args.max_videos}")
    print(f"   å¤„ç†æ¨¡å¼: {args.mode}")
    if args.mode in ['full', 'summary-only']:
        print(f"   AIæ¨¡å‹: {args.model}")
    print(f"   å­—å¹•ç­–ç•¥: å†…ç½®å­—å¹•ä¼˜å…ˆ")

    # ==================== æ­¥éª¤1: é‡‡é›†é¦–é¡µæ¨è ====================
    if args.mode != 'summary-only' and not args.csv:
        cookie_str = read_bilibili_cookie()

        if not cookie_str:
            print("\nâŒ æ— æ³•è¯»å–Cookieï¼Œè¯·æ£€æŸ¥ config/cookies.txt")
            return 1

        print(f"\nğŸª Cookie é•¿åº¦: {len(cookie_str)} å­—ç¬¦")
        print()

        videos = asyncio.run(scrape_homepage_recommend(
            cookie_str,
            refresh_count=args.refresh_count,
            max_videos=args.max_videos
        ))

        if not videos:
            print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•è§†é¢‘")
            return 1

        # å¯¼å‡ºCSVå’ŒJSON
        print()
        print("ğŸ’¾ å¯¼å‡ºæ•°æ®...")
        export_to_csv(videos, csv_path)
        export_to_json(videos, json_path)

    elif args.csv:
        print(f"\nğŸ“ ä½¿ç”¨å·²æœ‰CSV: {csv_path}")
        if not csv_path.exists():
            print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return 1
    else:
        print(f"\nğŸ“ è¯·æä¾›CSVæ–‡ä»¶æˆ–è¿è¡Œé‡‡é›†æ¨¡å¼")

    # ==================== æ­¥éª¤2: æå–å­—å¹• ====================
    if args.mode in ['scrape+subtitle', 'full']:
        if not _bilibili_api_available:
            print("\n" + "=" * 70)
            print("âš ï¸ å­—å¹•æå–åŠŸèƒ½éœ€è¦ bilibili_api æ¨¡å—")
            print("=" * 70)
            print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š")
            print("  pip install bilibili-api")
            print()
            print("æˆ–è€…è·³è¿‡å­—å¹•æå–ï¼Œä½¿ç”¨ä»…é‡‡é›†æ¨¡å¼ï¼š")
            print("  python ai_bilibili_homepage.py --mode scrape")
            print("=" * 70)
            return 1

        # ç¡®å®šå­—å¹•è¾“å‡ºç›®å½•
        date_str = datetime.now().strftime('%Y-%m-%d')
        if args.csv:
            # ä»CSVæ–‡ä»¶åæå–æ—¥æœŸ
            date_str = csv_path.stem.replace('homepage_videos_', '')
        subtitle_dir = SUBTITLE_OUTPUT / f"homepage_{date_str}"

        # æå–å­—å¹•
        success = asyncio.run(extract_subtitles_from_csv(csv_path, subtitle_dir))
        if not success:
            print("\nâš ï¸ å­—å¹•æå–å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•AIåˆ†æ...")
    elif args.mode == 'summary-only':
        # ä»…AIæ‘˜è¦æ¨¡å¼ï¼Œéœ€è¦ç¡®å®šå­—å¹•ç›®å½•
        date_str = csv_path.stem.replace('homepage_videos_', '')
        subtitle_dir = SUBTITLE_OUTPUT / f"homepage_{date_str}"
    else:
        # ä»…é‡‡é›†æ¨¡å¼ï¼Œä¸æå–å­—å¹•
        subtitle_dir = None

    # ==================== æ­¥éª¤3: AIåˆ†ææŠ¥å‘Šç”Ÿæˆ ====================
    if args.mode == 'full':
        # ç¡®å®šå­—å¹•ç›®å½•
        date_str = datetime.now().strftime('%Y-%m-%d')
        if args.csv:
            date_str = csv_path.stem.replace('homepage_videos_', '')
        subtitle_dir = SUBTITLE_OUTPUT / f"homepage_{date_str}"

        # ç”ŸæˆAIåˆ†ææŠ¥å‘Š
        generate_ai_analysis_report(csv_path, subtitle_dir, args.model)
    elif args.mode == 'summary-only':
        # ä»…AIæ‘˜è¦æ¨¡å¼
        date_str = csv_path.stem.replace('homepage_videos_', '')
        subtitle_dir = SUBTITLE_OUTPUT / f"homepage_{date_str}"
        generate_ai_analysis_report(csv_path, subtitle_dir, args.model)

    print()
    print("=" * 70)
    print("âœ… å®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    if args.mode != 'summary-only' and not args.csv:
        print(f"  - CSV: {csv_path}")
        print(f"  - JSON: {json_path}")

    # æ˜¾ç¤ºAIåˆ†ææŠ¥å‘Šè·¯å¾„
    if args.mode == 'full':
        date_str = datetime.now().strftime('%Y-%m-%d')
        if args.csv:
            date_str = csv_path.stem.replace('homepage_videos_', '')
        report_path = SUBTITLE_OUTPUT / f"homepage_{date_str}_AIæ€»ç»“.md"
        print(f"  - AIåˆ†ææŠ¥å‘Š: {report_path}")
    elif args.mode == 'summary-only':
        date_str = csv_path.stem.replace('homepage_videos_', '')
        report_path = SUBTITLE_OUTPUT / f"homepage_{date_str}_AIæ€»ç»“.md"
        print(f"  - AIåˆ†ææŠ¥å‘Š: {report_path}")
    print()
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
