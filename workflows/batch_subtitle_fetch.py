"""
æ‰¹é‡æå–Bç«™å­—å¹•å¹¶æ›´æ–°CSVçŠ¶æ€
åŸºäº test_bilibili_api.py çš„å­—å¹•è·å–åŠŸèƒ½

ä½¿ç”¨ç¤ºä¾‹:
    python batch_subtitle_fetch.py "MediaCrawler/bilibili_videos_output/å°å¤©fotos.csv"
"""

import asyncio
import json
import aiohttp
import csv
import sys
import re
import time
from pathlib import Path
from bilibili_api import video, Credential
from datetime import datetime

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# ============= é…ç½®åŒº =============
# Cookie æ–‡ä»¶è·¯å¾„ï¼ˆä» utils/ å›åˆ°çˆ¶ç›®å½•çš„ config/ï¼‰
COOKIE_FILE = Path(__file__).parent.parent / "config" / "cookies_bilibili_api.txt"
# å­—å¹•è¾“å‡ºç›®å½•ï¼ˆä» utils/ å›åˆ°çˆ¶ç›®å½•çš„ output/ï¼‰
SUBTITLE_OUTPUT_DIR = Path(__file__).parent.parent / "output" / "subtitles"
# =================================


def load_cookies(cookie_file: Path) -> dict:
    """ä» cookie æ–‡ä»¶åŠ è½½ cookies"""
    cookies = {}
    if not cookie_file.exists():
        print(f"è­¦å‘Š: Cookie æ–‡ä»¶ä¸å­˜åœ¨: {cookie_file}")
        return cookies

    with open(cookie_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue

            if "\t" in line:
                parts = line.split("\t")
                if len(parts) >= 7:
                    cookies[parts[5].strip()] = parts[6].strip()
            elif "=" in line:
                parts = line.split("=", 1)
                if len(parts) == 2:
                    cookies[parts[0]] = parts[1].rstrip(";").strip()

    return cookies


def get_credential() -> Credential:
    """è·å–è®¤è¯å‡­æ®"""
    cookies = load_cookies(COOKIE_FILE)
    sessdata = cookies.get("SESSDATA", "")
    bili_jct = cookies.get("bili_jct", "")
    buvid3 = cookies.get("buvid3", "")

    if not sessdata:
        print("é”™è¯¯: æœªæ‰¾åˆ° SESSDATA")
        print(f"è¯·æ£€æŸ¥ {COOKIE_FILE}")
        return None

    return Credential(
        sessdata=sessdata,
        bili_jct=bili_jct,
        buvid3=buvid3
    )


def extract_bvid(input_str: str) -> str:
    """ä»è¾“å…¥ä¸­æå– BV å·"""
    match = re.search(r'BV[\w]+', input_str, re.IGNORECASE)
    if match:
        return match.group(0)
    return input_str


def format_srt_time(seconds: float) -> str:
    """å°†ç§’æ•°è½¬æ¢ä¸º SRT æ—¶é—´ç æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


async def fetch_subtitle_srt(bvid: str, title: str, author_dir: Path) -> dict:
    """
    è·å–å•ä¸ªè§†é¢‘çš„ SRT å­—å¹•

    è¿”å›:
        {
            'success': bool,
            'srt_path': str or None,
            'error': str or None,
            'fallback_needed': bool  # æ˜¯å¦éœ€è¦ä½¿ç”¨è§†é¢‘ä¸‹è½½+Geminiåˆ†æä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        }
    """
    result = {'success': False, 'srt_path': None, 'error': None, 'fallback_needed': False}

    try:
        v = video.Video(bvid=bvid, credential=get_credential())
        author_dir.mkdir(parents=True, exist_ok=True)

        # è·å–è§†é¢‘ä¿¡æ¯
        info = await v.get_info()
        cid = info["cid"]

        # è·å–å­—å¹•åˆ—è¡¨
        player_info = await v.get_player_info(cid=cid)
        subtitles = player_info.get("subtitle", {}).get("subtitles", [])

        if not subtitles:
            result['error'] = 'è¯¥è§†é¢‘æ— å­—å¹•'
            result['fallback_needed'] = True  # æ ‡è®°éœ€è¦ä½¿ç”¨è§†é¢‘ä¸‹è½½+Geminiåˆ†æå¤‡é€‰æ–¹æ¡ˆ
            return result

        # ä¸‹è½½ç¬¬ä¸€æ¡å­—å¹•ï¼ˆé€šå¸¸æ˜¯ä¸­æ–‡ï¼‰
        sub = subtitles[0]
        url = "https:" + sub["subtitle_url"]
        lan = sub['lan']

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                data = await resp.json(content_type=None)

        # æ¸…ç†æ–‡ä»¶å
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
        base_name = f"{safe_title}_{lan}"
        srt_path = author_dir / f"{base_name}.srt"

        # ä¿å­˜ SRT
        with open(srt_path, "w", encoding="utf-8") as f:
            for i, item in enumerate(data.get("body", []), 1):
                start_time = format_srt_time(item['from'])
                end_time = format_srt_time(item['to'])
                f.write(f"{i}\n{start_time} --> {end_time}\n{item['content']}\n\n")

        result['success'] = True
        result['srt_path'] = str(srt_path)

    except Exception as e:
        result['error'] = str(e)

    return result


def read_csv_videos(csv_path: Path) -> list:
    """è¯»å– CSV æ–‡ä»¶ï¼Œè¿”å›è§†é¢‘åˆ—è¡¨"""
    videos = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            videos.append(row)

    return videos


def write_csv_status(csv_path: Path, videos: list):
    """å†™å› CSV æ–‡ä»¶ï¼Œæ›´æ–° subtitle_status å’Œ subtitle_error"""
    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        fieldnames = list(videos[0].keys())
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(videos)


def generate_summary_md(videos: list, author_name: str, output_dir: Path, total_elapsed: float, csv_path: Path = None) -> Path:
    """ç”Ÿæˆæ±‡æ€» MD æ–‡ä»¶"""
    md_path = output_dir / f"{author_name}_æ±‡æ€».md"

    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# {author_name} è§†é¢‘å­—å¹•æ±‡æ€»\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æ€»è§†é¢‘æ•°**: {len(videos)}\n\n")
        f.write(f"**å¤„ç†è€—æ—¶**: {total_elapsed:.2f}ç§’\n\n")
        f.write("---\n\n")

        # è¡¨æ ¼æ ‡é¢˜
        f.write("## è§†é¢‘åˆ—è¡¨\n\n")
        f.write("| åºå· | æ ‡é¢˜ | é“¾æ¥ | BVå· | æ—¶é•¿ | æ’­æ”¾é‡ | è¯„è®ºæ•° | å‘å¸ƒæ—¶é—´ | å­—å¹•çŠ¶æ€ |\n")
        f.write("|:----:|------|------|:----:|:----:|:------:|:------:|:--------:|:--------:|\n")

        for video_data in videos:
            seq = video_data.get('åºå·', '')
            title = video_data.get('æ ‡é¢˜', '').replace('|', '\\|')
            link = video_data.get('é“¾æ¥', '')
            bvid = video_data.get('BVå·', '')
            duration = video_data.get('æ—¶é•¿', '')
            views = video_data.get('æ’­æ”¾é‡', '')
            comments = video_data.get('è¯„è®ºæ•°', '')
            pub_time = video_data.get('å‘å¸ƒæ—¶é—´', '')
            status = video_data.get('subtitle_status', '')

            # çŠ¶æ€å›¾æ ‡
            status_icon = 'âœ…' if status == 'success' else ('âŒ' if status == 'failed' else 'â³')

            f.write(f"| {seq} | {title} | [é“¾æ¥]({link}) | {bvid} | {duration} | {views} | {comments} | {pub_time} | {status_icon} |\n")

        f.write("\n---\n\n")

        # ç»Ÿè®¡ä¿¡æ¯
        success_count = sum(1 for v in videos if v.get('subtitle_status') == 'success')
        fail_count = sum(1 for v in videos if v.get('subtitle_status') == 'failed')
        pending_count = len(videos) - success_count - fail_count
        fallback_needed_count = sum(1 for v in videos if v.get('fallback_needed', False))

        f.write("## ç»Ÿè®¡\n\n")
        f.write(f"- âœ… æˆåŠŸæå–: {success_count}\n")
        f.write(f"- âŒ æå–å¤±è´¥: {fail_count}\n")
        f.write(f"- â³ æœªå¤„ç†/è·³è¿‡: {pending_count}\n")

        if fallback_needed_count > 0:
            f.write(f"- ğŸ¬ éœ€è¦è§†é¢‘åˆ†æå¤‡é€‰æ–¹æ¡ˆ: {fallback_needed_count}\n")
            if csv_path:
                # æ”¯æŒ Path å’Œ string ä¸¤ç§ç±»å‹
                csv_name = csv_path.name if hasattr(csv_path, 'name') else Path(csv_path).name
                f.write(f"\nğŸ’¡ æç¤º: å¯ä»¥è¿è¡Œ `python workflows/auto_bili_workflow.py --csv \"{csv_name}\" --enable-fallback` æ¥å¤„ç†æ— å­—å¹•è§†é¢‘\n")
            else:
                f.write(f"\nğŸ’¡ æç¤º: å¯ä»¥è¿è¡Œ `python workflows/auto_bili_workflow.py --csv \"your_csv_file.csv\" --enable-fallback` æ¥å¤„ç†æ— å­—å¹•è§†é¢‘\n")

        # å¤±è´¥åˆ—è¡¨
        failed_videos = [v for v in videos if v.get('subtitle_status') == 'failed']
        if failed_videos:
            f.write("\n## å¤±è´¥è§†é¢‘åˆ—è¡¨\n\n")
            for v in failed_videos:
                f.write(f"- **{v.get('æ ‡é¢˜', '')}**: {v.get('subtitle_error', 'æœªçŸ¥é”™è¯¯')}\n")

    return md_path


async def process_batch(csv_path: str, limit: int = None, force: bool = False):
    """æ‰¹é‡å¤„ç†è§†é¢‘å­—å¹•æå–

    Args:
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        limit: é™åˆ¶å¤„ç†è§†é¢‘æ•°é‡ï¼ˆé»˜è®¤ï¼šå¤„ç†å…¨éƒ¨ï¼‰
        force: å¼ºåˆ¶é‡æ–°è·å–ï¼Œä¸è·³è¿‡å·²æˆåŠŸçš„è§†é¢‘
    """
    csv_file = Path(csv_path)

    if not csv_file.exists():
        print(f"é”™è¯¯: CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    # ä» CSV æ–‡ä»¶åæå–ä½œè€…å
    author_name = csv_file.stem  # å»æ‰è·¯å¾„å’Œåç¼€ï¼Œå¦‚ "å°å¤©fotos.csv" -> "å°å¤©fotos"
    print(f"ä½œè€…: {author_name}")

    print(f"è¯»å– CSV æ–‡ä»¶: {csv_path}")
    videos = read_csv_videos(csv_file)

    if not videos:
        print("é”™è¯¯: CSV æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
        return

    # åº”ç”¨é™åˆ¶
    if limit and limit < len(videos):
        videos = videos[:limit]
        print(f"é™åˆ¶å¤„ç†æ•°é‡: {limit}")

    print(f"æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
    print("=" * 60)

    # ç»Ÿè®¡
    success_count = 0
    fail_count = 0
    skip_count = 0

    # æ€»è®¡æ—¶
    total_start_time = time.time()

    # åˆ›å»ºä½œè€…ä¸“å±ç›®å½•
    author_dir = SUBTITLE_OUTPUT_DIR / author_name
    author_dir.mkdir(parents=True, exist_ok=True)
    print(f"å­—å¹•ä¿å­˜ç›®å½•: {author_dir}")

    for i, video_data in enumerate(videos, 1):
        bvid = video_data.get('BVå·', '')
        title = video_data.get('æ ‡é¢˜', '')
        link = video_data.get('é“¾æ¥', '')

        if not bvid:
            print(f"\n[{i}/{len(videos)}] è·³è¿‡: BVå·ä¸ºç©º")
            skip_count += 1
            continue

        # å¦‚æœå·²ç»æœ‰æˆåŠŸçŠ¶æ€ï¼Œå¯ä»¥é€‰æ‹©è·³è¿‡ï¼ˆé™¤é force=Trueï¼‰
        current_status = video_data.get('subtitle_status', '').strip()
        if current_status == 'success' and not force:
            print(f"\n[{i}/{len(videos)}] è·³è¿‡: {title[:30]}... (å·²æˆåŠŸ)")
            skip_count += 1
            continue

        # å•ä¸ªè§†é¢‘è®¡æ—¶
        video_start_time = time.time()

        print(f"\n[{i}/{len(videos)}] æ­£åœ¨å¤„ç†: {title}")
        print(f"  BV: {bvid}")

        # è·å–å­—å¹•
        result = await fetch_subtitle_srt(bvid, title, author_dir)

        # è®¡ç®—è€—æ—¶
        elapsed = time.time() - video_start_time

        if result['success']:
            print(f"  âœ… æˆåŠŸ | è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"  ä¿å­˜è·¯å¾„: {result['srt_path']}")
            video_data['subtitle_status'] = 'success'
            video_data['subtitle_error'] = ''
            success_count += 1
        else:
            print(f"  âŒ å¤±è´¥ | è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"  é”™è¯¯åŸå› : {result['error']}")
            video_data['subtitle_status'] = 'failed'
            video_data['subtitle_error'] = result['error']

            # æ·»åŠ fallbackè·Ÿè¸ªå­—æ®µ
            if result.get('fallback_needed', False):
                video_data['fallback_needed'] = True
                video_data['fallback_status'] = 'pending'
                print(f"  ğŸ’¡ å°†ä½¿ç”¨è§†é¢‘ä¸‹è½½+Geminiåˆ†æä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")

            fail_count += 1

        # æ€»è¿›åº¦
        total_elapsed = time.time() - total_start_time
        print(f"  æ€»è¿›åº¦è€—æ—¶: {total_elapsed:.2f}ç§’")

        # æ¯å¤„ç† 5 ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡è¿›åº¦
        if i % 5 == 0:
            write_csv_status(csv_file, videos)
            print(f"  [è¿›åº¦å·²ä¿å­˜]")

    # æœ€ç»ˆä¿å­˜
    write_csv_status(csv_file, videos)

    # æ€»è€—æ—¶
    total_elapsed = time.time() - total_start_time

    # ç”Ÿæˆ MD æ±‡æ€»æ–‡ä»¶
    md_path = generate_summary_md(videos, author_name, SUBTITLE_OUTPUT_DIR, total_elapsed, csv_file)

    print("\n" + "=" * 60)
    print(f"æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"  æˆåŠŸ: {success_count}")
    print(f"  å¤±è´¥: {fail_count}")
    print(f"  è·³è¿‡: {skip_count}")
    print(f"  æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
    print(f"  å­—å¹•ä¿å­˜ç›®å½•: {author_dir}")
    print(f"  æ±‡æ€»æ–‡ä»¶: {md_path}")


async def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="æ‰¹é‡æå–Bç«™å­—å¹•å¹¶æ›´æ–°CSVçŠ¶æ€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python batch_subtitle_fetch.py "data/videos.csv"
  python batch_subtitle_fetch.py "data/videos.csv" --limit 100
  python batch_subtitle_fetch.py "data/videos.csv" --limit 50 --force
        """
    )
    parser.add_argument("csv_file", help="CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--limit", "-l", type=int, default=None,
                        help="é™åˆ¶å¤„ç†è§†é¢‘æ•°é‡ï¼ˆé»˜è®¤ï¼šå¤„ç†å…¨éƒ¨ï¼‰")
    parser.add_argument("--force", "-f", action="store_true",
                        help="å¼ºåˆ¶é‡æ–°è·å–ï¼Œè·³è¿‡å·²æˆåŠŸçš„è§†é¢‘")

    args = parser.parse_args()

    try:
        await process_batch(args.csv_file, limit=args.limit, force=args.force)
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
