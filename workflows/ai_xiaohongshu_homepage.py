#!/usr/bin/env python3
"""
AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èå¹¶æ€»ç»“ï¼ˆå®Œæ•´ç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. åˆ·æ–°å°çº¢ä¹¦æ¨èé¡µï¼ˆè‡ªå®šä¹‰æ¬¡æ•°ï¼‰
2. é‡‡é›†æ¨èå†…å®¹ï¼ˆè§†é¢‘/å›¾æ–‡ã€ä½œè€…ä¿¡æ¯ï¼‰
3. å¯¼å‡ºCSV
4. AIç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
    python ai_xiaohongshu_homepage.py

    # ä»…é‡‡é›†ï¼ˆä¸ç”ŸæˆAIæŠ¥å‘Šï¼‰
    python ai_xiaohongshu_homepage.py --mode scrape

    # å®Œæ•´æµç¨‹ï¼ˆé‡‡é›†+AIåˆ†æï¼‰
    python ai_xiaohongshu_homepage.py --mode full
"""

import argparse
import asyncio
import sys
import csv
import re
import os
from pathlib import Path
from datetime import datetime

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from playwright.async_api import async_playwright

# ==================== è·¯å¾„é…ç½® ====================
# ä½¿ç”¨æ ¹ç›®å½•ä½œä¸ºé¡¹ç›®ç›®å½•ï¼ˆæ— è®ºè„šæœ¬åœ¨å“ªä¸ªå­ç›®å½•è¿è¡Œï¼‰
PROJECT_DIR = Path(__file__).parent.parent  # è·å–æ ¹ç›®å½•
OUTPUT_DIR = PROJECT_DIR / "output" / "xiaohongshu_homepage"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==================== Cookie è¯»å– ====================
def read_xhs_cookie():
    """ä» config/cookies.txt è¯»å–å°çº¢ä¹¦Cookie"""
    cookie_file = PROJECT_DIR / "config" / "cookies.txt"
    if not cookie_file.exists():
        print("âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: config/cookies.txt")
        return ""

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾ xiaohongshu_full= æ ¼å¼
    match = re.search(r'xiaohongshu_full=([^\n]+)', content)
    if match:
        return match.group(1)

    # æŸ¥æ‰¾ [xiaohongshu] éƒ¨åˆ†
    xhs_section = re.search(r'\[xiaohongshu\](.*?)\[', content, re.DOTALL)
    if xhs_section:
        section = xhs_section.group(1)
        cookies = []
        for line in section.split('\n'):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                cookies.append(f"{key.strip()}={value.strip()}")
        return '; '.join(cookies)

    return ""


# ==================== Playwright é‡‡é›† ====================
async def scrape_xiaohongshu_homepage(
    refresh_count: int = 3,
    max_notes: int = 50,
    cookie: str = ""
) -> list:
    """ä½¿ç”¨Playwrightçˆ¬å–å°çº¢ä¹¦æ¨èé¡µ"""
    notes_collected = []
    seen_urls = set()

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )

        # è®¾ç½®Cookie
        if cookie:
            try:
                cookies = []
                for item in cookie.split(';'):
                    item = item.strip()
                    if '=' in item:
                        key, value = item.split('=', 1)
                        cookies.append({
                            'name': key,
                            'value': value,
                            'domain': '.xiaohongshu.com',
                            'path': '/'
                        })
                await context.add_cookies(cookies)
                print("âœ… Cookieå·²è®¾ç½®")
            except Exception as e:
                print(f"âš ï¸  Cookieè®¾ç½®å¤±è´¥: {e}")

        page = await context.new_page()

        print(f"\nğŸ“¡ è®¿é—®å°çº¢ä¹¦é¦–é¡µ...")
        try:
            await page.goto('https://www.xiaohongshu.com/', wait_until='domcontentloaded', timeout=60000)
            await asyncio.sleep(5)
        except Exception as e:
            print(f"âš ï¸  é¡µé¢åŠ è½½é—®é¢˜: {e}")
            print("ğŸ’¡ æµè§ˆå™¨å·²æ‰“å¼€ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        page_content = await page.content()
        if 'ç™»å½•' in page_content and 'æ³¨å†Œ' in page_content:
            print("\nâš ï¸  æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€")
            print("ğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•")
            print("â³ ç­‰å¾…90ç§’...ç™»å½•å®Œæˆåä¼šè‡ªåŠ¨ç»§ç»­")
            await asyncio.sleep(90)
            print("âœ… ç»§ç»­æ‰§è¡Œ...")

        print(f"\nğŸ”„ å¼€å§‹é‡‡é›†æ¨èå†…å®¹ï¼ˆåˆ·æ–°{refresh_count}æ¬¡ï¼‰...")

        for i in range(refresh_count):
            print(f"\n  åˆ·æ–° {i+1}/{refresh_count}")

            # æ»šåŠ¨åŠ è½½
            for scroll in range(10):
                await page.evaluate('window.scrollBy(0, window.innerHeight)')
                await asyncio.sleep(1)

            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(2)

            # è·å–æ‰€æœ‰é“¾æ¥å’Œä¿¡æ¯ï¼ˆæœ€ç»ˆç‰ˆï¼‰
            try:
                notes_data = await page.evaluate('''
                    () => {
                        const notes = [];
                        const seen = new Set();

                        // æŸ¥æ‰¾æ‰€æœ‰ç¬”è®°å¡ç‰‡ï¼ˆä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼‰
                        const cards = document.querySelectorAll('section, article, [class*="note"], [class*="card"], div[class*="item"]');

                        cards.forEach(card => {
                            // ç›´æ¥æŸ¥æ‰¾å¸¦ xsec_token çš„é“¾æ¥
                            const link = card.querySelector('a[href*="xsec_token"]');
                            if (!link) return;

                            const url = link.href;

                            // ä» URL ä¸­æå– xsec_token å’Œ xsec_source
                            let xsecToken = '';
                            let xsecSource = 'pc_homepage';  // é»˜è®¤æ¥æºä¸ºé¦–é¡µ

                            try {
                                const urlParams = new URLSearchParams(url.split('?')[1]);
                                xsecToken = urlParams.get('xsec_token') || '';
                                if (urlParams.get('xsec_source')) {
                                    xsecSource = urlParams.get('xsec_source');
                                }
                            } catch (e) {
                                // URL è§£æå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç©ºå€¼
                            }

                            // æå–ç¬”è®°ID
                            let noteId = "";
                            if (url.includes('/explore/')) {
                                const idMatch = url.match(/\\/explore\\/([a-f0-9]{24})/);
                                if (idMatch) noteId = idMatch[1];
                            } else if (url.includes('/discovery/item/')) {
                                const idMatch = url.match(/\\/discovery\\/item\\/([a-f0-9]{24})/);
                                if (idMatch) noteId = idMatch[1];
                            }

                            if (!noteId) return;
                            if (seen.has(noteId)) return;
                            seen.add(noteId);

                            // è·å–æ ‡é¢˜ï¼ˆä½¿ç”¨å¤šç§æ–¹æ³•ï¼‰
                            let title = "æ— æ ‡é¢˜";

                            // æ–¹æ³•1: æŸ¥æ‰¾spanæˆ–divä¸­çš„æ–‡æœ¬
                            const textNodes = card.querySelectorAll('span, div, p, h1, h2, h3');
                            for (const node of textNodes) {
                                const text = node.textContent?.trim();
                                // æ ‡é¢˜ç‰¹å¾ï¼š3-100å­—ç¬¦ï¼Œä¸å«æ•°å­—åºå·
                                if (text && text.length > 3 && text.length < 100 && !/^\\d+$/.test(text)) {
                                    // æ’é™¤æ˜æ˜¾ä¸æ˜¯æ ‡é¢˜çš„å†…å®¹
                                    if (!text.includes('èµ') && !text.includes('å…³æ³¨') &&
                                        !text.includes('åˆ†äº«') && !text.includes('æ”¶è—')) {
                                        title = text.substring(0, 100);
                                        break;
                                    }
                                }
                            }

                            // æ–¹æ³•2: ä»é“¾æ¥çš„titleå±æ€§è·å–
                            if (title === "æ— æ ‡é¢˜") {
                                const linkTitle = link.getAttribute('title');
                                if (linkTitle && linkTitle.length > 3) {
                                    title = linkTitle.substring(0, 100);
                                }
                            }

                            // è·å–ä½œè€…
                            let author = "æœªçŸ¥ä½œè€…";
                            const authorNodes = card.querySelectorAll('span, a');
                            for (const node of authorNodes) {
                                const text = node.textContent?.trim();
                                // ä½œè€…ç‰¹å¾ï¼š1-30å­—ç¬¦ï¼Œå¯èƒ½æ˜¯äººå
                                if (text && text.length > 1 && text.length < 30) {
                                    // æ’é™¤åŒ…å«æ•°å­—çš„ï¼ˆå¯èƒ½æ˜¯ç‚¹èµæ•°ï¼‰
                                    if (!/\\d/.test(text)) {
                                        author = text;
                                        break;
                                    }
                                }
                            }

                            // è·å–ç‚¹èµæ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
                            let likes = "0";
                            const allNodes = card.querySelectorAll('*');
                            for (const node of allNodes) {
                                const text = node.textContent?.trim();
                                // æŸ¥æ‰¾åŒ…å«æ•°å­—çš„èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯ç‚¹èµæ•°ï¼‰
                                if (text && /^\\d+/.test(text)) {
                                    // éªŒè¯çˆ¶å…ƒç´ æ˜¯å¦æœ‰likeã€countç­‰class
                                    const parentClass = node.parentElement?.className || '';
                                    if (parentClass.includes('like') || parentClass.includes('count') ||
                                        parentClass.includes('interact')) {
                                        // æ’é™¤æ˜æ˜¾è¿‡å¤§çš„æ•°å­—
                                        const num = parseInt(text);
                                        if (num < 1000000 && num > 0) {
                                            likes = text;
                                            break;
                                        }
                                    }
                                }
                            }

                            // åˆ¤æ–­ç±»å‹ï¼ˆæœ€ç»ˆç‰ˆï¼‰
                            let type = 'image';

                            // æ–¹æ³•1: æ£€æŸ¥videoæ ‡ç­¾
                            const hasVideo = card.querySelector('video');
                            if (hasVideo) {
                                type = 'video';
                            } else {
                                // æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦æœ‰æ’­æ”¾å›¾æ ‡æˆ–æ—¶é•¿æ ‡è®°
                                const hasPlayIcon = card.querySelector('[class*="play"], [class*="video"], svg[class*="play"]');
                                const hasDuration = card.textContent.includes(':') && card.textContent.match(/\\d+:\\d+/);
                                if (hasPlayIcon || hasDuration) {
                                    type = 'video';
                                }
                            }

                            notes.push({
                                url: url,
                                noteId: noteId,
                                title: title,
                                author: author,
                                likes: likes,
                                type: type,
                                xsecToken: xsecToken,      // æ–°å¢å­—æ®µ
                                xsecSource: xsecSource     // æ–°å¢å­—æ®µ
                            });
                        });

                        return notes;
                    }
                ''')

                print(f"    æ‰¾åˆ° {len(notes_data)} ä¸ªç¬”è®°")

                for note in notes_data:
                    note_id = note['noteId']

                    # å»é‡
                    if note_id in seen_urls:
                        continue
                    seen_urls.add(note_id)

                    note_data = {
                        'åºå·': len(notes_collected) + 1,
                        'æ ‡é¢˜': note['title'],
                        'é“¾æ¥': note['url'],  # å°†åœ¨ä¸‹é¢é‡æ„
                        'å®Œæ•´é“¾æ¥': '',       # æ–°å¢å­—æ®µ
                        'ç¬”è®°ID': note_id,
                        'ä½œè€…': note['author'],
                        'ç‚¹èµæ•°': note['likes'],
                        'ç±»å‹': note['type'],
                        'xsec_token': note.get('xsecToken', ''),  # æ–°å¢å­—æ®µ
                        'xsec_source': note.get('xsecSource', 'pc_homepage'),  # æ–°å¢å­—æ®µ
                        'çˆ¬å–æ‰¹æ¬¡': i + 1,     # æ–°å¢å­—æ®µï¼šç¬¬å‡ æ¬¡çˆ¬å–
                        'é‡‡é›†æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    # é‡æ„å®Œæ•´é“¾æ¥ï¼ˆåŒ…å« xsec_tokenï¼‰
                    xsec_token = note.get('xsecToken', '')
                    xsec_source = note.get('xsecSource', 'pc_homepage')
                    if xsec_token:
                        full_url = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source={xsec_source}"
                        note_data['å®Œæ•´é“¾æ¥'] = full_url
                        note_data['é“¾æ¥'] = full_url  # æ›´æ–°é“¾æ¥å­—æ®µä¸ºå®Œæ•´é“¾æ¥
                    else:
                        note_data['å®Œæ•´é“¾æ¥'] = note['url']

                    notes_collected.append(note_data)
                    type_emoji = 'ğŸ¬' if note['type'] == 'video' else 'ğŸ–¼ï¸'
                    print(f"    âœ“ [{len(notes_collected)}] {type_emoji} {note['title']} | {note['author']} | {note['likes']}èµ")

                    if len(notes_collected) >= max_notes:
                        break

            except Exception as e:
                print(f"    âš ï¸  è§£æå‡ºé”™: {e}")

            # åˆ·æ–°
            if i < refresh_count - 1:
                print("    åˆ·æ–°é¡µé¢...")
                await page.reload(wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(3)

        await browser.close()

    print(f"\nâœ… é‡‡é›†å®Œæˆï¼å…±è·å– {len(notes_collected)} ä¸ªç¬”è®°")
    return notes_collected


# ==================== CSVå¯¼å‡º ====================
def export_to_csv(notes, output_path):
    """å¯¼å‡ºç¬”è®°åˆ°CSVï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
    csv_columns = ['åºå·', 'æ ‡é¢˜', 'é“¾æ¥', 'å®Œæ•´é“¾æ¥', 'ç¬”è®°ID', 'ä½œè€…', 'ç‚¹èµæ•°', 'ç±»å‹', 'xsec_token', 'xsec_source', 'çˆ¬å–æ‰¹æ¬¡', 'é‡‡é›†æ—¶é—´']

    # è¯»å–ç°æœ‰æ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
    existing_notes = []
    if output_path.exists():
        with open(output_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            existing_notes = list(reader)

    # åˆå¹¶æ•°æ®
    all_notes = existing_notes + notes

    # é‡æ–°ç¼–å·
    for i, note in enumerate(all_notes, 1):
        note['åºå·'] = i

    # å†™å…¥æ‰€æœ‰æ•°æ®
    with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=csv_columns)
        writer.writeheader()
        writer.writerows(all_notes)

    print(f"ğŸ“ CSVå·²æ›´æ–°: {output_path}")
    print(f"   ç°æœ‰è®°å½•: {len(existing_notes)} æ¡")
    print(f"   æ–°å¢è®°å½•: {len(notes)} æ¡")
    print(f"   æ€»è®¡: {len(all_notes)} æ¡")


# ==================== AIåˆ†æ ====================
def generate_ai_report(notes, output_dir):
    """ä½¿ç”¨Geminiç”ŸæˆAIæŠ¥å‘Š"""
    import json

    print("\nğŸ¤– å¼€å§‹AIåˆ†æ...")

    # æ£€æŸ¥Geminiæ˜¯å¦å¯ç”¨
    _gemini_available = False
    use_new_sdk = False

    try:
        from google import genai as genai_new
        genai = genai_new
        use_new_sdk = True
        _gemini_available = True
    except ImportError:
        try:
            import google.generativeai as genai_old
            genai = genai_old
            use_new_sdk = False
            _gemini_available = True
        except:
            pass

    if not _gemini_available:
        print("âš ï¸  æœªå®‰è£…google-generativeaiï¼Œè·³è¿‡AIåˆ†æ")
        print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install google-generativeai")
        return

    # é…ç½®APIï¼ˆä¿®æ­£æ‹¼å†™ï¼šGEMINI -> GEMINIï¼‰
    api_key = os.environ.get('GEMINI_API_KEY', '')
    config_file = PROJECT_DIR / "config" / "bot_config.json"
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            if 'gemini_api_key' in config:
                api_key = config['gemini_api_key']
        except:
            pass

    if not api_key:
        print("âš ï¸  æœªè®¾ç½®GEMINI_API_KEY")
        print("ğŸ’¡ è®¾ç½®: set GEMINI_API_KEY=your_key_here")
        return

    # æ ¹æ®SDKç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„API
    if use_new_sdk:
        # æ–° SDK (google-genai)
        client = genai.Client(api_key=api_key)
    else:
        # æ—§ SDK (google.generativeai)
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')

    # ç”ŸæˆæŠ¥å‘Š
    notes_text = "\n\n".join([
        f"{i+1}. {note.get('title', 'æ— æ ‡é¢˜')}\n"
        f"   ä½œè€…: {note.get('author', 'æœªçŸ¥')}\n"
        f"   ç±»å‹: {note.get('type', 'æœªçŸ¥')}\n"
        f"   ç‚¹èµ: {note.get('likes', '0')}\n"
        f"   é“¾æ¥: {note.get('url', '')}\n"
        for i, note in enumerate(notes[:15])  # åªåˆ†æå‰15ä¸ª
    ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦æ¨èå†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¶‹åŠ¿æŠ¥å‘Šã€‚

å°çº¢ä¹¦æ¨èå†…å®¹ï¼š
{notes_text}

è¯·ç”Ÿæˆä»¥ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š

## ğŸ“Š å°çº¢ä¹¦æ¨èè¶‹åŠ¿åˆ†æ

### ğŸ¯ å†…å®¹æ¦‚è§ˆ
- é‡‡é›†ç¬”è®°æ•°ï¼š{len(notes)}ç¯‡
- è§†é¢‘å æ¯”ï¼š{notes.count(lambda x: x['type'] == 'video')}ç¯‡ ({notes.count(lambda x: x['type'] == 'video')/len(notes)*100:.1f}%)
- å›¾æ–‡å æ¯”ï¼š{notes.count(lambda x: x['type'] == 'image')}ç¯‡ ({notes.count(lambda x: x['type'] == 'image')/len(notes)*100:.1f}%)

### ğŸ”¥ çƒ­é—¨ä¸»é¢˜ï¼ˆTop 5ï¼‰
æå–æœ€å—æ¬¢è¿çš„5ä¸ªä¸»é¢˜

### ğŸ‘¥ çƒ­é—¨ä½œè€…ï¼ˆTop 5ï¼‰
åˆ—ä¸¾å‘å¸ƒæœ€å¤šå†…å®¹çš„5ä¸ªä½œè€…

### ğŸ“ˆ è¶‹åŠ¿åˆ†æ
åˆ†æå½“å‰å°çº¢ä¹¦æ¨èçš„å†…å®¹è¶‹åŠ¿ï¼ŒåŒ…æ‹¬ï¼š
- çƒ­é—¨è¯é¢˜
- å†…å®¹åå¥½
- å—æ¬¢è¿çš„å†…å®¹ç±»å‹

### ğŸ’ å€¼å¾—å…³æ³¨çš„ç¬”è®°
æ¨è3-5ä¸ªå€¼å¾—æ·±å…¥é˜…è¯»çš„ç¬”è®°ï¼ˆé™„é“¾æ¥ï¼‰

è¯·ç¡®ä¿æŠ¥å‘Šç»“æ„å®Œæ•´ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å®è´¨å†…å®¹ã€‚"""

    try:
        if use_new_sdk:
            # æ–° SDK è°ƒç”¨æ–¹å¼
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt
            )
            report = response.text if hasattr(response, 'text') and response.text else str(response)
        else:
            # æ—§ SDK è°ƒç”¨æ–¹å¼
            response = model.generate_content(prompt)
            report = response.text if hasattr(response, 'text') and response.text else "ç”Ÿæˆå¤±è´¥"

        # ä¿å­˜æŠ¥å‘Š
        date_str = datetime.now().strftime('%Y-%m-%d')
        report_path = output_dir / f"xiaohongshu_homepage_{date_str}_AIæŠ¥å‘Š.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“ AIæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ‘˜è¦
        lines = report.split('\n')
        print("\n" + "=" * 70)
        print("  ğŸ“– AIåˆ†ææŠ¥å‘Š")
        print("=" * 70)
        for line in lines[:20]:
            print(line)
        if len(lines) > 20:
            print(f"  ... è¿˜æœ‰ {len(lines) - 20} è¡Œ")
        print("=" * 70)

    except Exception as e:
        print(f"âŒ AIåˆ†æå¤±è´¥: {e}")


# ==================== ä¸»ç¨‹åº ====================
async def main():
    parser = argparse.ArgumentParser(description='AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èå¹¶æ€»ç»“')

    parser.add_argument('--refresh-count', type=int, default=3,
                       help='åˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('--max-notes', type=int, default=50,
                       help='æœ€å¤šé‡‡é›†ç¬”è®°æ•°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--mode', type=str, default='scrape',
                       choices=['scrape', 'full'],
                       help='æ¨¡å¼: scrape=ä»…é‡‡é›†, full=é‡‡é›†+AIåˆ†æ')

    args = parser.parse_args()

    print(f"\n{'='*70}")
    print(f"  AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨è")
    print(f"{'='*70}")
    print(f"\nğŸ“Š é…ç½®:")
    print(f"  â€¢ åˆ·æ–°æ¬¡æ•°: {args.refresh_count}")
    print(f"  â€¢ æœ€å¤šç¬”è®°: {args.max_notes}")
    print(f"  â€¢ åˆ†ææ¨¡å¼: {args.mode}")

    # è¯»å–Cookie
    cookie = read_xhs_cookie()
    if not cookie:
        print("\nâš ï¸  æœªæ‰¾åˆ°Cookieï¼Œå°†ä½¿ç”¨æ— Cookieæ¨¡å¼ï¼ˆéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰")

    # é‡‡é›†æ•°æ®
    notes = await scrape_xiaohongshu_homepage(
        refresh_count=args.refresh_count,
        max_notes=args.max_notes,
        cookie=cookie
    )

    if not notes:
        print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•ç¬”è®°")
        return

    # å¯¼å‡ºCSV
    date_str = datetime.now().strftime('%Y-%m-%d')
    csv_path = OUTPUT_DIR / f"xiaohongshu_homepage_{date_str}.csv"
    export_to_csv(notes, csv_path)

    # AIåˆ†æ
    if args.mode == 'full':
        print(f"\nğŸ“Š å‡†å¤‡è¿›è¡Œ AI åˆ†æï¼Œç¬”è®°æ•°é‡: {len(notes)}")
        # æ·»åŠ è°ƒè¯•è¾“å‡º
        if notes:
            print(f"ğŸ“‹ ç¬¬ä¸€æ¡ç¬”è®°æ•°æ®ç¤ºä¾‹:")
            first_note = notes[0]
            for key, value in first_note.items():
                print(f"   {key}: {value}")
        else:
            print("âš ï¸  notes ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œ AI åˆ†æ")
        generate_ai_report(notes, OUTPUT_DIR)

    print(f"\n{'='*70}")
    print(f"  âœ… å®Œæˆï¼")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    asyncio.run(main())
