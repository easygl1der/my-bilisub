#!/usr/bin/env python3
"""
AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èå¹¶æ€»ç»“ï¼ˆå®Œæ•´ç‰ˆ - æ€§èƒ½ä¼˜åŒ–ï¼‰

åŠŸèƒ½ï¼š
1. åˆ·æ–°å°çº¢ä¹¦æ¨èé¡µï¼ˆè‡ªå®šä¹‰æ¬¡æ•°ï¼‰
2. é‡‡é›†æ¨èå†…å®¹ï¼ˆè§†é¢‘/å›¾æ–‡ã€ä½œè€…ä¿¡æ¯ï¼‰
3. å¯¼å‡ºCSV
4. AIç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå¯é€‰ï¼‰

æ€§èƒ½ä¼˜åŒ–ï¼š
- æ™ºèƒ½æ»šåŠ¨ï¼šè‡ªåŠ¨æ£€æµ‹é¡µé¢æ˜¯å¦è¿˜æœ‰æ–°å†…å®¹åŠ è½½
- ä¼˜åŒ–ç­‰å¾…ç­–ç•¥ï¼šä½¿ç”¨networkidleæ›¿ä»£å›ºå®šå»¶è¿Ÿ
- è½®è¯¢ç™»å½•æ£€æŸ¥ï¼šæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œæœ€å¤šç­‰å¾…90ç§’
- ä¼˜åŒ–DOMè§£æï¼šå‡å°‘ä¸å¿…è¦çš„DOMéå†
- å¯é€‰æ— å¤´æ¨¡å¼ï¼š--headless è¿è¡Œæ›´å¿«ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨ï¼‰

ä½¿ç”¨ç¤ºä¾‹:
    python ai_xiaohongshu_homepage.py

    # ä»…é‡‡é›†ï¼ˆä¸ç”ŸæˆAIæŠ¥å‘Šï¼‰
    python ai_xiaohongshu_homepage.py --mode scrape

    # å®Œæ•´æµç¨‹ï¼ˆé‡‡é›†+AIåˆ†æï¼‰
    python ai_xiaohongshu_homepage.py --mode full

    # ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆæ›´å¿«ï¼Œé€‚åˆè‡ªåŠ¨ä»»åŠ¡ï¼‰
    python ai_xiaohongshu_homepage.py --mode full --headless
"""

import argparse
import asyncio
import sys
import csv
import re
import os
from pathlib import Path
from datetime import datetime
import time

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from playwright.async_api import async_playwright

# ==================== è·¯å¾„é…ç½® ====================
# ä½¿ç”¨æ ¹ç›®å½•ä½œä¸ºé¡¹ç›®ç›®å½•ï¼ˆæ— è®ºè„šæœ¬åœ¨å“ªä¸ªå­ç›®å½•è¿è¡Œï¼‰
PROJECT_DIR = Path(__file__).parent.parent  # è·å–æ ¹ç›®å½•
OUTPUT_DIR = PROJECT_DIR / "output" / "xiaohongshu_homepage"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==================== Cookie è¯»å– ====================
def read_xhs_cookie():
    """ä» config/cookies.txt è¯»å–å°çº¢ä¹¦Cookie"""
    cookie_file = PROJECT_DIR / "config" / "cookies.txt"
    if not cookie_file.exists():
        print("âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: config/cookies.txt")
        return ""

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾ xiaohongshu_full= æ ¼å¼
    match = re.search(r'xiaohongshu_full=([^\n]+)', content)
    if match:
        return match.group(1)

    # æŸ¥æ‰¾ [xiaohongshu] éƒ¨åˆ†
    xhs_section = re.search(r'\[xiaohongshu\](.*?)\[', content, re.DOTALL)
    if xhs_section:
        section = xhs_section.group(1)
        cookies = []
        for line in section.split('\n'):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                cookies.append(f"{key.strip()}={value.strip()}")
        return '; '.join(cookies)

    return ""


# ==================== Playwright é‡‡é›† ====================
async def scrape_xiaohongshu_homepage(
    refresh_count: int = 3,
    max_notes: int = 50,
    cookie: str = "",
    headless: bool = False
) -> list:
    """ä½¿ç”¨Playwrightçˆ¬å–å°çº¢ä¹¦æ¨èé¡µ"""
    notes_collected = []
    seen_urls = set()

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆä¼˜åŒ–ï¼šç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½ä»¥æå‡é€Ÿåº¦ï¼‰
        browser = await p.chromium.launch(
            headless=headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--no-sandbox'
            ]
        )
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN',
            timezone_id='Asia/Shanghai'
        )

        # è®¾ç½®Cookie
        if cookie:
            try:
                cookies = []
                for item in cookie.split(';'):
                    item = item.strip()
                    if '=' in item:
                        key, value = item.split('=', 1)
                        cookies.append({
                            'name': key,
                            'value': value,
                            'domain': '.xiaohongshu.com',
                            'path': '/'
                        })
                await context.add_cookies(cookies)
                print("âœ… Cookieå·²è®¾ç½®")
            except Exception as e:
                print(f"âš ï¸  Cookieè®¾ç½®å¤±è´¥: {e}")

        page = await context.new_page()

        print(f"\nğŸ“¡ è®¿é—®å°çº¢ä¹¦é¦–é¡µ...")
        try:
            await page.goto('https://www.xiaohongshu.com/', wait_until='networkidle', timeout=60000)
        except Exception as e:
            print(f"âš ï¸  é¡µé¢åŠ è½½é—®é¢˜: {e}")
            print("ğŸ’¡ æµè§ˆå™¨å·²æ‰“å¼€ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

        # æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šè½®è¯¢æ£€æŸ¥è€Œéå›ºå®šç­‰å¾…90ç§’ï¼‰
        async def check_logged_in():
            try:
                content = await page.content()
                return not ('ç™»å½•' in content and 'æ³¨å†Œ' in content)
            except:
                return False

        if not await check_logged_in():
            print("\nâš ï¸  æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€")
            print("ğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•")
            print("â³ æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ç™»å½•çŠ¶æ€ï¼Œæœ€å¤šç­‰å¾…90ç§’...")

            # è½®è¯¢æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œæœ€å¤š90ç§’
            max_wait = 18  # 18æ¬¡ * 5ç§’ = 90ç§’
            for i in range(max_wait):
                await asyncio.sleep(5)
                if await check_logged_in():
                    print(f"âœ… å·²æ£€æµ‹åˆ°ç™»å½•ï¼(è€—æ—¶ {5 * (i+1)}ç§’)")
                    break
                if i == max_wait - 1:
                    print("âš ï¸  è¶…æ—¶æœªæ£€æµ‹åˆ°ç™»å½•ï¼Œç»§ç»­æ‰§è¡Œ...")

        print(f"\nğŸ”„ å¼€å§‹é‡‡é›†æ¨èå†…å®¹ï¼ˆåˆ·æ–°{refresh_count}æ¬¡ï¼‰...")

        for i in range(refresh_count):
            print(f"\n  åˆ·æ–° {i+1}/{refresh_count}")

            # ä¼˜åŒ–ï¼šæ™ºèƒ½æ»šåŠ¨ï¼Œç›´åˆ°å†…å®¹ä¸å†æ˜æ˜¾å¢åŠ 
            prev_height = 0
            scroll_stuck_count = 0
            max_scrolls = 10

            for scroll in range(max_scrolls):
                await page.evaluate('window.scrollBy(0, window.innerHeight * 0.8)')

                # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦å¢åŠ 
                current_height = await page.evaluate('document.body.scrollHeight')
                if current_height == prev_height:
                    scroll_stuck_count += 1
                    if scroll_stuck_count >= 2:  # è¿ç»­2æ¬¡é«˜åº¦ä¸å˜åˆ™åœæ­¢
                        break
                else:
                    scroll_stuck_count = 0
                    prev_height = current_height

                # å‡å°‘ç­‰å¾…æ—¶é—´ï¼šé¦–æ¬¡ç­‰å¾…ç¨é•¿ï¼Œåç»­ç­‰å¾…æ—¶é—´é€’å‡
                wait_time = 0.5 if scroll < 3 else 0.3
                await asyncio.sleep(wait_time)

            # å‡å°‘å†…å®¹åŠ è½½ç­‰å¾…æ—¶é—´
            await asyncio.sleep(1)

            # è·å–æ‰€æœ‰é“¾æ¥å’Œä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šå‡å°‘DOMéå†ï¼Œæå‰é€€å‡ºï¼‰
            try:
                notes_data = await page.evaluate('''
                    () => {
                        const notes = [];
                        const seen = new Set();
                        const MAX_NOTES = 50;  // é™åˆ¶è¿”å›æ•°é‡ï¼Œå‡å°‘æ•°æ®å¤„ç†æ—¶é—´

                        // æŸ¥æ‰¾æ‰€æœ‰ç¬”è®°å¡ç‰‡ï¼ˆä½¿ç”¨æ›´é«˜æ•ˆçš„é€‰æ‹©å™¨ï¼‰
                        const links = document.querySelectorAll('a[href*="xsec_token"]');

                        for (const link of links) {
                            if (notes.length >= MAX_NOTES) break;

                            const url = link.href;
                            const card = link.closest('section, article, [class*="note"], [class*="card"], div[class*="item"]');
                            if (!card) continue;

                            // ä» URL ä¸­æå– xsec_token å’Œ xsec_source
                            let xsecToken = '';
                            let xsecSource = 'pc_homepage';

                            try {
                                const urlParams = new URLSearchParams(url.split('?')[1]);
                                xsecToken = urlParams.get('xsec_token') || '';
                                if (urlParams.get('xsec_source')) {
                                    xsecSource = urlParams.get('xsec_source');
                                }
                            } catch (e) {}

                            // æå–ç¬”è®°ID
                            let noteId = "";
                            if (url.includes('/explore/')) {
                                const idMatch = url.match(/\\/explore\\/([a-f0-9]{24})/);
                                if (idMatch) noteId = idMatch[1];
                            } else if (url.includes('/discovery/item/')) {
                                const idMatch = url.match(/\\/discovery\\/item\\/([a-f0-9]{24})/);
                                if (idMatch) noteId = idMatch[1];
                            }

                            if (!noteId || seen.has(noteId)) continue;
                            seen.add(noteId);

                            // è·å–æ ‡é¢˜ï¼ˆç®€åŒ–ç‰ˆï¼‰
                            let title = "æ— æ ‡é¢˜";
                            const linkTitle = link.getAttribute('title');
                            if (linkTitle && linkTitle.length > 3) {
                                title = linkTitle.substring(0, 100);
                            } else {
                                // ä»…æŸ¥æ‰¾æœ€è¿‘çº§åˆ«çš„æ–‡æœ¬èŠ‚ç‚¹
                                const textNode = card.querySelector('span, div, p');
                                const text = textNode?.textContent?.trim();
                                if (text && text.length > 3 && text.length < 100 && !/^\\d+$/.test(text)) {
                                    title = text.substring(0, 100);
                                }
                            }

                            // è·å–ä½œè€…ï¼ˆç®€åŒ–ç‰ˆï¼‰
                            let author = "æœªçŸ¥ä½œè€…";
                            const authorNode = card.querySelector('a[href*="/user/profile/"], span.author');
                            if (authorNode) {
                                const text = authorNode.textContent?.trim();
                                if (text && text.length > 1 && text.length < 30 && !/\\d/.test(text)) {
                                    author = text;
                                }
                            }

                            // è·å–ç‚¹èµæ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
                            let likes = "0";
                            const likeNode = card.querySelector('[class*="like"], [class*="count"]');
                            if (likeNode) {
                                const text = likeNode.textContent?.trim();
                                if (text && /^\\d+$/.test(text)) {
                                    const num = parseInt(text);
                                    if (num > 0 && num < 1000000) {
                                        likes = text;
                                    }
                                }
                            }

                            // åˆ¤æ–­ç±»å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
                            const type = card.querySelector('video, [class*="play"], [class*="duration"]') ||
                                        (card.textContent.includes(':') && /\\d+:\\d+/.test(card.textContent))
                                        ? 'video' : 'image';

                            notes.push({
                                url: url,
                                noteId: noteId,
                                title: title,
                                author: author,
                                likes: likes,
                                type: type,
                                xsecToken: xsecToken,
                                xsecSource: xsecSource
                            });
                        }

                        return notes;
                    }
                ''')

                print(f"    æ‰¾åˆ° {len(notes_data)} ä¸ªç¬”è®°")

                for note in notes_data:
                    note_id = note['noteId']

                    # å»é‡
                    if note_id in seen_urls:
                        continue
                    seen_urls.add(note_id)

                    note_data = {
                        'åºå·': len(notes_collected) + 1,
                        'æ ‡é¢˜': note['title'],
                        'é“¾æ¥': note['url'],  # å°†åœ¨ä¸‹é¢é‡æ„
                        'å®Œæ•´é“¾æ¥': '',       # æ–°å¢å­—æ®µ
                        'ç¬”è®°ID': note_id,
                        'ä½œè€…': note['author'],
                        'ç‚¹èµæ•°': note['likes'],
                        'ç±»å‹': note['type'],
                        'xsec_token': note.get('xsecToken', ''),  # æ–°å¢å­—æ®µ
                        'xsec_source': note.get('xsecSource', 'pc_homepage'),  # æ–°å¢å­—æ®µ
                        'çˆ¬å–æ‰¹æ¬¡': i + 1,     # æ–°å¢å­—æ®µï¼šç¬¬å‡ æ¬¡çˆ¬å–
                        'é‡‡é›†æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    # é‡æ„å®Œæ•´é“¾æ¥ï¼ˆåŒ…å« xsec_tokenï¼‰
                    xsec_token = note.get('xsecToken', '')
                    xsec_source = note.get('xsecSource', 'pc_homepage')
                    if xsec_token:
                        full_url = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source={xsec_source}"
                        note_data['å®Œæ•´é“¾æ¥'] = full_url
                        note_data['é“¾æ¥'] = full_url  # æ›´æ–°é“¾æ¥å­—æ®µä¸ºå®Œæ•´é“¾æ¥
                    else:
                        note_data['å®Œæ•´é“¾æ¥'] = note['url']

                    notes_collected.append(note_data)
                    type_emoji = 'ğŸ¬' if note['type'] == 'video' else 'ğŸ–¼ï¸'
                    print(f"    âœ“ [{len(notes_collected)}] {type_emoji} {note['title']} | {note['author']} | {note['likes']}èµ")

                    if len(notes_collected) >= max_notes:
                        break

            except Exception as e:
                print(f"    âš ï¸  è§£æå‡ºé”™: {e}")

            # åˆ·æ–°
            if i < refresh_count - 1:
                print("    åˆ·æ–°é¡µé¢...")
                await page.reload(wait_until='networkidle', timeout=60000)
                await asyncio.sleep(1)  # å‡å°‘ç­‰å¾…æ—¶é—´

        await browser.close()

    print(f"\nâœ… é‡‡é›†å®Œæˆï¼å…±è·å– {len(notes_collected)} ä¸ªç¬”è®°")
    return notes_collected


# ==================== CSVå¯¼å‡º ====================
def export_to_csv(notes, output_path):
    """å¯¼å‡ºç¬”è®°åˆ°CSVï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
    csv_columns = ['åºå·', 'æ ‡é¢˜', 'é“¾æ¥', 'å®Œæ•´é“¾æ¥', 'ç¬”è®°ID', 'ä½œè€…', 'ç‚¹èµæ•°', 'ç±»å‹', 'xsec_token', 'xsec_source', 'çˆ¬å–æ‰¹æ¬¡', 'é‡‡é›†æ—¶é—´']

    # è¯»å–ç°æœ‰æ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
    existing_notes = []
    if output_path.exists():
        with open(output_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            existing_notes = list(reader)

    # åˆå¹¶æ•°æ®
    all_notes = existing_notes + notes

    # é‡æ–°ç¼–å·
    for i, note in enumerate(all_notes, 1):
        note['åºå·'] = i

    # å†™å…¥æ‰€æœ‰æ•°æ®
    with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=csv_columns)
        writer.writeheader()
        writer.writerows(all_notes)

    print(f"ğŸ“ CSVå·²æ›´æ–°: {output_path}")
    print(f"   ç°æœ‰è®°å½•: {len(existing_notes)} æ¡")
    print(f"   æ–°å¢è®°å½•: {len(notes)} æ¡")
    print(f"   æ€»è®¡: {len(all_notes)} æ¡")


# ==================== AIåˆ†æ ====================
def generate_ai_report(notes, output_dir):
    """ä½¿ç”¨Geminiç”ŸæˆAIæŠ¥å‘Š"""
    import json

    print("\nğŸ¤– å¼€å§‹AIåˆ†æ...")

    # æ£€æŸ¥Geminiæ˜¯å¦å¯ç”¨
    _gemini_available = False
    use_new_sdk = False

    try:
        from google import genai as genai_new
        genai = genai_new
        use_new_sdk = True
        _gemini_available = True
    except ImportError:
        try:
            import google.generativeai as genai_old
            genai = genai_old
            use_new_sdk = False
            _gemini_available = True
        except:
            pass

    if not _gemini_available:
        print("âš ï¸  æœªå®‰è£…google-generativeaiï¼Œè·³è¿‡AIåˆ†æ")
        print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install google-generativeai")
        return

    # é…ç½®APIï¼šä¼˜å…ˆä» bot_config.json è¯»å–
    api_key = None
    config_file = PROJECT_DIR / "config" / "bot_config.json"

    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            if 'gemini_api_key' in config:
                api_key = config['gemini_api_key']
                print(f"âœ… API Key è¯»å–æˆåŠŸ: {api_key[:20]}...{api_key[-5:]}")
        except Exception as e:
            print(f"âš ï¸  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ï¼Œå†å°è¯•ç¯å¢ƒå˜é‡
    if not api_key:
        api_key = os.environ.get('GEMINI_API_KEY', '')
        if api_key:
            print(f"âœ… API Key ä»ç¯å¢ƒå˜é‡è¯»å–: {api_key[:20]}...{api_key[-5:]}")

    if not api_key:
        print("âš ï¸  æœªè®¾ç½® API Key")
        print("ğŸ’¡ æ–¹æ³•1: åœ¨ config/bot_config.json ä¸­æ·»åŠ  gemini_api_key")
        print("ğŸ’¡ æ–¹æ³•2: è®¾ç½®ç¯å¢ƒå˜é‡: set GEMINI_API_KEY=your_key_here")
        return

    # æ ¹æ®SDKç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„API
    if use_new_sdk:
        # æ–° SDK (google-genai)
        client = genai.Client(api_key=api_key)
    else:
        # æ—§ SDK (google.generativeai)
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')

    # ç”ŸæˆæŠ¥å‘Š - ä½¿ç”¨ä¸CSVç›¸åŒçš„å®Œæ•´æ•°æ®
    notes_text = "\n\n".join([
        f"{note.get('åºå·', i+1)}. ã€{note.get('æ ‡é¢˜', 'æ— æ ‡é¢˜')}ã€‘\n"
        f"   ä½œè€…: {note.get('ä½œè€…', 'æœªçŸ¥')}\n"
        f"   ç±»å‹: {note.get('ç±»å‹', 'æœªçŸ¥')}\n"
        f"   ç‚¹èµ: {note.get('ç‚¹èµæ•°', '0')}\n"
        f"   é“¾æ¥: {note.get('é“¾æ¥', '')}\n"
        f"   ç¬”è®°ID: {note.get('ç¬”è®°ID', '')}\n"
        f"   çˆ¬å–æ‰¹æ¬¡: ç¬¬{note.get('çˆ¬å–æ‰¹æ¬¡', 1)}æ¬¡\n"
        f"   é‡‡é›†æ—¶é—´: {note.get('é‡‡é›†æ—¶é—´', '')}\n"
        for i, note in enumerate(notes)  # åˆ†ææ‰€æœ‰ç¬”è®°
    ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦æ¨èå†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„è¶‹åŠ¿æŠ¥å‘Šã€‚

å°çº¢ä¹¦æ¨èå†…å®¹ï¼š
{notes_text}

è¯·ç”Ÿæˆä»¥ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š

## ğŸ“Š å°çº¢ä¹¦æ¨èè¶‹åŠ¿åˆ†æ

### ğŸ¯ å†…å®¹æ¦‚è§ˆ
- é‡‡é›†ç¬”è®°æ•°ï¼š{len(notes)}ç¯‡
- è§†é¢‘å æ¯”ï¼š{sum(1 for n in notes if n.get('ç±»å‹') == 'video')}ç¯‡ ({sum(1 for n in notes if n.get('ç±»å‹') == 'video')/len(notes)*100:.1f}%)
- å›¾æ–‡å æ¯”ï¼š{sum(1 for n in notes if n.get('ç±»å‹') == 'image')}ç¯‡ ({sum(1 for n in notes if n.get('ç±»å‹') == 'image')/len(notes)*100:.1f}%)
- çˆ¬å–æ‰¹æ¬¡ï¼šå…±{max((n.get('çˆ¬å–æ‰¹æ¬¡', 1) for n in notes), default=1)}æ¬¡åˆ·æ–°

### ğŸ”¥ çƒ­é—¨ä¸»é¢˜ï¼ˆTop 5ï¼‰
åŸºäºç¬”è®°æ ‡é¢˜å’Œå†…å®¹ï¼Œæå–æœ€å—æ¬¢è¿çš„5ä¸ªä¸»é¢˜

### ğŸ‘¥ çƒ­é—¨ä½œè€…ï¼ˆTop 5ï¼‰
åˆ—ä¸¾å‡ºç°æœ€é¢‘ç¹çš„5ä¸ªä½œè€…ï¼Œæ ‡æ³¨å„è‡ªå‡ºç°æ¬¡æ•°å’Œå¹³å‡ç‚¹èµæ•°

### ğŸ“ˆ è¶‹åŠ¿åˆ†æ
åŸºäºæ‰€æœ‰ç¬”è®°æ•°æ®ï¼Œåˆ†æå½“å‰å°çº¢ä¹¦æ¨èçš„å†…å®¹è¶‹åŠ¿ï¼š
- çƒ­é—¨è¯é¢˜åˆ†å¸ƒ
- å†…å®¹åå¥½ç‰¹å¾
- å—æ¬¢è¿çš„å†…å®¹ç±»å‹
- ä¸åŒçˆ¬å–æ‰¹æ¬¡çš„å†…å®¹å·®å¼‚ï¼ˆå¦‚æœæ˜æ˜¾ï¼‰

### ğŸ’ å€¼å¾—å…³æ³¨çš„ç¬”è®°
ç»¼åˆç‚¹èµæ•°ã€å†…å®¹è´¨é‡ï¼Œæ¨è3-5ä¸ªå€¼å¾—æ·±å…¥é˜…è¯»çš„ç¬”è®°ï¼ˆé™„å®Œæ•´é“¾æ¥å’Œæ¨èç†ç”±ï¼‰

### ğŸ“‹ æ•°æ®æ´å¯Ÿï¼ˆå¯é€‰ï¼‰
å¦‚æœæœ‰ç‰¹åˆ«æœ‰è¶£çš„æ•°æ®å‘ç°ï¼Œè¯·åœ¨æ­¤è¯´æ˜

è¯·ç¡®ä¿æŠ¥å‘Šç»“æ„å®Œæ•´ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å®è´¨å†…å®¹ï¼Œæ•°æ®å¼•ç”¨è¦å‡†ç¡®ã€‚"""

    try:
        if use_new_sdk:
            # æ–° SDK è°ƒç”¨æ–¹å¼
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt
            )
            report = response.text if hasattr(response, 'text') and response.text else str(response)
        else:
            # æ—§ SDK è°ƒç”¨æ–¹å¼
            response = model.generate_content(prompt)
            report = response.text if hasattr(response, 'text') and response.text else "ç”Ÿæˆå¤±è´¥"

        # ä¿å­˜æŠ¥å‘Š
        date_str = datetime.now().strftime('%Y-%m-%d')
        report_path = output_dir / f"xiaohongshu_homepage_{date_str}_AIæŠ¥å‘Š.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“ AIæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ‘˜è¦
        lines = report.split('\n')
        print("\n" + "=" * 70)
        print("  ğŸ“– AIåˆ†ææŠ¥å‘Š")
        print("=" * 70)
        for line in lines[:20]:
            print(line)
        if len(lines) > 20:
            print(f"  ... è¿˜æœ‰ {len(lines) - 20} è¡Œ")
        print("=" * 70)

    except Exception as e:
        print(f"âŒ AIåˆ†æå¤±è´¥: {e}")


# ==================== ä¸»ç¨‹åº ====================
async def main():
    parser = argparse.ArgumentParser(description='AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èå¹¶æ€»ç»“')

    parser.add_argument('--refresh-count', type=int, default=3,
                       help='åˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('--max-notes', type=int, default=50,
                       help='æœ€å¤šé‡‡é›†ç¬”è®°æ•°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--mode', type=str, default='scrape',
                       choices=['scrape', 'full'],
                       help='æ¨¡å¼: scrape=ä»…é‡‡é›†, full=é‡‡é›†+AIåˆ†æ')
    parser.add_argument('--headless', action='store_true',
                       help='ä½¿ç”¨æ— å¤´æ¨¡å¼è¿è¡Œï¼ˆæ›´å¿«ï¼Œä½†ä¸æ˜¾ç¤ºæµè§ˆå™¨ï¼‰')

    args = parser.parse_args()

    print(f"\n{'='*70}")
    print(f"  AIè‡ªåŠ¨åˆ·å°çº¢ä¹¦æ¨èï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print(f"{'='*70}")
    print(f"\nğŸ“Š é…ç½®:")
    print(f"  â€¢ åˆ·æ–°æ¬¡æ•°: {args.refresh_count}")
    print(f"  â€¢ æœ€å¤šç¬”è®°: {args.max_notes}")
    print(f"  â€¢ åˆ†ææ¨¡å¼: {args.mode}")
    print(f"  â€¢ æ— å¤´æ¨¡å¼: {'æ˜¯' if args.headless else 'å¦'}")

    # è¯»å–Cookie
    cookie = read_xhs_cookie()
    if not cookie:
        print("\nâš ï¸  æœªæ‰¾åˆ°Cookieï¼Œå°†ä½¿ç”¨æ— Cookieæ¨¡å¼ï¼ˆéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰")

    # é‡‡é›†æ•°æ®ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰
    start_time = time.time()
    notes = await scrape_xiaohongshu_homepage(
        refresh_count=args.refresh_count,
        max_notes=args.max_notes,
        cookie=cookie,
        headless=args.headless
    )
    scrape_time = time.time() - start_time

    if not notes:
        print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•ç¬”è®°")
        return

    print(f"\nâ±ï¸  é‡‡é›†è€—æ—¶: {scrape_time:.1f}ç§’")
    print(f"ğŸ“Š é‡‡é›†é€Ÿåº¦: {len(notes)/scrape_time:.2f} ç¬”è®°/ç§’")

    # å¯¼å‡ºCSV
    date_str = datetime.now().strftime('%Y-%m-%d')
    csv_path = OUTPUT_DIR / f"xiaohongshu_homepage_{date_str}.csv"
    export_to_csv(notes, csv_path)

    # AIåˆ†æ
    if args.mode == 'full':
        print(f"\nğŸ“Š å‡†å¤‡è¿›è¡Œ AI åˆ†æï¼Œç¬”è®°æ•°é‡: {len(notes)}")
        # æ·»åŠ è°ƒè¯•è¾“å‡º
        if notes:
            print(f"ğŸ“‹ ç¬¬ä¸€æ¡ç¬”è®°æ•°æ®ç¤ºä¾‹:")
            first_note = notes[0]
            for key, value in first_note.items():
                print(f"   {key}: {value}")
        else:
            print("âš ï¸  notes ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œ AI åˆ†æ")
        generate_ai_report(notes, OUTPUT_DIR)

    print(f"\n{'='*70}")
    print(f"  âœ… å®Œæˆï¼")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    asyncio.run(main())
