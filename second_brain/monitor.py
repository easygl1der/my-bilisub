#!/usr/bin/env python3
"""
è§†é¢‘ç›‘æ§æ¨¡å— - å®æ—¶æ£€æµ‹UPä¸»å‘å¸ƒçš„æ–°è§†é¢‘

æ”¯æŒå¹³å°ï¼š
- Bç«™ (bilibili)
- å°çº¢ä¹¦ (xiaohongshu)
- YouTube (youtube)
"""

import asyncio
import time
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional, AsyncGenerator
from pathlib import Path
import re
import html
import urllib3
import os
import json

# ç¦ç”¨ SSL è­¦å‘Šï¼ˆä»…ç”¨äºå¼€å‘ç¯å¢ƒï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import feedparser
    HAS_FEEDPARSER = True
except ImportError:
    HAS_FEEDPARSER = False

try:
    import aiohttp
    HAS_AIOHTTP = True
except ImportError:
    HAS_AIOHTTP = False

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False


# ==================== å¹³å°API ====================

class BilibiliAPI:
    """Bç«™API"""

    BASE_URL = "https://api.bilibili.com"

    @classmethod
    def _load_cookies(cls) -> Dict[str, str]:
        """ä»æ–‡ä»¶åŠ è½½Bç«™ cookies"""
        cookie_files = [
            "cookies_bilibili.txt",
            "config/cookies_bilibili.txt",
            ".cookies/bilibili.txt",
        ]

        cookies = {}
        for cookie_file in cookie_files:
            cookie_path = Path(cookie_file)
            if cookie_path.exists():
                try:
                    content = cookie_path.read_text(encoding='utf-8')
                    # æ”¯æŒ Netscape æ ¼å¼å’Œç®€å•æ ¼å¼
                    for line in content.strip().split('\n'):
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                        if '=' in line:
                            key, value = line.split('=', 1)
                            cookies[key.strip()] = value.strip()
                    if cookies:
                        print(f"   â””â”€ ğŸª å·²åŠ è½½ {len(cookies)} ä¸ª cookies")
                    break
                except Exception as e:
                    pass
        return cookies

    @classmethod
    def _get_headers(cls) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´ï¼ˆåŒ…å« cookiesï¼‰"""
        cookies = cls._load_cookies()
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.bilibili.com",
            "Accept": "application/json",
        }
        if cookies:
            headers["Cookie"] = "; ".join([f"{k}={v}" for k, v in cookies.items()])
        return headers

    @classmethod
    def get_user_info(cls, uid: str) -> Optional[Dict]:
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        url = f"{cls.BASE_URL}/x/space/acc/info"
        params = {"mid": uid}
        headers = cls._get_headers()
        try:
            resp = requests.get(url, params=params, headers=headers, timeout=10, verify=True)
            if resp.status_code == 200:
                data = resp.json()
                if data.get("code") == 0:
                    info = data["data"]
                    return {
                        "uid": uid,
                        "name": info.get("name"),
                        "avatar": info.get("face"),
                        "fans": info.get("follower"),
                        "sign": info.get("sign"),
                    }
        except requests.exceptions.SSLError:
            print(f"   â””â”€ âš ï¸ SSLé”™è¯¯ï¼Œå°è¯•å¿½ç•¥éªŒè¯...")
            try:
                resp = requests.get(url, params=params, headers=headers, timeout=10, verify=False)
                if resp.status_code == 200:
                    data = resp.json()
                    if data.get("code") == 0:
                        info = data["data"]
                        return {
                            "uid": uid,
                            "name": info.get("name"),
                            "avatar": info.get("face"),
                            "fans": info.get("follower"),
                            "sign": info.get("sign"),
                        }
            except Exception as e:
                print(f"   â””â”€ âŒ è·å–Bç«™ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
        except Exception as e:
            print(f"   â””â”€ âŒ è·å–Bç«™ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
        return None

    @classmethod
    def get_user_videos(cls, uid: str, limit: int = 30) -> List[Dict]:
        """è·å–ç”¨æˆ·è§†é¢‘åˆ—è¡¨ï¼ˆå¤šæ–¹æ³•å°è¯•ï¼‰"""
        videos = []

        # æ–¹æ³•1: ä¼˜å…ˆä½¿ç”¨RSSï¼ˆæ›´ç¨³å®šï¼Œæ— é™æµé—®é¢˜ï¼‰
        print(f"   â””â”€ ğŸ“¡ å°è¯•RSSæ–¹å¼...")
        videos = cls.get_space_videos(uid, limit)

        # æ–¹æ³•2: å¦‚æœRSSå¤±è´¥ï¼Œå°è¯•Bç«™API
        if not videos:
            print(f"   â””â”€ ğŸ”„ RSSå¤±è´¥ï¼Œå°è¯•APIå¤‡ç”¨æ–¹å¼...")
            page = 1
            page_size = min(30, limit)
            headers = cls._get_headers()

            try:
                while len(videos) < limit:
                    url = f"{cls.BASE_URL}/x/space/arc/search"
                    params = {
                        "mid": uid,
                        "ps": page_size,
                        "pn": page,
                        "order": "pubdate"
                    }

                    resp = requests.get(url, params=params, headers=headers, timeout=15)
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("code") == 0:
                            list_data = data.get("data", {}).get("list", {})
                            vlist = list_data.get("vlist", {})

                            # vlist å¯èƒ½æ˜¯å­—å…¸ï¼Œå¤„ç†è¿™ç§æƒ…å†µ
                            if isinstance(vlist, dict):
                                vlist = list_data.get("arc", {}).get("list", {})

                            if not vlist:
                                break

                            for v in vlist:
                                if len(videos) >= limit:
                                    break

                                videos.append({
                                    "platform": "bilibili",
                                    "video_id": v.get("bvid"),
                                    "title": v.get("title"),
                                    "description": v.get("description"),
                                    "duration": v.get("length"),
                                    "published_at": datetime.fromtimestamp(v.get("created")).isoformat() if v.get("created") else None,
                                    "thumbnail": v.get("pic"),
                                    "view_count": v.get("play"),
                                    "danmaku_count": v.get("video_review"),
                                    "url": f"https://www.bilibili.com/video/{v.get('bvid')}",
                                })

                            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š
                            if not vlist or len(vlist) < page_size:
                                break

                            page += 1
                        else:
                            print(f"   â””â”€ âš ï¸ APIè¿”å›é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                            break
                    else:
                        print(f"   â””â”€ âš ï¸ APIè¯·æ±‚å¤±è´¥: HTTP {resp.status_code}")
                        break
            except requests.exceptions.SSLError:
                print(f"   â””â”€ âš ï¸ SSLé”™è¯¯")
            except Exception as e:
                print(f"   â””â”€ âš ï¸ APIæ–¹å¼å¤±è´¥: {e}")

        # æ–¹æ³•3: å¦‚æœAPIä¹Ÿå¤±è´¥ï¼Œå°è¯•HTMLè§£æ
        if not videos:
            print(f"   â””â”€ ğŸŒ APIå¤±è´¥ï¼Œå°è¯•HTMLé¡µé¢è§£æ...")
            videos = cls.get_videos_from_html(uid, limit)

        return videos

    @classmethod
    def get_space_videos(cls, uid: str, limit: int = 30) -> List[Dict]:
        """é€šè¿‡RSSè·å–ç©ºé—´è§†é¢‘ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼Œä¸ä¾èµ–feedparserï¼‰"""
        # å°è¯•å¤šä¸ª RSS æº
        rss_urls = [
            f"https://rsshub.app/bilibili/user/video/{uid}",
            f"https://rss.yochat.cn/bilibili/user/video/{uid}",
        ]

        for rss_url in rss_urls:
            try:
                resp = requests.get(rss_url, timeout=15)
                if resp.status_code != 200:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯ RSSHub çš„é™åˆ¶æ¶ˆæ¯
                if b"cost considerations" in resp.content or b"restrict access" in resp.content:
                    continue

                root = ET.fromstring(resp.content)
                # RSS 2.0 format
                videos = []

                for item in root.findall('.//item'):
                    if len(videos) >= limit:
                        break

                    # è·å–æ ‡é¢˜
                    title = item.find('title')
                    title_text = title.text if title is not None else ""

                    # è·å–é“¾æ¥
                    link = item.find('link')
                    link_text = link.text if link is not None else ""

                    # æå–bvid
                    bvid_match = re.search(r'/video/(BV[\w]+)', link_text)
                    if not bvid_match:
                        continue
                    bvid = bvid_match.group(1)

                    # è·å–å‘å¸ƒæ—¶é—´
                    pub_date = item.find('pubDate')
                    published = pub_date.text if pub_date is not None else ""

                    # è·å–æè¿°
                    desc = item.find('description')
                    description = desc.text if desc is not None else ""

                    videos.append({
                        "platform": "bilibili",
                        "video_id": bvid,
                        "title": title_text,
                        "description": description,
                        "published_at": published,
                        "url": link_text,
                    })

                if videos:
                    return videos

            except Exception as e:
                continue

        return []

    @classmethod
    def get_videos_from_html(cls, uid: str, limit: int = 30) -> List[Dict]:
        """é€šè¿‡è§£æBç«™ç©ºé—´é¡µé¢è·å–è§†é¢‘ï¼ˆæœ€åçš„å¤‡ç”¨æ–¹æ³•ï¼‰"""
        space_url = f"https://space.bilibili.com/{uid}/video"
        headers = cls._get_headers()
        videos = []

        try:
            resp = requests.get(space_url, headers=headers, timeout=15)
            if resp.status_code != 200:
                return []

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä» HTML ä¸­æå–è§†é¢‘æ•°æ®
            # Bç«™é¡µé¢é€šå¸¸åŒ…å«ä¸€ä¸ª __INITIAL_STATE__ å¯¹è±¡
            pattern = r'window\.__INITIAL_STATE__\s*=\s*({.*?});'
            match = re.search(pattern, resp.text)

            if match:
                data_str = match.group(1)
                try:
                    data = json.loads(data_str)

                    # å°è¯•ä»ä¸åŒè·¯å¾„è·å–è§†é¢‘åˆ—è¡¨
                    video_list = None
                    if 'videoUserDetail' in data:
                        video_list = data['videoUserDetail'].get('list', {}).get('list', {}).get('vlist', [])
                    elif 'space' in data:
                        video_list = data['space'].get('videoList', [])

                    if video_list and isinstance(video_list, list):
                        for v in video_list[:limit]:
                            bvid = v.get('bvid') or v.get('aid')
                            if not bvid:
                                continue

                            videos.append({
                                "platform": "bilibili",
                                "video_id": bvid,
                                "title": v.get('title', ''),
                                "description": v.get('description', ''),
                                "duration": v.get('length', ''),
                                "published_at": datetime.fromtimestamp(v.get('created', 0)).isoformat() if v.get('created') else None,
                                "thumbnail": v.get('pic', ''),
                                "view_count": v.get('play', 0),
                                "danmaku_count": v.get('video_review', 0),
                                "url": f"https://www.bilibili.com/video/{bvid}",
                            })

                except json.JSONDecodeError:
                    pass

            # å¦‚æœ __INITIAL_STATE__ æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ç”¨ BeautifulSoup
            if not videos and HAS_BS4:
                soup = BeautifulSoup(resp.text, 'html.parser')

                # æŸ¥æ‰¾è§†é¢‘å¡ç‰‡å…ƒç´ 
                video_cards = soup.find_all('a', href=re.compile(r'/video/BV'))
                for card in video_cards[:limit]:
                    href = card.get('href', '')
                    bvid_match = re.search(r'BV[\w]+', href)
                    if bvid_match:
                        bvid = bvid_match.group(0)
                        title_elem = card.find('span', class_='video-title') or card.find('title')
                        title = title_elem.get('title', '') if title_elem and hasattr(title_elem, 'get') else card.get_text(strip=True)

                        videos.append({
                            "platform": "bilibili",
                            "video_id": bvid,
                            "title": title,
                            "url": f"https://www.bilibili.com/video/{bvid}",
                        })

        except Exception as e:
            print(f"   â””â”€ âš ï¸ HTMLè§£æå¤±è´¥: {e}")

        return videos


class XiaohongshuAPI:
    """å°çº¢ä¹¦API"""

    @classmethod
    def get_user_videos(cls, uid: str, limit: int = 30) -> List[Dict]:
        """è·å–ç”¨æˆ·è§†é¢‘åˆ—è¡¨ï¼ˆé€šè¿‡çˆ¬è™«æˆ–APIï¼‰"""
        # å°çº¢ä¹¦éœ€è¦ç‰¹æ®Šçš„APIæˆ–çˆ¬è™«æ–¹å¼
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å®ç°ï¼Œå®é™…éœ€è¦ä½¿ç”¨ MediaCrawler ä¸­çš„æ–¹æ³•
        print(f"   â””â”€ âš ï¸ å°çº¢ä¹¦APIæš‚æœªå®Œæ•´å®ç°ï¼Œè¯·ä½¿ç”¨ MediaCrawler")
        return []

    @classmethod
    def get_user_info(cls, uid: str) -> Optional[Dict]:
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        # éœ€è¦é€šè¿‡çˆ¬è™«è·å–
        return None


class YouTubeAPI:
    """YouTube API"""

    RSS_BASE = "https://www.youtube.com/feeds/videos.xml"

    @classmethod
    def _parse_rss_regex(cls, content: str, limit: int) -> List[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æRSSï¼ˆå½“xml.etreeä¸å¯ç”¨æ—¶ï¼‰"""
        videos = []

        try:
            # æå–æ‰€æœ‰ <entry> æ ‡ç­¾çš„å†…å®¹
            entries = re.findall(r'<entry>(.*?)</entry>', content, re.DOTALL)

            for entry_content in entries[:limit]:
                # æå–è§†é¢‘ID
                video_id_match = re.search(r'<videoId>([^<]+)</videoId>', entry_content)
                video_id = video_id_match.group(1) if video_id_match else ""

                # æå–æ ‡é¢˜
                title_match = re.search(r'<title>([^<]+)</title>', entry_content)
                title = title_match.group(1) if title_match else ""
                title = html.unescape(title) if title else ""

                # æå–é“¾æ¥
                link_match = re.search(r'<link[^>]+href="([^"]+)"', entry_content)
                url = link_match.group(1) if link_match else ""

                # æå–å‘å¸ƒæ—¶é—´
                published_match = re.search(r'<published>([^<]+)</published>', entry_content)
                published = published_match.group(1) if published_match else ""

                # æå–æè¿°
                desc_match = re.search(r'<media:description>([^<]*)</media:description>', entry_content)
                description = desc_match.group(1) if desc_match else ""
                if description:
                    description = html.unescape(description)

                # æå–ç¼©ç•¥å›¾
                thumbnail_match = re.search(r'<yt:thumbnail url="([^"]+)"', entry_content)
                thumbnail = thumbnail_match.group(1) if thumbnail_match else ""

                if video_id:
                    videos.append({
                        "platform": "youtube",
                        "video_id": video_id,
                        "title": title,
                        "description": description,
                        "published_at": published,
                        "thumbnail": thumbnail,
                        "url": url,
                    })
        except Exception as e:
            pass

        return videos

    @classmethod
    def _parse_rss(cls, rss_url: str, limit: int = 30) -> List[Dict]:
        """è§£æRSSï¼ˆä¼˜å…ˆä½¿ç”¨xml.etreeï¼Œå¤±è´¥åˆ™ç”¨æ­£åˆ™ï¼‰"""
        try:
            resp = requests.get(rss_url, timeout=15)
            if resp.status_code != 200:
                return []

            content = resp.text

            # å…ˆå°è¯•ä½¿ç”¨ xml.etree
            try:
                root = ET.fromstring(resp.content)
                # YouTubeä½¿ç”¨Atomæ ¼å¼
                ns = {'atom': 'http://www.w3.org/2005/Atom',
                      'yt': 'http://www.youtube.com/xml/schemas/2015',
                      'media': 'http://search.yahoo.com/mrss/'}

                videos = []
                for entry in root.findall('atom:entry', ns):
                    if len(videos) >= limit:
                        break

                    # è·å–è§†é¢‘ID
                    video_id = entry.find('atom:id', ns).text.split(':')[-1] if entry.find('atom:id', ns) is not None else ""

                    # è·å–æ ‡é¢˜
                    title_elem = entry.find('atom:title', ns)
                    title = title_elem.text if title_elem is not None else ""

                    # è·å–é“¾æ¥
                    link_elem = entry.find('atom:link', ns)
                    url = link_elem.get('href') if link_elem is not None else ""

                    # è·å–å‘å¸ƒæ—¶é—´
                    published_elem = entry.find('atom:published', ns)
                    published = published_elem.text if published_elem is not None else ""

                    # è·å–æè¿°
                    content_elem = entry.find('atom:content', ns)
                    description = content_elem.text if content_elem is not None else ""
                    if description:
                        description = html.unescape(description)

                    # è·å–ç¼©ç•¥å›¾
                    thumbnail_elem = entry.find('atom:group/atom:thumbnail', ns)
                    thumbnail = thumbnail_elem.get('url') if thumbnail_elem is not None else ""

                    videos.append({
                        "platform": "youtube",
                        "video_id": video_id,
                        "title": title,
                        "description": description,
                        "published_at": published,
                        "thumbnail": thumbnail,
                        "url": url,
                    })

                return videos
            except (ImportError, Exception) as e:
                # xml.etree å¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ
                print(f"   â””â”€ âš ï¸ XMLè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è§£æ...")
                return cls._parse_rss_regex(content, limit)

        except Exception as e:
            print(f"   â””â”€ âŒ è§£æRSSå¤±è´¥: {e}")
            return []

    @classmethod
    def get_channel_videos(cls, channel_id: str, limit: int = 30) -> List[Dict]:
        """è·å–é¢‘é“è§†é¢‘åˆ—è¡¨ï¼ˆé€šè¿‡RSSï¼Œæ— éœ€API Keyï¼‰"""
        rss_url = f"{cls.RSS_BASE}?channel_id={channel_id}"
        return cls._parse_rss(rss_url, limit)

    @classmethod
    def get_user_videos(cls, username: str, limit: int = 30) -> List[Dict]:
        """è·å–ç”¨æˆ·è§†é¢‘åˆ—è¡¨"""
        rss_url = f"{cls.RSS_BASE}?user={username}"
        return cls._parse_rss(rss_url, limit)


# ==================== è§†é¢‘ç›‘æ§å™¨ ====================

class VideoMonitor:
    """è§†é¢‘ç›‘æ§å™¨ - æ£€æµ‹æ–°è§†é¢‘"""

    def __init__(self, database):
        self.db = database
        self.platform_apis = {
            "bilibili": BilibiliAPI,
            "xiaohongshu": XiaohongshuAPI,
            "youtube": YouTubeAPI,
        }

    def check_creator(self, creator: Dict) -> List[Dict]:
        """
        æ£€æŸ¥å•ä¸ªåšä¸»çš„æ–°è§†é¢‘

        Args:
            creator: åšä¸»ä¿¡æ¯ dict

        Returns:
            æ–°è§†é¢‘åˆ—è¡¨
        """
        platform = creator["platform"]
        uid = creator["uid"]
        creator_id = creator.get("db_id")

        print(f"\n{'='*60}")
        print(f"ğŸ“º æ£€æŸ¥: [{platform.upper()}] {creator['name']}")
        print(f"{'='*60}")

        api_class = self.platform_apis.get(platform)
        if not api_class:
            print(f"   â””â”€ âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}")
            return []

        # è·å–è§†é¢‘åˆ—è¡¨
        max_videos = 50  # æ¯æ¬¡æœ€å¤šè·å–50ä¸ª
        videos = api_class.get_user_videos(uid, limit=max_videos)

        if not videos:
            print(f"   â””â”€ ğŸ“­ æœªæ‰¾åˆ°è§†é¢‘")
            return []

        # è¿‡æ»¤æ–°è§†é¢‘
        new_videos = []
        for video in videos:
            video_id = video.get("video_id", "")
            if not video_id:
                continue

            if not self.db.video_exists(video_id, platform):
                # ä¿å­˜åˆ°æ•°æ®åº“
                video_id_in_db = self.db.add_video(
                    creator_id=creator_id,
                    platform=platform,
                    video_id=video_id,
                    title=video.get("title", ""),
                    description=video.get("description", ""),
                    duration=int(video.get("duration", 0)) if video.get("duration", "").isdigit() else None,
                    published_at=video.get("published_at"),
                    thumbnail_url=video.get("thumbnail"),
                    video_url=video.get("url"),
                    view_count=int(video.get("view_count", 0) or 0),
                    danmaku_count=int(video.get("danmaku_count", 0) or 0)
                )

                new_videos.append({
                    **video,
                    "db_id": video_id_in_db
                })

        # è®°å½•æ—¥å¿—
        self.db.log(platform, "check", len(videos), f"å‘ç° {len(new_videos)} ä¸ªæ–°è§†é¢‘")

        if new_videos:
            print(f"   â””â”€ âœ… å‘ç° {len(new_videos)} ä¸ªæ–°è§†é¢‘ï¼")
            for v in new_videos:
                print(f"      - {v.get('title', 'æœªçŸ¥æ ‡é¢˜')[:40]}... ({v.get('published_at', '')[:10]})")
        else:
            print(f"   â””â”€ âœ… æš‚æ— æ–°è§†é¢‘")

        return new_videos

    def check_all_creators(self, creators: List[Dict]) -> List[Dict]:
        """
        æ£€æŸ¥æ‰€æœ‰åšä¸»çš„æ–°è§†é¢‘

        Args:
            creators: åšä¸»åˆ—è¡¨

        Returns:
            æ‰€æœ‰æ–°è§†é¢‘åˆ—è¡¨
        """
        all_new_videos = []

        for creator in creators:
            # å¦‚æœæ²¡æœ‰db_idï¼Œå…ˆå°è¯•è·å–
            if not creator.get("db_id"):
                existing = self.db.get_creator(creator["platform"], creator["uid"])
                if existing:
                    creator["db_id"] = existing["id"]
                else:
                    # æ·»åŠ æ–°åšä¸»åˆ°æ•°æ®åº“
                    api_class = self.platform_apis.get(creator["platform"])
                    if api_class and hasattr(api_class, "get_user_info"):
                        info = api_class.get_user_info(creator["uid"])
                        if info:
                            creator["db_id"] = self.db.add_creator(
                                platform=creator["platform"],
                                uid=creator["uid"],
                                name=info.get("name", creator.get("name", "")),
                                category=creator.get("category", ""),
                                avatar_url=info.get("avatar"),
                                fans_count=info.get("fans", 0)
                            )
                        else:
                            creator["db_id"] = self.db.add_creator(
                                platform=creator["platform"],
                                uid=creator["uid"],
                                name=creator.get("name", ""),
                                category=creator.get("category", "")
                            )

            new_videos = self.check_creator(creator)
            all_new_videos.extend(new_videos)

        return all_new_videos

    def run_once(self, creators: List[Dict]) -> Dict[str, int]:
        """
        è¿è¡Œä¸€æ¬¡æ£€æŸ¥

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\n{'='*70}")
        print(f"ğŸ” å¼€å§‹æ£€æŸ¥æ–°è§†é¢‘...")
        print(f"   åšä¸»æ•°é‡: {len(creators)}")
        print(f"   æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}")

        start_time = time.time()
        new_videos = self.check_all_creators(creators)
        elapsed = time.time() - start_time

        stats = {
            "total_creators": len(creators),
            "new_videos": len(new_videos),
            "elapsed_time": elapsed,
        }

        print(f"\n{'='*70}")
        print(f"ğŸ“Š æ£€æŸ¥å®Œæˆ")
        print(f"{'='*70}")
        print(f"   æ£€æŸ¥åšä¸»: {stats['total_creators']} ä¸ª")
        print(f"   æ–°å¢è§†é¢‘: {stats['new_videos']} ä¸ª")
        print(f"   è€—æ—¶: {stats['elapsed_time']:.1f} ç§’")

        return stats

    def run_loop(self, creators: List[Dict], interval: int = 300,
                 callback=None, max_iterations: int = None):
        """
        æŒç»­ç›‘æ§å¾ªç¯

        Args:
            creators: åšä¸»åˆ—è¡¨
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
            callback: å‘ç°æ–°è§†é¢‘æ—¶çš„å›è°ƒå‡½æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆNone=æ— é™ï¼‰
        """
        iteration = 0

        print(f"\n{'='*70}")
        print(f"ğŸ”„ å¯åŠ¨ç›‘æ§å¾ªç¯")
        print(f"   æ£€æŸ¥é—´éš”: {interval} ç§’ ({interval//60} åˆ†é’Ÿ)")
        print(f"   åšä¸»æ•°é‡: {len(creators)}")
        print(f"{'='*70}\n")

        try:
            while True:
                iteration += 1
                print(f"\nğŸ“ ç¬¬ {iteration} è½®æ£€æŸ¥ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

                # æ£€æŸ¥æ–°è§†é¢‘
                new_videos = self.check_all_creators(creators)

                # è°ƒç”¨å›è°ƒ
                if callback and new_videos:
                    try:
                        callback(new_videos)
                    except Exception as e:
                        print(f"   â””â”€ âŒ å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

                # æ£€æŸ¥æ˜¯å¦é€€å‡º
                if max_iterations and iteration >= max_iterations:
                    print(f"\nâœ… è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œé€€å‡ºç›‘æ§")
                    break

                # ç­‰å¾…ä¸‹ä¸€è½®
                next_check = datetime.now() + timedelta(seconds=interval)
                print(f"\nâ° ä¸‹æ¬¡æ£€æŸ¥: {next_check.strftime('%H:%M:%S')}")
                print(f"   ç­‰å¾…ä¸­... (æŒ‰ Ctrl+C åœæ­¢)")

                time.sleep(interval)

        except KeyboardInterrupt:
            print(f"\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢ç›‘æ§")
        except Exception as e:
            print(f"\n\nâŒ ç›‘æ§å‡ºé”™: {e}")
            raise


# ==================== å‘½ä»¤è¡Œå·¥å…· ====================

def add_creator_command(db, platform: str, uid: str, name: str, category: str = ""):
    """æ·»åŠ åšä¸»å‘½ä»¤"""
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing = db.get_creator(platform, uid)
    if existing:
        print(f"âš ï¸ åšä¸»å·²å­˜åœ¨: [{platform}] {name}")
        return

    # æ·»åŠ åˆ°æ•°æ®åº“
    creator_id = db.add_creator(platform, uid, name, category)
    print(f"âœ… æ·»åŠ åšä¸»: [{platform}] {name} (ID: {creator_id})")


def list_creators_command(db):
    """åˆ—å‡ºåšä¸»å‘½ä»¤"""
    creators = db.get_creators()

    if not creators:
        print("ğŸ“­ æš‚æ— åšä¸»")
        return

    print(f"\nğŸ“º åšä¸»åˆ—è¡¨ ({len(creators)} ä¸ª):\n")
    print(f"{'å¹³å°':<12} {'UID':<20} {'åç§°':<20} {'åˆ†ç±»':<10} {'çŠ¶æ€'}")
    print("-" * 80)

    for c in creators:
        status = "âœ… å¯ç”¨" if c["enabled"] else "âŒ ç¦ç”¨"
        print(f"{c['platform']:<12} {c['uid']:<20} {c['name']:<20} {c.get('category', '') or 'N/A':<10} {status}")


def check_once_command(db, config):
    """å•æ¬¡æ£€æŸ¥å‘½ä»¤"""
    creators = db.get_creators(enabled_only=True)

    if not creators:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„åšä¸»ï¼Œè¯·å…ˆæ·»åŠ åšä¸»")
        return

    monitor = VideoMonitor(db)
    monitor.run_once(creators)


def monitor_command(db, config):
    """æŒç»­ç›‘æ§å‘½ä»¤"""
    creators = db.get_creators(enabled_only=True)

    if not creators:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„åšä¸»ï¼Œè¯·å…ˆæ·»åŠ åšä¸»")
        return

    interval = config.get("monitor.check_interval", 300)

    # å‘ç°æ–°è§†é¢‘æ—¶çš„å›è°ƒ
    def on_new_videos(videos):
        print(f"\nğŸ”” æ–°è§†é¢‘é€šçŸ¥: {len(videos)} ä¸ª")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨é€é€šçŸ¥é€»è¾‘

    monitor = VideoMonitor(db)
    monitor.run_loop(creators, interval=interval, callback=on_new_videos)
