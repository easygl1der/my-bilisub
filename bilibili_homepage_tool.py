#!/usr/bin/env python3
"""
Bç«™é¦–é¡µæ¨èé‡‡é›†ä¸åˆ†æä¸€ä½“åŒ–å·¥å…·

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨çˆ¬å–Bç«™é¦–é¡µæ¨èè§†é¢‘
2. æ”¶é›†è§†é¢‘æ ‡é¢˜ã€UPä¸»ã€é“¾æ¥ç­‰å®Œæ•´ä¿¡æ¯
3. ä½¿ç”¨ Gemini API è¿›è¡Œå†…å®¹åˆ†ç±»åˆ†æ
4. ç”Ÿæˆæ¨èåå¥½åˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬ç”¨æ³•ï¼ˆçˆ¬å–å¹¶åˆ†æï¼‰
    python bilibili_homepage_tool.py

    # æŒ‡å®šåˆ·æ–°æ¬¡æ•°å’Œæœ€å¤§è§†é¢‘æ•°
    python bilibili_homepage_tool.py --refresh 15 --max-videos 150

    # ä»…çˆ¬å–ä¸åˆ†æ
    python bilibili_homepage_tool.py --no-analyze

    # ä½¿ç”¨å·²æœ‰æ•°æ®è¿›è¡Œåˆ†æ
    python bilibili_homepage_tool.py --analyze-only --input output/homepage/homepage_videos_20250222.csv

    # æŒ‡å®š Gemini æ¨¡å‹
    python bilibili_homepage_tool.py --model flash

    # è¾“å‡ºåˆ°æŒ‡å®šæ–‡ä»¶
    python bilibili_homepage_tool.py --output my_report.md
"""

import sys
import json
import csv
import argparse
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from urllib.parse import urljoin

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32' and sys.stdout.isatty():
    try:
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except (ValueError, AttributeError):
        pass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "MediaCrawler"))

# ==================== é…ç½® ====================

class Config:
    """å…¨å±€é…ç½®"""
    # é¦–é¡µçˆ¬å–é…ç½®
    HOMEPAGE_URL = "https://www.bilibili.com"
    DEFAULT_REFRESH_COUNT = 10
    DEFAULT_MAX_VIDEOS = 100
    VIDEOS_PER_PAGE = 50

    # æµè§ˆå™¨é…ç½®
    HEADLESS = False  # æ˜¯å¦æ— å¤´æ¨¡å¼
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"

    # è¾“å‡ºé…ç½®
    OUTPUT_DIR = PROJECT_ROOT / "output" / "homepage"

    # Playwrighté…ç½®
    PLAYWRIGHT_TIMEOUT = 30000
    PAGE_LOAD_TIMEOUT = 60000

    # ç™»å½•é…ç½®
    ENABLE_LOGIN = True  # æ˜¯å¦å¯ç”¨ç™»å½•
    COOKIE_FILE = PROJECT_ROOT / "config" / "cookies.txt"  # Cookieæ–‡ä»¶è·¯å¾„
    USER_DATA_DIR = PROJECT_ROOT / "browser_data" / "bilibili_homepage"  # æµè§ˆå™¨ç”¨æˆ·æ•°æ®ç›®å½•


# ==================== æ—¥å¿—å·¥å…· ====================

class Logger:
    """ç®€å•æ—¥å¿—å·¥å…·"""
    @staticmethod
    def info(msg: str):
        print(f"[INFO] {msg}")

    @staticmethod
    def error(msg: str):
        print(f"[ERROR] {msg}")

    @staticmethod
    def success(msg: str):
        print(f"[SUCCESS] {msg}")

    @staticmethod
    def warning(msg: str):
        print(f"[WARNING] {msg}")


# ==================== é¦–é¡µçˆ¬å–å™¨ ====================

class BilibiliHomepageCrawler:
    """Bç«™é¦–é¡µæ¨èçˆ¬å–å™¨"""

    def __init__(self, refresh_count: int = 10, max_videos: int = 100,
                 headless: bool = False, auto_login: bool = True):
        self.refresh_count = refresh_count
        self.max_videos = max_videos
        self.headless = headless
        self.auto_login = auto_login
        self.all_videos = []
        self.seen_bvids = set()

    async def crawl(self) -> List[Dict]:
        """
        çˆ¬å–Bç«™é¦–é¡µæ¨èè§†é¢‘

        Returns:
            è§†é¢‘åˆ—è¡¨ï¼Œæ¯ä¸ªè§†é¢‘åŒ…å«:
            - bvid: è§†é¢‘BVå·
            - title: è§†é¢‘æ ‡é¢˜
            - uploader: UPä¸»åç§°
            - uploader_url: UPä¸»ä¸»é¡µé“¾æ¥
            - video_url: è§†é¢‘é“¾æ¥
            - cover_url: å°é¢é“¾æ¥
            - timestamp: é‡‡é›†æ—¶é—´æˆ³
        """
        try:
            from playwright.async_api import async_playwright

            Logger.info("=" * 60)
            Logger.info("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
            Logger.info("=" * 60)

            # æ£€æŸ¥ Cookie æ–‡ä»¶
            cookies = self._load_cookies()
            if cookies:
                Logger.info(f"âœ… å·²åŠ è½½ {len(cookies)} ä¸ª Cookie")
            else:
                Logger.warning("âš ï¸  æœªæ‰¾åˆ° Cookieï¼Œå°†ä½¿ç”¨æœªç™»å½•çŠ¶æ€çˆ¬å–")

            async with async_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨ï¼Œä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ä»¥ä¿å­˜ç™»å½•çŠ¶æ€
                user_data_dir = str(Config.USER_DATA_DIR)
                browser = await p.chromium.launch_persistent_context(
                    user_data_dir=user_data_dir,
                    headless=self.headless,
                    channel="chrome",
                    viewport={"width": 1920, "height": 1080},
                    user_agent=Config.USER_AGENT,
                )

                # å¦‚æœæœ‰ Cookie æ–‡ä»¶ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
                if cookies:
                    await browser.add_cookies(cookies)

                page = browser.pages[0] if browser.pages else await browser.new_page()

                Logger.info("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
                Logger.info("")

                # æ£€æŸ¥ç™»å½•çŠ¶æ€
                await page.goto(Config.HOMEPAGE_URL, wait_until="networkidle", timeout=Config.PAGE_LOAD_TIMEOUT)
                await asyncio.sleep(2)

                # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•
                is_logged_in = await self._check_login_status(page)
                if is_logged_in:
                    Logger.success("âœ… å·²æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
                else:
                    Logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
                    # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨ç™»å½•ä¸”ä¸æ˜¯æ— å¤´æ¨¡å¼ï¼Œå°è¯•æ‰‹åŠ¨ç™»å½•
                    if self.auto_login and not self.headless:
                        Logger.info("ğŸ”„ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•Bç«™...")
                        Logger.info("   ç™»å½•æˆåŠŸåï¼Œç¨‹åºå°†è‡ªåŠ¨ç»§ç»­...")
                        Logger.info("   ï¼ˆå¦‚éœ€è·³è¿‡ï¼Œè¯·æŒ‰ Ctrl+Cï¼‰")
                        try:
                            # ç­‰å¾…ç”¨æˆ·ç™»å½•ï¼ˆæœ€å¤šç­‰å¾…2åˆ†é’Ÿï¼‰
                            for i in range(120):
                                await asyncio.sleep(1)
                                is_logged_in = await self._check_login_status(page)
                                if is_logged_in:
                                    Logger.success("âœ… ç™»å½•æˆåŠŸï¼")
                                    break
                                if i % 10 == 0 and i > 0:
                                    Logger.info(f"   ç­‰å¾…ç™»å½•ä¸­... ({i}/120ç§’)")
                            else:
                                Logger.warning("â±ï¸  ç­‰å¾…ç™»å½•è¶…æ—¶ï¼Œå°†ä½¿ç”¨æœªç™»å½•çŠ¶æ€ç»§ç»­")
                        except KeyboardInterrupt:
                            Logger.info("â­ï¸  ç”¨æˆ·è·³è¿‡ç™»å½•ï¼Œç»§ç»­æ‰§è¡Œ...")
                    else:
                        Logger.info("   æç¤ºï¼šå¯ä»¥å°† Cookie ä¿å­˜åˆ° config/cookies.txt æ–‡ä»¶ä¸­ä»¥è·å–ä¸ªæ€§åŒ–æ¨è")
                        Logger.info("   æˆ–ä½¿ç”¨ --no-login è·³è¿‡ç™»å½•æç¤º")

                Logger.info("")
                Logger.info("=" * 60)
                Logger.info(f"ğŸ“º å¼€å§‹çˆ¬å–é¦–é¡µæ¨è (åˆ·æ–°{self.refresh_count}æ¬¡, æœ€å¤š{self.max_videos}ä¸ªè§†é¢‘)")
                Logger.info("=" * 60)
                Logger.info("")

                # å¤šæ¬¡åˆ·æ–°è·å–æ¨è
                for i in range(self.refresh_count):
                    await self._crawl_single_refresh(page, i + 1)

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡
                    if len(self.all_videos) >= self.max_videos:
                        Logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§è§†é¢‘æ•°é™åˆ¶: {self.max_videos}")
                        break

                    # æ»šåŠ¨é¡µé¢ç­‰å¾…åŠ è½½æ›´å¤š
                    if i < self.refresh_count - 1:
                        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        await asyncio.sleep(2)

                await browser.close()

            Logger.info("")
            Logger.info("=" * 60)
            Logger.success(f"âœ… çˆ¬å–å®Œæˆ! å…±æ”¶é›† {len(self.all_videos)} ä¸ªè§†é¢‘")
            Logger.info("=" * 60)

            return self.all_videos

        except ImportError:
            Logger.error("âŒ æœªå®‰è£… playwrightï¼Œè¯·å…ˆå®‰è£…: pip install playwright && playwright install chromium")
            return []
        except Exception as e:
            Logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _load_cookies(self):
        """ä»æ–‡ä»¶åŠ è½½ Cookie"""
        cookie_file = Config.COOKIE_FILE
        if not cookie_file.exists():
            return []

        try:
            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookie_str = f.read().strip()

            if not cookie_str:
                return []

            # è§£æ Cookie å­—ç¬¦ä¸²
            cookies = []
            for item in cookie_str.split(';'):
                item = item.strip()
                if '=' in item:
                    name, value = item.split('=', 1)
                    cookies.append({
                        'name': name.strip(),
                        'value': value.strip(),
                        'domain': '.bilibili.com',
                        'path': '/',
                    })

            return cookies

        except Exception as e:
            Logger.warning(f"âš ï¸  åŠ è½½ Cookie å¤±è´¥: {e}")
            return []

    async def _check_login_status(self, page):
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        try:
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒæˆ–ç™»å½•æŒ‰é’®
            await page.wait_for_selector(".header-entry-mini, .nav-user-info", timeout=5000)
            return True
        except:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®
            try:
                login_btn = await page.query_selector(".header-login-entry")
                if login_btn:
                    return False
            except:
                pass
            return False

    async def _crawl_single_refresh(self, page, refresh_num: int):
        """å•æ¬¡åˆ·æ–°çˆ¬å–"""
        try:
            Logger.info(f"[{refresh_num}/{self.refresh_count}] åˆ·æ–°é¡µé¢...")

            # è®¿é—®é¦–é¡µ
            await page.goto(Config.HOMEPAGE_URL, wait_until="networkidle", timeout=Config.PAGE_LOAD_TIMEOUT)
            await asyncio.sleep(2)

            # ç­‰å¾…è§†é¢‘å¡ç‰‡åŠ è½½
            try:
                await page.wait_for_selector("a[href*='/video/BV']", timeout=10000)
            except:
                Logger.warning("æœªæ£€æµ‹åˆ°è§†é¢‘å¡ç‰‡ï¼Œå°è¯•ç»§ç»­...")

            # è·å–æ‰€æœ‰è§†é¢‘å¡ç‰‡
            video_cards = await page.query_selector_all("a[href*='/video/BV']")
            Logger.info(f"[{refresh_num}/{self.refresh_count}] å‘ç° {len(video_cards)} ä¸ªè§†é¢‘å¡ç‰‡")

            # è§£æè§†é¢‘ä¿¡æ¯
            for idx, card in enumerate(video_cards[:Config.VIDEOS_PER_PAGE]):
                try:
                    video_info = await self._parse_video_card(card)
                    if video_info and video_info['bvid'] not in self.seen_bvids:
                        self.seen_bvids.add(video_info['bvid'])
                        self.all_videos.append(video_info)
                        Logger.info(f"  [{len(self.all_videos)}/{self.max_videos}] {video_info['title'][:40]}... @ {video_info['uploader']}")

                        if len(self.all_videos) >= self.max_videos:
                            break

                except Exception as e:
                    Logger.warning(f"  è§£æè§†é¢‘å¡ç‰‡å¤±è´¥: {e}")
                    continue

        except Exception as e:
            Logger.error(f"åˆ·æ–° {refresh_num} å¤±è´¥: {e}")

    async def _parse_video_card(self, card):
        """è§£æå•ä¸ªè§†é¢‘å¡ç‰‡"""
        try:
            # è·å–è§†é¢‘é“¾æ¥
            href = await card.get_attribute("href")
            if not href or "/video/BV" not in href:
                return None

            # æå–BVå·
            if "/video/BV" in href:
                bvid_part = href.split("/video/BV")[-1].split("?")[0].split("/")[0]
                bvid = "BV" + bvid_part
            else:
                return None

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ’­æˆ–å¹¿å‘Š
            is_live = await card.query_selector(".is-live")
            is_ad = await card.query_selector(".ad-card")
            if is_live or is_ad:
                return None

            # è·å–è§†é¢‘æ ‡é¢˜ - å°è¯•å¤šä¸ªé€‰æ‹©å™¨
            title = ""
            title_selectors = [
                ".info-title",
                ".title",
                "h3",
                ".video-title",
                "[class*='title']"
            ]
            for selector in title_selectors:
                title_elem = await card.query_selector(selector)
                if title_elem:
                    title = await title_elem.inner_text()
                    if title.strip():
                        break

            # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•è·å–å±æ€§
            if not title.strip():
                title = await card.get_attribute("title") or ""

            # è·å–UPä¸»ä¿¡æ¯
            uploader = ""
            uploader_url = ""
            uploader_selectors = [
                ".up-name",
                ".author-name",
                ".info--author a",
                ".author",
                "[class*='author'] a",
                "[class*='up']"
            ]
            for selector in uploader_selectors:
                uploader_elem = await card.query_selector(selector)
                if uploader_elem:
                    uploader = await uploader_elem.inner_text()
                    uploader_href = await uploader_elem.get_attribute("href")
                    if uploader_href:
                        uploader_url = uploader_href if uploader_href.startswith("http") else "https:" + uploader_href
                    if uploader.strip():
                        break

            # è·å–å°é¢
            cover_url = ""
            cover_elem = await card.query_selector("img")
            if cover_elem:
                cover_url = await cover_elem.get_attribute("src") or ""
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if cover_url and not cover_url.startswith("http"):
                    cover_url = urljoin("https:", cover_url)

            # æ„å»ºå®Œæ•´è§†é¢‘URL
            video_url = f"https://www.bilibili.com/video/{bvid}"

            return {
                "bvid": bvid,
                "title": title.strip(),
                "uploader": uploader.strip(),
                "uploader_url": uploader_url.strip(),
                "video_url": video_url,
                "cover_url": cover_url.strip(),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        except Exception as e:
            Logger.warning(f"è§£æè§†é¢‘å¡ç‰‡å¤±è´¥: {e}")
            return None


# ==================== æ•°æ®å­˜å‚¨ ====================

class DataStorage:
    """æ•°æ®å­˜å‚¨å·¥å…·"""

    @staticmethod
    def save_to_csv(videos: List[Dict], output_path: str):
        """ä¿å­˜ä¸ºCSVæ ¼å¼"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        if not videos:
            Logger.warning("æ²¡æœ‰è§†é¢‘æ•°æ®å¯ä¿å­˜")
            return

        fieldnames = ['bvid', 'title', 'uploader', 'uploader_url', 'video_url', 'cover_url', 'timestamp']

        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(videos)

        Logger.success(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {output_file}")

    @staticmethod
    def save_to_json(videos: List[Dict], output_path: str):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        data = {
            "é‡‡é›†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "è§†é¢‘æ•°é‡": len(videos),
            "è§†é¢‘åˆ—è¡¨": videos
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        Logger.success(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {output_file}")

    @staticmethod
    def load_from_csv(csv_path: str) -> List[Dict]:
        """ä»CSVè¯»å–"""
        videos = []
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                videos.append({
                    'bvid': row.get('bvid', ''),
                    'title': row.get('title', ''),
                    'uploader': row.get('uploader', ''),
                    'uploader_url': row.get('uploader_url', ''),
                    'video_url': row.get('video_url', ''),
                    'cover_url': row.get('cover_url', ''),
                    'timestamp': row.get('timestamp', ''),
                })
        return videos

    @staticmethod
    def load_from_json(json_path: str) -> List[Dict]:
        """ä»JSONè¯»å–"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data.get('è§†é¢‘åˆ—è¡¨', [])


# ==================== AIåˆ†æ ====================

class GeminiAnalyzer:
    """Gemini AIåˆ†æå™¨"""

    def __init__(self, model: str = 'flash-lite'):
        self.model = model

    def analyze(self, videos: List[Dict]) -> Dict:
        """
        ä½¿ç”¨Geminiåˆ†æè§†é¢‘ç±»å‹

        Returns:
            {'report': str, 'success': bool, 'error': str, 'tokens': int}
        """
        if not videos:
            return {
                'report': 'æ²¡æœ‰è§†é¢‘å¯ä¾›åˆ†æ',
                'success': False,
                'error': 'è§†é¢‘åˆ—è¡¨ä¸ºç©º'
            }

        # å¯¼å…¥Geminiå®¢æˆ·ç«¯
        try:
            sys.path.insert(0, str(PROJECT_ROOT / "analysis"))
            from gemini_subtitle_summary import GeminiClient
        except ImportError:
            return {
                'report': '',
                'success': False,
                'error': 'æ— æ³•å¯¼å…¥Geminiå®¢æˆ·ç«¯ï¼Œè¯·ç¡®ä¿analysis/gemini_subtitle_summary.pyå­˜åœ¨'
            }

        # æ„å»ºè§†é¢‘åˆ—è¡¨æ–‡æœ¬
        videos_text = self._format_videos(videos)

        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹Bç«™é¦–é¡µæ¨èè§†é¢‘åˆ—è¡¨ï¼Œå°†å®ƒä»¬åˆ†ç±»ç»Ÿè®¡ã€‚

è§†é¢‘åˆ—è¡¨:
{videos_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä½¿ç”¨ Markdown æ ¼å¼ï¼‰:

## è§†é¢‘ç±»å‹åˆ†å¸ƒ
| ç±»å‹ | æ•°é‡ | å æ¯” |
|------|------|------|
| AI/å¤§æ¨¡å‹/ç§‘æŠ€ | XX | XX% |
| çŸ¥è¯†/ç¤¾ç§‘/äººæ–‡ | XX | XX% |
| ... | ... | ... |

è¯·æ ¹æ®è§†é¢‘æ ‡é¢˜å’Œ UP ä¸»å‡†ç¡®åˆ†ç±»ï¼Œç¡®ä¿æ€»æ•°ç­‰äº {len(videos)}ã€‚

## æ¨èåå¥½åˆ†æ
[æè¿°è´¦å·çš„æ¨èåå¥½ï¼Œåå‘å“ªäº›ç±»å‹çš„å†…å®¹]
- ä¸»è¦å…´è¶£é¢†åŸŸ: ...
- å†…å®¹æ·±åº¦: ...
- è§†é¢‘é£æ ¼: ...

## é«˜é¢‘ UP ä¸»
| UPä¸» | å‡ºç°æ¬¡æ•° | ä»£è¡¨å†…å®¹ |
|------|----------|----------|
| ... | ... | ... |

## å†…å®¹ç‰¹è‰²åˆ†æ
[åˆ†ææ¨èå†…å®¹çš„ç‰¹ç‚¹ï¼Œå¦‚:]
- è§†é¢‘é•¿åº¦ç‰¹ç‚¹
- UP ä¸»ç±»å‹ï¼ˆä¸ªäºº/æœºæ„ï¼‰
- å†…å®¹æ—¶æ•ˆæ€§
- å…¶ä»–æ˜¾è‘—ç‰¹å¾

## å»ºè®®ä¸æ´å¯Ÿ
[åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®]

---

**è§†é¢‘åˆ†ç±»å‚è€ƒ**:
- AI/å¤§æ¨¡å‹/ç§‘æŠ€: AIå·¥å…·ã€å¤§æ¨¡å‹ã€ç¼–ç¨‹ã€ç§‘æŠ€èµ„è®¯
- çŸ¥è¯†/ç¤¾ç§‘/äººæ–‡: å†å²ã€å“²å­¦ã€ç¤¾ä¼šè§‚å¯Ÿã€äººæ–‡ç§‘æ™®
- è´¢ç»/èŒåœº: ç†è´¢ã€èŒä¸šå‘å±•ã€åˆ›ä¸šã€å•†ä¸šåˆ†æ
- Vlog/æ—…è¡Œ: ç”Ÿæ´»è®°å½•ã€æ—…è¡Œã€æ—¥å¸¸åˆ†äº«
- æ•°ç è¯„æµ‹: æ‰‹æœºã€ç”µè„‘ã€å¤–è®¾è¯„æµ‹
- æ¸¸æˆå¨±ä¹: æ¸¸æˆè§†é¢‘ã€å¨±ä¹å†…å®¹
- åŠ¨æ¼«/å½±è§†: åŠ¨æ¼«ã€ç”µå½±ã€å‰§é›†ç›¸å…³
- éŸ³ä¹/èˆè¹ˆ: éŸ³ä¹ç¿»å”±ã€èˆè¹ˆ
- ç¾é£Ÿ/ç”Ÿæ´»: ç¾é£Ÿã€ç”Ÿæ´»æŠ€å·§
- ç¤¾ä¼šçºªå®: ç¤¾ä¼šæ–°é—»ã€çºªå®æŠ¥é“
- å…¶ä»–: æ— æ³•å½’ç±»çš„"""

        try:
            client = GeminiClient(model=self.model)
            result = client.generate_content(prompt)

            if result['success']:
                return {
                    'report': result['text'],
                    'success': True,
                    'tokens': result.get('tokens', 0),
                }
            else:
                return {
                    'report': '',
                    'success': False,
                    'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                }

        except Exception as e:
            return {
                'report': '',
                'success': False,
                'error': str(e)
            }

    def _format_videos(self, videos: List[Dict]) -> str:
        """æ ¼å¼åŒ–è§†é¢‘åˆ—è¡¨"""
        text = ""
        for i, video in enumerate(videos, 1):
            text += f"{i}. æ ‡é¢˜: {video.get('title', 'æœªçŸ¥')}\n"
            text += f"   UPä¸»: {video.get('uploader', 'æœªçŸ¥')}\n"
            text += f"   é“¾æ¥: {video.get('video_url', '')}\n\n"
        return text


# ==================== ç»Ÿè®¡åˆ†æ ====================

def calculate_statistics(videos: List[Dict]) -> Dict:
    """è®¡ç®—åŸºç¡€ç»Ÿè®¡æ•°æ®"""
    if not videos:
        return {}

    # ç»Ÿè®¡UPä¸»å‡ºç°æ¬¡æ•°
    uploader_count = {}
    for video in videos:
        uploader = video.get('uploader', 'æœªçŸ¥UPä¸»')
        uploader_count[uploader] = uploader_count.get(uploader, 0) + 1

    # æ’åº
    top_uploaders = sorted(uploader_count.items(), key=lambda x: x[1], reverse=True)

    return {
        'æ€»è§†é¢‘æ•°': len(videos),
        'å”¯ä¸€UPä¸»æ•°': len(uploader_count),
        'é«˜é¢‘UPä¸»': top_uploaders,
    }


# ==================== æŠ¥å‘Šç”Ÿæˆ ====================

def generate_report(videos: List[Dict], ai_report: str,
                    stats: Dict, model: str) -> str:
    """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
    from analysis.gemini_subtitle_summary import GEMINI_MODELS

    report_lines = [
        "# Bç«™é¦–é¡µæ¨èåˆ†ææŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**åˆ†æè§†é¢‘æ•°**: {len(videos)}",
        f"**ä½¿ç”¨æ¨¡å‹**: {GEMINI_MODELS.get(model, model)}",
        "",
        "---",
        "",
        "## åŸºç¡€ç»Ÿè®¡",
        "",
        f"- **æ€»è§†é¢‘æ•°**: {stats.get('æ€»è§†é¢‘æ•°', 0)}",
        f"- **å”¯ä¸€UPä¸»æ•°**: {stats.get('å”¯ä¸€UPä¸»æ•°', 0)}",
        "",
        "## é«˜é¢‘ UP ä¸» (å‰10)",
        "",
        "| UPä¸» | å‡ºç°æ¬¡æ•° |",
        "|------|----------|",
    ]

    for uploader, count in stats.get('é«˜é¢‘UPä¸»', [])[:10]:
        report_lines.append(f"| {uploader} | {count} |")

    report_lines.extend([
        "",
        "---",
        "",
        "## AI åˆ†ææŠ¥å‘Š",
        "",
        ai_report,
        "",
        "---",
        "",
        "## é™„å½•: å®Œæ•´è§†é¢‘åˆ—è¡¨",
        "",
    ])

    for i, video in enumerate(videos, 1):
        report_lines.append(f"{i}. **{video.get('title', 'æœªçŸ¥')}**")
        report_lines.append(f"   - UPä¸»: {video.get('uploader', 'æœªçŸ¥')}")
        report_lines.append(f"   - BVå·: {video.get('bvid', '')}")
        report_lines.append(f"   - é“¾æ¥: {video.get('video_url', '')}")
        report_lines.append("")

    return "\n".join(report_lines)


# ==================== ä¸»ç¨‹åº ====================

async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    videos = []

    # å¦‚æœæ˜¯ä»…åˆ†ææ¨¡å¼ï¼Œä»æ–‡ä»¶è¯»å–
    if args.analyze_only:
        if not args.input:
            Logger.error("âŒ --analyze-only æ¨¡å¼éœ€è¦æŒ‡å®š --input å‚æ•°")
            return

        Logger.info("=" * 60)
        Logger.info("ğŸ“‚ ä»æ–‡ä»¶è¯»å–æ•°æ®...")
        Logger.info("=" * 60)

        input_path = Path(args.input)
        if not input_path.exists():
            Logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return

        if input_path.suffix == '.csv':
            videos = DataStorage.load_from_csv(args.input)
        elif input_path.suffix == '.json':
            videos = DataStorage.load_from_json(args.input)
        else:
            Logger.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_path.suffix}")
            return

        if not videos:
            Logger.error("âŒ æ²¡æœ‰è¯»å–åˆ°è§†é¢‘æ•°æ®")
            return

        Logger.success(f"âœ… æˆåŠŸè¯»å– {len(videos)} ä¸ªè§†é¢‘")

    # å¦åˆ™è¿›è¡Œçˆ¬å–
    else:
        crawler = BilibiliHomepageCrawler(
            refresh_count=args.refresh,
            max_videos=args.max_videos,
            headless=args.headless,
            auto_login=not args.no_login
        )
        videos = await crawler.crawl()

        # ä¿å­˜çˆ¬å–çš„æ•°æ®
        if videos:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_path = Config.OUTPUT_DIR / f"homepage_videos_{timestamp}.csv"
            json_path = Config.OUTPUT_DIR / f"homepage_videos_{timestamp}.json"

            DataStorage.save_to_csv(videos, str(csv_path))
            DataStorage.save_to_json(videos, str(json_path))

    # åˆ†æ
    if not args.no_analyze and videos:
        # è®¡ç®—ç»Ÿè®¡
        stats = calculate_statistics(videos)

        Logger.info("")
        Logger.info("=" * 60)
        Logger.info("ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        Logger.info("=" * 60)
        Logger.info(f"  æ€»è§†é¢‘æ•°: {stats['æ€»è§†é¢‘æ•°']}")
        Logger.info(f"  å”¯ä¸€UPä¸»æ•°: {stats['å”¯ä¸€UPä¸»æ•°']}")
        Logger.info(f"  é«˜é¢‘UPä¸» (å‰5):")
        for uploader, count in stats['é«˜é¢‘UPä¸»'][:5]:
            Logger.info(f"    {uploader}: {count} æ¬¡")

        Logger.info("")
        Logger.info("=" * 60)
        Logger.info("ğŸ¤– æ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
        Logger.info("=" * 60)

        analyzer = GeminiAnalyzer(model=args.model)
        result = analyzer.analyze(videos)

        if not result['success']:
            Logger.error(f"âŒ AI åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return

        Logger.success(f"âœ… åˆ†æå®Œæˆ (ä½¿ç”¨ tokens: {result.get('tokens', 0)})")

        # ç”ŸæˆæŠ¥å‘Š
        report = generate_report(videos, result['report'], stats, args.model)

        # ä¿å­˜æŠ¥å‘Š
        if args.output:
            output_path = args.output
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Config.OUTPUT_DIR / f"homepage_analysis_{timestamp}.md"

        report_file = Path(output_path)
        report_file.parent.mkdir(parents=True, exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        Logger.success(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        Logger.info("")
        Logger.info("=" * 60)
        Logger.info("ğŸ“‹ åˆ†ææŠ¥å‘Šæ‘˜è¦:")
        Logger.info("=" * 60)
        preview = result['report'][:2000]
        print(preview)
        if len(result['report']) > 2000:
            print("...")
            print(f"\n(å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_file})")

    Logger.info("")
    Logger.info("=" * 60)
    Logger.success("âœ… å…¨éƒ¨å®Œæˆ!")
    Logger.info("=" * 60)


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="Bç«™é¦–é¡µæ¨èé‡‡é›†ä¸åˆ†æä¸€ä½“åŒ–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # åŸºæœ¬ç”¨æ³•ï¼ˆçˆ¬å–å¹¶åˆ†æï¼‰
    python bilibili_homepage_tool.py

    # æŒ‡å®šåˆ·æ–°æ¬¡æ•°å’Œæœ€å¤§è§†é¢‘æ•°
    python bilibili_homepage_tool.py --refresh 15 --max-videos 150

    # ä»…çˆ¬å–ä¸åˆ†æ
    python bilibili_homepage_tool.py --no-analyze

    # ä½¿ç”¨å·²æœ‰æ•°æ®è¿›è¡Œåˆ†æ
    python bilibili_homepage_tool.py --analyze-only --input output/homepage/homepage_videos_20250222.csv

    # æŒ‡å®š Gemini æ¨¡å‹
    python bilibili_homepage_tool.py --model flash

    # æ— å¤´æ¨¡å¼è¿è¡Œ
    python bilibili_homepage_tool.py --headless
        """
    )

    # çˆ¬å–å‚æ•°
    parser.add_argument('-r', '--refresh', type=int, default=Config.DEFAULT_REFRESH_COUNT,
                        help=f'é¦–é¡µåˆ·æ–°æ¬¡æ•°ï¼ˆé»˜è®¤: {Config.DEFAULT_REFRESH_COUNT}ï¼‰')
    parser.add_argument('-M', '--max-videos', type=int, default=Config.DEFAULT_MAX_VIDEOS,
                        help=f'æœ€å¤§é‡‡é›†è§†é¢‘æ•°ï¼ˆé»˜è®¤: {Config.DEFAULT_MAX_VIDEOS}ï¼‰')
    parser.add_argument('--headless', action='store_true',
                        help='ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆåå°è¿è¡Œæµè§ˆå™¨ï¼‰')
    parser.add_argument('--no-login', action='store_true',
                        help='è·³è¿‡ç™»å½•æç¤ºï¼Œç›´æ¥ä½¿ç”¨æœªç™»å½•çŠ¶æ€çˆ¬å–')

    # åˆ†æå‚æ•°
    parser.add_argument('--no-analyze', action='store_true',
                        help='è·³è¿‡AIåˆ†æï¼Œä»…çˆ¬å–æ•°æ®')
    parser.add_argument('--analyze-only', action='store_true',
                        help='ä»…åˆ†ææ¨¡å¼ï¼Œä¸è¿›è¡Œçˆ¬å–')
    parser.add_argument('-i', '--input', type=str,
                        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº--analyze-onlyæ¨¡å¼ï¼‰')
    parser.add_argument('-m', '--model', choices=['flash', 'flash-lite', 'pro'],
                        default='flash-lite', help='Gemini æ¨¡å‹ï¼ˆé»˜è®¤: flash-liteï¼‰')

    # è¾“å‡ºå‚æ•°
    parser.add_argument('-o', '--output', type=str,
                        help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„')

    args = parser.parse_args()

    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
