#!/usr/bin/env python3
"""
Bç«™è¯„è®ºçˆ¬å–å·¥å…·
ä½¿ç”¨å·²æœ‰çš„ Cookie çˆ¬å–æŒ‡å®šè§†é¢‘çš„è¯„è®º

ä½¿ç”¨æ–¹æ³•:
    python fetch_bili_comments.py
"""

import json
import os
import sys
import time
import re
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlencode

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

try:
    import requests
except ImportError:
    print("è¯·å®‰è£… requests: pip install requests")
    sys.exit(1)


# ============================================================================
# é…ç½®åŒºåŸŸ
# ============================================================================

# Bç«™ Cookie - ä» config/cookies.txt ç»Ÿä¸€è¯»å–
BILI_COOKIE = ""
try:
    import sys
    from pathlib import Path

    # æ·»åŠ  platforms/bilibili ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ cookie_manager
    sys.path.insert(0, str(Path(__file__).parent))
    # æ·»åŠ  config ç›®å½•åˆ°è·¯å¾„
    sys.path.insert(0, str(Path(__file__).parent.parent / "config"))

    # ç®€åŒ–Cookieè¯»å–é€»è¾‘
    cookie_file = Path(__file__).parent.parent / "config" / "cookies.txt"
    if cookie_file.exists():
        with open(cookie_file, 'r', encoding='utf-8') as f:
            content = f.read()
        # æŸ¥æ‰¾biliç›¸å…³çš„Cookie
        for line in content.split('\n'):
            if line.startswith('[bilibili]') or 'bilibili' in line.lower():
                parts = line.split('=', 1)
                if len(parts) == 2:
                    BILI_COOKIE += parts[0].strip() + '=' + parts[1].strip() + '; '
                # å¦‚æœæ‰¾åˆ°biliéƒ¨åˆ†ï¼Œå¼€å§‹æå–
                if line.strip() == '[bilibili]':
                    continue
                break
        BILI_COOKIE = BILI_COOKIE.rstrip('; ')

    if not BILI_COOKIE:
        print("âš ï¸ Bç«™ Cookie æœªé…ç½®ï¼Œè¯·åœ¨ config/cookies.txt ä¸­æ·»åŠ  [bilibili] éƒ¨åˆ†")
    else:
        print("âœ… å·²åŠ è½½ Bç«™ Cookie")

except Exception as e:
    print(f"âš ï¸ æ— æ³•è¯»å– Cookie æ–‡ä»¶: {e}")

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "bili_comments_output"

# æ¯é¡µè¯„è®ºæ•°
PAGE_SIZE = 20

# è¯·æ±‚å»¶è¿Ÿ
REQUEST_DELAY = 1


# ============================================================================
# WBI ç­¾å
# ============================================================================

def get_mixin_key(orig: str) -> str:
    """å¯¹ imgKey å’Œ subKey è¿›è¡Œå­—ç¬¦é¡ºåºæ‰“ä¹±ç¼–ç """
    mixin_key_enc_tab = [
        46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,
        33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,
        61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,
        36, 20, 34, 44, 52
    ]
    return ''.join([orig[i] for i in mixin_key_enc_tab])[:32]


def get_wbi_keys() -> tuple:
    """è·å–æœ€æ–°çš„ img_key å’Œ sub_key"""
    headers = get_headers()
    try:
        resp = requests.get('https://api.bilibili.com/x/web-interface/nav', headers=headers, timeout=10)
        resp.raise_for_status()
        json_content = resp.json()
        img_url = json_content['data']['wbi_img']['img_url']
        sub_url = json_content['data']['wbi_img']['sub_url']
        img_key = img_url.rsplit('/', 1)[1].split('.')[0]
        sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]
        return img_key, sub_key
    except Exception as e:
        print(f"âš ï¸  è·å–WBIå¯†é’¥å¤±è´¥: {e}")
        return '', ''


def sign_wbi_params(params: dict) -> dict:
    """ä¸ºè¯·æ±‚å‚æ•°è¿›è¡Œ wbi ç­¾å"""
    img_key, sub_key = get_wbi_keys()
    if not img_key or not sub_key:
        return params

    mixin_key = get_mixin_key(img_key + sub_key)
    curr_time = round(time.time())
    params['wts'] = curr_time
    params = dict(sorted(params.items()))

    # è¿‡æ»¤ value ä¸­çš„ "!'()*" å­—ç¬¦
    params = {
        k: ''.join(filter(lambda chr: chr not in "!'()*", str(v)))
        for k, v in params.items()
    }
    query = urlencode(params)
    wbi_sign = hashlib.md5((query + mixin_key).encode()).hexdigest()
    params['w_rid'] = wbi_sign
    return params


# ============================================================================
# HTTP å®¢æˆ·ç«¯
# ============================================================================

def get_headers():
    """è·å–è¯·æ±‚å¤´"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.bilibili.com/',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Origin': 'https://www.bilibili.com',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-site',
        'Cookie': BILI_COOKIE,
        'x-requested-with': 'fetch',
    }


class BiliCommentClient:
    """Bç«™è¯„è®ºå®¢æˆ·ç«¯"""

    def __init__(self):
        self.headers = get_headers()

    def get_comments(self, video_id: str, max_count: int = 100) -> List[Dict]:
        """è·å–è§†é¢‘è¯„è®º"""
        all_comments = []

        print(f"\nğŸ“º å¼€å§‹çˆ¬å–Bç«™è§†é¢‘è¯„è®º")
        print(f"   è§†é¢‘ ID: {video_id}")
        print(f"   ç›®æ ‡æ•°é‡: {max_count}\n")

        # åˆ¤æ–­æ˜¯ BV å·è¿˜æ˜¯ AV å·
        if video_id.startswith('BV'):
            oid = self.bv_to_aid(video_id)
            if not oid:
                print("âŒ æ— æ³•è·å–è§†é¢‘ AV å·")
                return []
        else:
            oid = video_id

        page = 1

        while len(all_comments) < max_count:
            print(f"   æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")

            try:
                comments = self._fetch_page(oid, page)
                if not comments:
                    break

                all_comments.extend(comments)
                print(f"   âœ… è·å–åˆ° {len(comments)} æ¡è¯„è®ºï¼Œæ€»è®¡ {len(all_comments)} æ¡")

                if len(comments) < PAGE_SIZE:
                    break

                page += 1
                time.sleep(REQUEST_DELAY)

            except Exception as e:
                print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
                break

        return all_comments[:max_count]

    def _fetch_page(self, oid: int, page: int) -> List[Dict]:
        """è·å–ä¸€é¡µè¯„è®º"""
        # ä½¿ç”¨æ—§ç‰ˆ APIï¼ˆä¸éœ€è¦ WBI ç­¾åï¼‰
        url = "https://api.bilibili.com/x/v2/reply"

        params = {
            'type': 1,
            'oid': oid,
            'mode': 3,  # æŒ‰çƒ­é—¨æ’åº
            'ps': PAGE_SIZE,
            'pn': page,
        }

        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=15)

            data = response.json()

            if data.get('code') == 0:
                replies = data.get('data', {}).get('replies', [])
                if replies is None:
                    replies = []
                return self._parse_comments(replies)
            else:
                print(f"   âš ï¸  API é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return []

        except Exception as e:
            print(f"   âš ï¸  è¯·æ±‚å¼‚å¸¸: {e}")
            return []

    def _parse_comments(self, replies: List[Dict]) -> List[Dict]:
        """è§£æè¯„è®ºæ•°æ®"""
        parsed = []

        for reply in replies:
            try:
                member = reply.get("member", {})
                if member is None:
                    member = {}
                content = reply.get("content", {})
                if content is None:
                    content = {}
                like_count = reply.get("like", 0)

                parsed.append({
                    "comment_id": reply.get("rpid", ""),
                    "content": content.get("message", ""),
                    "likes": like_count,
                    "author": member.get("uname", ""),
                    "create_time": reply.get("ctime", 0),
                    "platform": "bilibili"
                })
            except Exception as e:
                continue

        return parsed

    def bv_to_aid(self, bvid: str) -> Optional[int]:
        """å°† BV å·è½¬æ¢ä¸º AV å·"""
        url = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            data = response.json()

            if data.get('code') == 0:
                aid = data.get('data', {}).get('aid')
                print(f"   âœ… BV å·è½¬ AV å·: {bvid} -> av{aid}")
                return aid
        except Exception as e:
            print(f"   âš ï¸  è½¬æ¢å¤±è´¥: {e}")

        return None


# ============================================================================
# URL è§£æ
# ============================================================================

def extract_video_id(url: str) -> Optional[str]:
    """ä» URL ä¸­æå–è§†é¢‘ ID"""
    # Bç«™ URL æ ¼å¼:
    # https://www.bilibili.com/video/BV1xx411c7mD/
    # https://www.bilibili.com/video/av123456/
    # https://www.bilibili.com/video/b-GCuaGxeZQr6wndCDzE_rcg (Base64æ ¼å¼)
    # https://b23.tv/xxxxx

    # BV å·
    bv_match = re.search(r'BV([a-zA-Z0-9]{10})', url)
    if bv_match:
        return 'BV' + bv_match.group(1)

    # b- æ ¼å¼ (Base64ç¼–ç çš„è§†é¢‘ID)
    b_match = re.search(r'/video/b-([a-zA-Z0-9_-]+)', url)
    if b_match:
        return b_match.group(1)

    # ç›´æ¥è¾“å…¥çš„è§†é¢‘ID
    if re.match(r'^[a-zA-Z0-9_-]{10,}$', url.strip()):
        return url.strip()

    # AV å·
    av_match = re.search(r'av(\d+)', url)
    if av_match:
        return av_match.group(1)

    return None


# ============================================================================
# ä¿å­˜ç»“æœ
# ============================================================================

def save_comments(comments: List[Dict], video_id: str) -> str:
    """ä¿å­˜è¯„è®ºåˆ° CSV"""
    if not comments:
        return None

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    csv_file = os.path.join(OUTPUT_DIR, f"bili_comments_{video_id}_{timestamp}.csv")

    import csv
    with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['comment_id', 'author', 'content', 'likes', 'create_time', 'platform'])
        writer.writeheader()
        writer.writerows(comments)

    return csv_file


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main(url: str = None, count: int = None):
    """ä¸»ç¨‹åº"""
    print("\n" + "="*70)
    print("Bç«™è¯„è®ºçˆ¬å–å·¥å…·")
    print("="*70)

    # è·å–è§†é¢‘é“¾æ¥
    if not url:
        print("\nè¯·è¾“å…¥Bç«™è§†é¢‘é“¾æ¥")
        print("ç¤ºä¾‹: https://www.bilibili.com/video/BV1xx411c7mD/\n")
        url = input("è§†é¢‘é“¾æ¥: ").strip()

    if not url:
        print("âŒ é“¾æ¥ä¸èƒ½ä¸ºç©º")
        return

    # æå–è§†é¢‘ ID
    video_id = extract_video_id(url)
    if not video_id:
        print("âŒ æ— æ³•ä»é“¾æ¥ä¸­æå–è§†é¢‘ ID")
        return

    print(f"\nâœ… è§†é¢‘ ID: {video_id}")

    # è·å–è¯„è®ºæ•°é‡
    if count:
        max_count = count
    else:
        try:
            count_input = input("\nè¦çˆ¬å–å¤šå°‘æ¡è¯„è®º? (é»˜è®¤50): ").strip()
            max_count = int(count_input) if count_input else 50
        except:
            max_count = 50

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = BiliCommentClient()

    # è·å–è¯„è®º
    comments = client.get_comments(video_id, max_count)

    if not comments:
        print("\nâŒ æœªè·å–åˆ°è¯„è®º")
        return

    print(f"\nâœ… æˆåŠŸè·å– {len(comments)} æ¡è¯„è®º")

    # ä¿å­˜ç»“æœ
    csv_file = save_comments(comments, video_id.replace('/', '_'))
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {csv_file}")

    # æ˜¾ç¤ºé¢„è§ˆ
    print("\nğŸ“ è¯„è®ºé¢„è§ˆ:")
    for i, comment in enumerate(comments[:5], 1):
        content = comment.get('content', '')[:80]
        if len(content) == 80:
            content += "..."
        print(f"   {i}. [{comment['likes']}èµ] {comment['author']}: {content}")

    if len(comments) > 5:
        print(f"   ... è¿˜æœ‰ {len(comments) - 5} æ¡")

    print("\n" + "="*70)
    print("âœ… å®Œæˆï¼å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ†æè¯„è®º:")
    print(f"   python comment_analyzer.py -csv {csv_file} -o analysis.md")
    print("="*70)


if __name__ == "__main__":
    import sys
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    url = sys.argv[1] if len(sys.argv) > 1 else None
    count = int(sys.argv[2]) if len(sys.argv) > 2 else None

    try:
        main(url, count)
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
