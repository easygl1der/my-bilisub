#!/usr/bin/env python3
"""
Bç«™è¯„è®ºçˆ¬å–å·¥å…·
ä½¿ç”¨å·²æœ‰çš„ Cookie çˆ¬å–æŒ‡å®šè§†é¢‘çš„è¯„è®ºï¼ˆæŒ‰çƒ­åº¦æ’åºï¼Œè·å–æœ€çƒ­è¯„è®ºï¼‰

ä½¿ç”¨æ–¹æ³•:
    python fetch_bili_comments.py "è§†é¢‘é“¾æ¥" [è¯„è®ºæ•°é‡]

ç¤ºä¾‹:
    # çˆ¬å–å‰50æ¡æœ€çƒ­è¯„è®ºï¼ˆé»˜è®¤ï¼‰
    python fetch_bili_comments.py "https://www.bilibili.com/video/BV1UPZtBiEFS"

    # çˆ¬å–å‰20æ¡æœ€çƒ­è¯„è®º
    python fetch_bili_comments.py "https://www.bilibili.com/video/BV1UPZtBiEFS" 20

    # çˆ¬å–å…¨éƒ¨æœ€çƒ­è¯„è®º
    python fetch_bili_comments.py "https://www.bilibili.com/video/BV1UPZtBiEFS" 0

    # è¾“å‡ºä¸º Markdown æ ¼å¼
    python fetch_bili_comments.py "è§†é¢‘é“¾æ¥" -f md

    # åªçˆ¬å–æœ‰ç‚¹èµæ•°çš„ä¸»è¯„è®º
    python fetch_bili_comments.py "è§†é¢‘é“¾æ¥" --only-liked
"""

import json
import os
import sys
import time
import re
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlencode

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

try:
    import requests
except ImportError:
    print("è¯·å®‰è£… requests: pip install requests")
    sys.exit(1)


# ============================================================================
# é…ç½®åŒºåŸŸ
# ============================================================================

# Bç«™ Cookie - ä» config/cookies.txt ç»Ÿä¸€è¯»å–
BILI_COOKIE = ""
try:
    import sys
    from pathlib import Path

    # è¯»å–Cookieæ–‡ä»¶ - ä»è„šæœ¬è·¯å¾„å¾€ä¸Šä¸¤çº§åˆ°æ ¹ç›®å½•
    script_dir = Path(__file__).resolve().parent
    # script_dir æ˜¯ platforms/bilibili
    # å¾€ä¸Šä¸¤çº§ï¼šplatforms/bilibili -> platforms -> æ ¹ç›®å½•
    root_dir = script_dir.parent.parent
    cookie_file = root_dir / "config" / "cookies.txt"

    if cookie_file.exists():
        with open(cookie_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # æŸ¥æ‰¾ [bilibili] éƒ¨åˆ†
        in_bilibili_section = False
        for line in content.split('\n'):
            line = line.strip()

            # è¿›å…¥bilibiliéƒ¨åˆ†
            if line == '[bilibili]':
                in_bilibili_section = True
                continue
            # é€€å‡ºbilibiliéƒ¨åˆ†
            elif line.startswith('['):
                in_bilibili_section = False
                continue
            # æ”¶é›†Cookie
            elif in_bilibili_section and '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                BILI_COOKIE += f"{key.strip()}={value.strip()}; "

        # ç§»é™¤æœ€åçš„åˆ†å·å’Œç©ºæ ¼
        BILI_COOKIE = BILI_COOKIE.rstrip('; ')

    if BILI_COOKIE:
        print(f"âœ… å·²åŠ è½½ Bç«™ Cookie (é•¿åº¦: {len(BILI_COOKIE)} å­—ç¬¦)")
    else:
        print("âš ï¸ Bç«™ Cookie æœªé…ç½®ï¼Œè¯·åœ¨ config/cookies.txt ä¸­æ·»åŠ  [bilibili] éƒ¨åˆ†")

except Exception as e:
    print(f"âš ï¸ æ— æ³•è¯»å– Cookie æ–‡ä»¶: {e}")

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "bili_comments_output"

# æ¯é¡µè¯„è®ºæ•°
PAGE_SIZE = 20  # Bç«™ API é¡µç é™åˆ¶

# è¯·æ±‚å»¶è¿Ÿ
REQUEST_DELAY = 1


# ============================================================================
# WBI ç­¾å
# ============================================================================

def get_mixin_key(orig: str) -> str:
    """å¯¹ imgKey å’Œ subKey è¿›è¡Œå­—ç¬¦é¡ºåºæ‰“ä¹±ç¼–ç """
    mixin_key_enc_tab = [
        46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,
        33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,
        61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,
        36, 20, 34, 44, 52
    ]
    return ''.join([orig[i] for i in mixin_key_enc_tab])[:32]


def get_wbi_keys() -> tuple:
    """è·å–æœ€æ–°çš„ img_key å’Œ sub_key"""
    headers = get_headers()
    try:
        resp = requests.get('https://api.bilibili.com/x/web-interface/nav', headers=headers, timeout=10)
        resp.raise_for_status()
        json_content = resp.json()
        img_url = json_content['data']['wbi_img']['img_url']
        sub_url = json_content['data']['wbi_img']['sub_url']
        img_key = img_url.rsplit('/', 1)[1].split('.')[0]
        sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]
        return img_key, sub_key
    except Exception as e:
        print(f"âš ï¸  è·å–WBIå¯†é’¥å¤±è´¥: {e}")
        return '', ''


def sign_wbi_params(params: dict) -> dict:
    """ä¸ºè¯·æ±‚å‚æ•°è¿›è¡Œ wbi ç­¾å"""
    img_key, sub_key = get_wbi_keys()
    if not img_key or not sub_key:
        return params

    mixin_key = get_mixin_key(img_key + sub_key)
    curr_time = round(time.time())
    params['wts'] = curr_time
    params = dict(sorted(params.items()))

    # è¿‡æ»¤ value ä¸­çš„ "!'()*" å­—ç¬¦
    params = {
        k: ''.join(filter(lambda chr: chr not in "!'()*", str(v)))
        for k, v in params.items()
    }
    query = urlencode(params)
    wbi_sign = hashlib.md5((query + mixin_key).encode()).hexdigest()
    params['w_rid'] = wbi_sign
    return params


# ============================================================================
# HTTP å®¢æˆ·ç«¯
# ============================================================================

def get_headers():
    """è·å–è¯·æ±‚å¤´"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.bilibili.com/',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Origin': 'https://www.bilibili.com',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-site',
        'Cookie': BILI_COOKIE,
        'x-requested-with': 'fetch',
    }


class BiliCommentClient:
    """Bç«™è¯„è®ºå®¢æˆ·ç«¯"""

    def __init__(self):
        self.headers = get_headers()

    def get_comments(self, video_id: str, max_count: int = 50, only_liked: bool = False) -> List[Dict]:
        """è·å–è§†é¢‘æœ€çƒ­è¯„è®º

        Args:
            video_id: è§†é¢‘ IDï¼ˆBV æˆ– AV å·ï¼‰
            max_count: æœ€å¤§è¯„è®ºæ•°ï¼Œé»˜è®¤ 50 æ¡æœ€çƒ­è¯„è®º
            only_liked: æ˜¯å¦åªçˆ¬å–æœ‰ç‚¹èµæ•°çš„ä¸»è¯„è®ºï¼ˆé»˜è®¤ Falseï¼‰

        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        print(f"\nğŸ“º å¼€å§‹çˆ¬å–Bç«™è§†é¢‘æœ€çƒ­è¯„è®º")
        print(f"   è§†é¢‘ ID: {video_id}")
        print(f"   ğŸ”¥ æ¨¡å¼ï¼šæ”¶é›†è¯„è®ºåæŒ‰ç‚¹èµæ•°æ’åº")
        if only_liked:
            print(f"   ğŸ” è¿‡æ»¤æ¨¡å¼ï¼šä»…çˆ¬å–æœ‰ç‚¹èµæ•°çš„ä¸»è¯„è®º")

        # åˆ¤æ–­æ˜¯ BV å·è¿˜æ˜¯ AV å·
        if video_id.startswith('BV'):
            oid = self.bv_to_aid(video_id)
            if not oid:
                print("âŒ æ— æ³•è·å–è§†é¢‘ AV å·")
                return []
        else:
            oid = video_id

        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†å¤šé¡µè¯„è®ºï¼ˆé»˜è®¤çˆ¬å–å‰10é¡µï¼‰
        all_collected = []
        page = 1
        max_pages_to_fetch = 10  # æœ€å¤šçˆ¬å–10é¡µï¼ˆçº¦200æ¡è¯„è®ºï¼‰
        total_available = None

        print(f"\n   ğŸ“¥ ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†è¯„è®º...")
        while page <= max_pages_to_fetch:
            print(f"   æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")

            try:
                result = self._fetch_page(oid, page)
                comments = result.get('comments', [])

                # è·å–æ€»è¯„è®ºæ•°ï¼ˆé¦–æ¬¡è·å–æ—¶ï¼‰
                if total_available is None:
                    total_available = result.get('total_count', 0)
                    if total_available > 0:
                        print(f"   ğŸ“Š è§†é¢‘å…±æœ‰ {total_available} æ¡ä¸»è¯„è®º")
                    # ç¡®å®šè¦çˆ¬å–çš„é¡µæ•°
                    if total_available < PAGE_SIZE * max_pages_to_fetch:
                        max_pages_to_fetch = (total_available + PAGE_SIZE - 1) // PAGE_SIZE
                        print(f"   ğŸ“„ å°†çˆ¬å– {max_pages_to_fetch} é¡µ")

                if not comments:
                    print(f"   âœ… ç¬¬ {page} é¡µæ— è¯„è®ºï¼Œç»“æŸæ”¶é›†")
                    break

                all_collected.extend(comments)
                print(f"   âœ… æœ¬é¡µè·å– {len(comments)} æ¡ï¼Œç´¯è®¡ {len(all_collected)} æ¡")

                page += 1

            except Exception as e:
                print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
                break

        if not all_collected:
            print("   âŒ æœªè·å–åˆ°ä»»ä½•è¯„è®º")
            return []

        print(f"\n   ğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šæŒ‰ç‚¹èµæ•°æ’åº...")

        # ç¬¬äºŒé˜¶æ®µï¼šæŒ‰ç‚¹èµæ•°é™åºæ’åº
        all_collected.sort(key=lambda x: x.get('likes', 0), reverse=True)

        # ç»Ÿè®¡ç‚¹èµæ•°åˆ†å¸ƒ
        if all_collected:
            max_likes = max(c.get('likes', 0) for c in all_collected)
            min_likes = min(c.get('likes', 0) for c in all_collected)
            avg_likes = sum(c.get('likes', 0) for c in all_collected) / len(all_collected)
            print(f"   ğŸ“ˆ ç‚¹èµç»Ÿè®¡ï¼šæœ€é«˜ {max_likes} èµï¼Œæœ€ä½ {min_likes} èµï¼Œå¹³å‡ {avg_likes:.1f} èµ")

        # ç¡®å®šæœ€ç»ˆæ•°é‡
        target_count = min(max_count if max_count else len(all_collected), len(all_collected))
        hot_comments = all_collected[:target_count]

        print(f"   âœ… ç­›é€‰å‡º {len(hot_comments)} æ¡æœ€çƒ­è¯„è®º")

        # æ˜¾ç¤ºå‰3æ¡æœ€çƒ­è¯„è®ºé¢„è§ˆ
        print(f"\n   ğŸ”¥ çƒ­è¯„é¢„è§ˆï¼š")
        for i, comment in enumerate(hot_comments[:3], 1):
            content = comment.get('content', '')[:40]
            likes = comment.get('likes', 0)
            author = comment.get('author', 'æœªçŸ¥')
            print(f"      {i}. [{likes}èµ] {author}: {content}{'...' if len(comment.get('content', '')) > 40 else ''}")

        # ç¬¬ä¸‰é˜¶æ®µï¼šä¸ºæ¯æ¡çƒ­è¯„è·å–å›å¤
        print(f"\n   ğŸ’¬ ç¬¬ä¸‰é˜¶æ®µï¼šè·å–çƒ­è¯„å›å¤...")

        final_comments = []
        for i, comment in enumerate(hot_comments, 1):
            if i % 10 == 0 or i == len(hot_comments):
                print(f"   è¿›åº¦: {i}/{len(hot_comments)}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ç‚¹èµæ•°è¦æ±‚
            if only_liked and comment.get('likes', 0) <= 0:
                continue

            # è·å–å­è¯„è®º
            rpid = comment.get('comment_id')
            rcount = comment.get('rcount', 0)

            if rcount > 0:
                # éœ€è¦è·å–å­è¯„è®ºï¼Œå…ˆç”¨APIè·å–
                sub_replies = self._fetch_replies(oid, int(rpid))
                comment['replies'] = sub_replies
            else:
                comment['replies'] = []

            final_comments.append(comment)

        print(f"\n   ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡ï¼š")
        print(f"      ğŸ“Š æ€»å…±ç­›é€‰ {len(final_comments)} æ¡çƒ­è¯„")
        print(f"      ğŸ’¬ åŒ…å«å­è¯„è®º {sum(1 for c in final_comments for _ in c.get('replies', []))} æ¡")

        return final_comments

    def _fetch_page(self, oid: int, page: int) -> Dict:
        """è·å–ä¸€é¡µè¯„è®º

        Returns:
            dict: {
                'comments': è¯„è®ºåˆ—è¡¨,
                'total_count': æ€»è¯„è®ºæ•°
            }
        """
        # ä½¿ç”¨æ—§ç‰ˆ APIï¼ˆä¸éœ€è¦ WBI ç­¾åï¼‰
        url = "https://api.bilibili.com/x/v2/reply"

        params = {
            'type': 1,
            'oid': oid,
            'mode': 0,  # æŒ‰çƒ­åº¦æ’åºï¼ˆè·å–æœ€çƒ­è¯„è®ºï¼‰
            'ps': PAGE_SIZE,
            'pn': page,
        }

        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=15)

            data = response.json()

            if data.get('code') == 0:
                replies = data.get('data', {}).get('replies', [])
                page_info = data.get('data', {}).get('page', {})
                total_count = page_info.get('count', 0)

                if replies is None:
                    replies = []

                return {
                    'comments': self._parse_comments(replies, oid=oid),
                    'total_count': total_count
                }
            else:
                print(f"   âš ï¸  API é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return {'comments': [], 'total_count': 0}

        except Exception as e:
            print(f"   âš ï¸  è¯·æ±‚å¼‚å¸¸: {e}")
            return {'comments': [], 'total_count': 0}

    def _fetch_replies(self, oid: int, root_rpid: int) -> List[Dict]:
        """è·å–æŒ‡å®šè¯„è®ºçš„å…¨éƒ¨å­è¯„è®º

        Args:
            oid: è§†é¢‘ IDï¼ˆAV å·ï¼‰
            root_rpid: æ ¹è¯„è®º ID

        Returns:
            å­è¯„è®ºåˆ—è¡¨
        """
        all_replies = []
        page = 1

        while True:
            url = "https://api.bilibili.com/x/v2/reply/reply"
            params = {
                'oid': oid,
                'root': root_rpid,
                'pn': page,
                'ps': PAGE_SIZE,
            }

            try:
                response = requests.get(url, headers=self.headers, params=params, timeout=15)
                data = response.json()

                if data.get('code') == 0:
                    replies_data = data.get('data', {}).get('replies', {})
                    page_info = data.get('data', {}).get('page', {})

                    # replies æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkey æ˜¯å›å¤ç±»å‹
                    for reply_type in replies_data.values():
                        if reply_type:
                            for reply in reply_type:
                                all_replies.append(reply)

                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
                    if page_info.get('num', 0) <= page:
                        break

                    page += 1
                else:
                    break

            except Exception as e:
                print(f"      âš ï¸  è·å–å­è¯„è®ºå¤±è´¥: {e}")
                break

        return self._parse_comments(all_replies)

    def _parse_comment(self, reply: Dict, level: int = 0, oid: int = None) -> Dict:
        """è§£æå•æ¡è¯„è®ºï¼ˆé€’å½’å¤„ç†å›å¤ï¼‰

        Args:
            reply: è¯„è®ºæ•°æ®
            level: è¯„è®ºå±‚çº§
            oid: è§†é¢‘ IDï¼ˆç”¨äºè·å–å­è¯„è®ºï¼‰
        """
        try:
            member = reply.get("member", {})
            if member is None:
                member = {}
            content = reply.get("content", {})
            if content is None:
                content = {}
            like_count = reply.get("like", 0)

            # å¤„ç†å›å¤å…³ç³»
            parent_rpid = reply.get("parent", 0)
            reply_to = None
            if parent_rpid and parent_rpid != 0:
                # è·å–è¢«å›å¤è€…çš„ç”¨æˆ·åï¼ˆéœ€è¦åœ¨ä¸Šå±‚ç»´æŠ¤ä¸€ä¸ªæ˜ å°„ï¼‰
                reply_to = parent_rpid

            # åŸºç¡€è¯„è®ºæ•°æ®
            comment_data = {
                "comment_id": str(reply.get("rpid", "")),
                "content": content.get("message", ""),
                "likes": like_count,
                "author": member.get("uname", ""),
                "author_mid": str(member.get("mid", "")),
                "author_avatar": member.get("face", ""),
                "create_time": reply.get("ctime", 0),
                "reply_to": reply_to,
                "level": level,
                "platform": "bilibili",
                "replies": []
            }

            # è·å–å®Œæ•´çš„å­è¯„è®º
            rpid = reply.get("rpid", 0)
            rcount = reply.get("rcount", 0)  # å­è¯„è®ºæ•°é‡

            # åªæœ‰ä¸»è¯„è®ºæ‰éœ€è¦è·å–å­è¯„è®ºï¼Œä¸”oid ä¸ä¸º None æ—¶
            if level == 0 and oid is not None and rcount > 0:
                # API è¿”å›çš„ replies åªæœ‰å‰ 3 æ¡ï¼Œéœ€è¦å•ç‹¬è¯·æ±‚è·å–å…¨éƒ¨
                sub_replies = self._fetch_replies(oid, rpid)
                comment_data["replies"] = sub_replies
            else:
                # é€’å½’å¤„ç†å­è¯„è®ºï¼ˆä½¿ç”¨ API è¿”å›çš„æ•°æ®ï¼‰
                sub_replies = reply.get("replies", [])
                if sub_replies:
                    for sub_reply in sub_replies:
                        sub_data = self._parse_comment(sub_reply, level + 1)
                        comment_data["replies"].append(sub_data)

            return comment_data
        except Exception as e:
            return None

    def _parse_comments(self, replies: List[Dict], oid: int = None) -> List[Dict]:
        """è§£æè¯„è®ºæ•°æ®ï¼ˆæ”¯æŒåµŒå¥—ç»“æ„ï¼‰

        Args:
            replies: è¯„è®ºåˆ—è¡¨
            oid: è§†é¢‘ IDï¼ˆç”¨äºè·å–å­è¯„è®ºï¼‰
        """
        parsed = []

        for reply in replies:
            comment = self._parse_comment(reply, oid=oid)
            if comment:
                parsed.append(comment)

        return parsed


    def bv_to_aid(self, bvid: str) -> Optional[int]:
        """å°† BV å·è½¬æ¢ä¸º AV å·"""
        url = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            data = response.json()

            if data.get('code') == 0:
                aid = data.get('data', {}).get('aid')
                print(f"   âœ… BV å·è½¬ AV å·: {bvid} -> av{aid}")
                return aid
        except Exception as e:
            print(f"   âš ï¸  è½¬æ¢å¤±è´¥: {e}")

        return None


# ============================================================================
# URL è§£æ
# ============================================================================

def extract_video_id(url: str) -> Optional[str]:
    """ä» URL ä¸­æå–è§†é¢‘ ID"""
    # Bç«™ URL æ ¼å¼:
    # https://www.bilibili.com/video/BV1xx411c7mD/
    # https://www.bilibili.com/video/av123456/
    # https://www.bilibili.com/video/b-GCuaGxeZQr6wndCDzE_rcg (Base64æ ¼å¼)
    # https://b23.tv/xxxxx

    # BV å·
    bv_match = re.search(r'BV([a-zA-Z0-9]{10})', url)
    if bv_match:
        return 'BV' + bv_match.group(1)

    # b- æ ¼å¼ (Base64ç¼–ç çš„è§†é¢‘ID)
    b_match = re.search(r'/video/b-([a-zA-Z0-9_-]+)', url)
    if b_match:
        return b_match.group(1)

    # ç›´æ¥è¾“å…¥çš„è§†é¢‘ID
    if re.match(r'^[a-zA-Z0-9_-]{10,}$', url.strip()):
        return url.strip()

    # AV å·
    av_match = re.search(r'av(\d+)', url)
    if av_match:
        return av_match.group(1)

    return None


# ============================================================================
# ä¿å­˜ç»“æœ
# ============================================================================

def save_comments(comments: List[Dict], video_id: str, output_format: str = "json") -> str:
    """ä¿å­˜è¯„è®ºåˆ° JSON æˆ– Markdownï¼ˆæ”¯æŒåµŒå¥—ç»“æ„ï¼‰"""
    if not comments:
        return None

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # ç»Ÿè®¡æ€»è¯„è®ºæ•°ï¼ˆåŒ…æ‹¬å­è¯„è®ºï¼‰
    def count_all_comments(comment_list):
        """é€’å½’ç»Ÿè®¡æ‰€æœ‰è¯„è®ºæ•°"""
        count = 0
        for comment in comment_list:
            count += 1
            count += count_all_comments(comment.get("replies", []))
        return count

    total_count = count_all_comments(comments)

    if output_format == "json":
        # JSON æ ¼å¼
        json_file = os.path.join(OUTPUT_DIR, f"bili_comments_{video_id}_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                "video_id": video_id,
                "total_comments": total_count,
                "fetch_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "comments": comments
            }, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ JSONå·²ä¿å­˜: {json_file}")
        return json_file

    elif output_format == "md":
        # Markdown æ ¼å¼
        md_file = os.path.join(OUTPUT_DIR, f"bili_comments_{video_id}_{timestamp}.md")

        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# Bç«™è§†é¢‘è¯„è®º\n\n")
            f.write(f"**è§†é¢‘ID**: {video_id}\n\n")
            f.write(f"**è¯„è®ºæ€»æ•°**: {total_count}\n")
            f.write(f"**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")

            # é€’å½’å†™å…¥è¯„è®º
            def write_comment(comment: Dict, level: int = 0):
                """é€’å½’å†™å…¥å•æ¡è¯„è®º"""
                indent = "  " * level
                prefix = "â”œâ”€ " if level > 0 else ""

                # æ—¶é—´æ ¼å¼åŒ–
                create_time = comment.get("create_time", 0)
                time_str = datetime.fromtimestamp(create_time).strftime('%Y-%m-%d %H:%M:%S') if create_time else "æœªçŸ¥"

                f.write(f"{indent}{prefix}**{comment['author']}**\n")
                f.write(f"{indent}    ID: `{comment['comment_id']}`\n")
                f.write(f"{indent}    æ—¶é—´: {time_str}\n")
                f.write(f"{indent}    ç‚¹èµ: {comment['likes']}\n")
                f.write(f"{indent}    å†…å®¹: {comment['content']}\n")

                # é€’å½’å†™å…¥å­è¯„è®º
                for reply in comment.get("replies", []):
                    write_comment(reply, level + 1)

            for comment in comments:
                write_comment(comment)
                f.write("\n")

        print(f"ğŸ’¾ Markdownå·²ä¿å­˜: {md_file}")
        return md_file

    else:
        # é»˜è®¤ CSVï¼ˆæ‰å¹³åŒ–ç»“æ„ï¼‰
        csv_file = os.path.join(OUTPUT_DIR, f"bili_comments_{video_id}_{timestamp}.csv")
        import csv

        # æ‰å¹³åŒ–è¯„è®ºæ•°æ®
        def flatten_comments(comment_list, flat_list=None):
            """é€’å½’æ‰å¹³åŒ–è¯„è®ºåˆ—è¡¨"""
            if flat_list is None:
                flat_list = []

            for comment in comment_list:
                flat_list.append({
                    "comment_id": comment.get("comment_id", ""),
                    "author": comment.get("author", ""),
                    "content": comment.get("content", ""),
                    "likes": comment.get("likes", 0),
                    "create_time": comment.get("create_time", 0),
                    "platform": comment.get("platform", ""),
                    "level": comment.get("level", 0)
                })
                # é€’å½’å¤„ç†å­è¯„è®º
                if comment.get("replies"):
                    flatten_comments(comment["replies"], flat_list)

            return flat_list

        flat_comments = flatten_comments(comments)

        with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['comment_id', 'author', 'content', 'likes', 'create_time', 'platform', 'level'])
            writer.writeheader()
            writer.writerows(flat_comments)

        print(f"ğŸ’¾ CSVå·²ä¿å­˜: {csv_file}")
        return csv_file


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main(url: str = None, count: int = None, output_format: str = "json", only_liked: bool = False):
    """ä¸»ç¨‹åº"""
    print("\n" + "="*70)
    print("Bç«™è¯„è®ºçˆ¬å–å·¥å…·ï¼ˆæœ€çƒ­è¯„è®ºæ¨¡å¼ï¼‰")
    print("="*70)

    # åˆ¤æ–­æ˜¯å¦ä¸ºäº¤äº’å¼æ¨¡å¼
    is_interactive = (url is None)

    # è·å–è§†é¢‘é“¾æ¥
    if not url:
        print("\nè¯·è¾“å…¥Bç«™è§†é¢‘é“¾æ¥")
        print("ç¤ºä¾‹: https://www.bilibili.com/video/BV1xx411c7mD/\n")
        url = input("è§†é¢‘é“¾æ¥: ").strip()

    if not url:
        print("âŒ é“¾æ¥ä¸èƒ½ä¸ºç©º")
        return

    # æå–è§†é¢‘ ID
    video_id = extract_video_id(url)
    if not video_id:
        print("âŒ æ— æ³•ä»é“¾æ¥ä¸­æå–è§†é¢‘ ID")
        return

    print(f"\nâœ… è§†é¢‘ ID: {video_id}")
    print("ğŸ”¥ å½“å‰æ¨¡å¼ï¼šæ”¶é›†å¤šé¡µè¯„è®ºï¼ŒæŒ‰ç‚¹èµæ•°æ’åºè·å–æœ€çƒ­è¯„è®º")

    # è·å–è¯„è®ºæ•°é‡
    if count is not None:
        # å‘½ä»¤è¡ŒæŒ‡å®šäº†æ•°é‡ï¼Œ0 è¡¨ç¤ºçˆ¬å–å…¨éƒ¨æ”¶é›†åˆ°çš„è¯„è®º
        max_count = count if count != 0 else None
    elif is_interactive:
        # äº¤äº’å¼æ¨¡å¼ï¼šè¯¢é—®ç”¨æˆ·
        try:
            count_input = input("\nè¦çˆ¬å–å¤šå°‘æ¡æœ€çƒ­è¯„è®º? (ç•™ç©ºè¡¨ç¤º50æ¡ï¼Œ0è¡¨ç¤ºå…¨éƒ¨æ”¶é›†çš„è¯„è®º): ").strip()
            if count_input == '':
                max_count = 50  # é»˜è®¤50æ¡
            else:
                max_count = int(count_input) if int(count_input) != 0 else None
        except:
            max_count = 50
    else:
        # å‘½ä»¤è¡Œæ¨¡å¼ï¼ŒæœªæŒ‡å®šæ•°é‡ï¼šé»˜è®¤50æ¡
        max_count = 50

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = BiliCommentClient()

    # è·å–è¯„è®º
    comments = client.get_comments(video_id, max_count, only_liked)

    if not comments:
        print("\nâŒ æœªè·å–åˆ°è¯„è®º")
        return

    print(f"\nâœ… æˆåŠŸè·å– {len(comments)} æ¡ä¸»è¯„è®ºï¼ˆåŒ…å«å­è¯„è®ºï¼‰")

    # ä¿å­˜ç»“æœ
    output_file = save_comments(comments, video_id.replace('/', '_'), output_format)

    # æ˜¾ç¤ºé¢„è§ˆï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("\nğŸ“ è¯„è®ºé¢„è§ˆ:")
    def count_all_comments(comment_list):
        count = 0
        for comment in comment_list:
            count += 1
            count += count_all_comments(comment.get("replies", []))
        return count

    total_count = count_all_comments(comments)
    print(f"   ä¸»è¯„è®ºæ•°: {len(comments)} æ¡")
    print(f"   æ€»è¯„è®ºæ•°: {total_count} æ¡ï¼ˆå«å­è¯„è®ºï¼‰")

    for i, comment in enumerate(comments[:3], 1):
        content = comment.get('content', '')[:60]
        if len(content) == 60:
            content += "..."
        sub_count = count_all_comments(comment.get("replies", []))
        print(f"   {i}. [{comment['likes']}èµ] {comment['author']}: {content}")
        if sub_count > 0:
            print(f"      â””â”€ {sub_count} æ¡å›å¤")

    if len(comments) > 3:
        print(f"   ... è¿˜æœ‰ {len(comments) - 3} æ¡ä¸»è¯„è®º")

    print("\n" + "="*70)
    print("âœ… å®Œæˆï¼")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("="*70)


if __name__ == "__main__":
    import sys
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description="Bç«™è¯„è®ºçˆ¬å–å·¥å…·ï¼ˆæ”¯æŒåµŒå¥—å›å¤ï¼‰")
    parser.add_argument("url", help="è§†é¢‘é“¾æ¥")
    parser.add_argument("count", nargs="?", type=int, default=50, help="è¯„è®ºæ•°é‡ï¼ˆé»˜è®¤ 50 æ¡æœ€çƒ­è¯„è®ºï¼Œ0 è¡¨ç¤ºå…¨éƒ¨æœ€çƒ­ï¼‰")
    parser.add_argument("-f", "--format", choices=["json", "md", "csv"], default="json",
                       help="è¾“å‡ºæ ¼å¼ï¼šjsonï¼ˆåµŒå¥—ç»“æ„ï¼‰ã€mdï¼ˆå¯è¯»æ ¼å¼ï¼‰ã€csvï¼ˆæ‰å¹³åŒ–ï¼‰ï¼Œé»˜è®¤json")
    parser.add_argument("--only-liked", action="store_true",
                       help="åªçˆ¬å–æœ‰ç‚¹èµæ•°çš„ä¸»è¯„è®ºï¼ˆå­è¯„è®ºå…¨éƒ¨ä¿ç•™ï¼‰")

    args = parser.parse_args()

    try:
        main(args.url, args.count, args.format, args.only_liked)
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
