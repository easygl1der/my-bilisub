# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (HTML ç‰ˆ - v5)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Cookie ç›´æ¥è®¿é—®ç¬”è®°é¡µé¢
2. ä» HTML ä¸­æå–æ‰€æœ‰è¯„è®ºï¼ˆå•å±‚æ‰å¹³ï¼‰
3. åœ¨ Python ç«¯æŒ‰"å›å¤ XXX : â€¦"è§„åˆ™æ„å»ºè¯„è®º-å›å¤æ ‘
4. è¾“å‡º JSONï¼šæ¯æ¡é¡¶çº§è¯„è®º + repliesï¼ˆä¸é‡å¤è‡ªå·±ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python fetch_xhs_comments_v5.py "ç¬”è®°é“¾æ¥"

éœ€è¦å…ˆå®‰è£…ï¼š
    pip install playwright
    playwright install chromium
"""

import asyncio
import json
import sys
import re
from pathlib import Path
from datetime import datetime

if sys.platform == "win32":
    import io

    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

from playwright.async_api import async_playwright

OUTPUT_DIR = Path("xhs_comments_output")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==================== JSï¼šè¯Šæ–­ DOMï¼Œæ‰¾è¯„è®ºç›¸å…³ç±» ====================

_JS_DIAGNOSE = r"""
(function () {
    var result = {
        all_comment_related: [],
        sample_html: ""
    };

    var all_els = document.querySelectorAll("*");
    var seen_cls = {};
    for (var i = 0; i < all_els.length; i++) {
        var el = all_els[i];
        var cls = el.className;
        if (typeof cls === "string" && cls.toLowerCase().indexOf("comment") !== -1) {
            var parts = cls.trim().split(/\s+/);
            for (var j = 0; j < parts.length; j++) {
                var p = parts[j];
                if (p && !seen_cls[p]) {
                    seen_cls[p] = true;
                    var count = document.querySelectorAll("." + CSS.escape(p)).length;
                    result.all_comment_related.push({ cls: p, count: count });
                }
            }
        }
    }

    result.all_comment_related.sort(function (a, b) {
        return b.count - a.count;
    });

    if (result.all_comment_related.length > 0) {
        var topCls = result.all_comment_related[0].cls;
        var topEl = document.querySelector("." + CSS.escape(topCls));
        if (topEl) {
            result.sample_html = topEl.outerHTML.substring(0, 3000);
        }
    }

    return result;
})();
"""

# ==================== JSï¼šæ‰å¹³æå–æ‰€æœ‰è¯„è®º ====================


def build_extract_js(root_cls: str) -> str:
    """
    æ ¹æ®æ ¹è¯„è®ºç±»åç”Ÿæˆ JSï¼Œè¿”å›æ‰å¹³åˆ—è¡¨ï¼š
    {id, nickname, content, like_count, create_time}
    """
    return rf"""
(function(rootCls) {{
    var comments = [];
    var seen = {{}};

    function getText(el) {{
        return el ? el.textContent.trim() : "";
    }}

    function getUniqueId(item, index) {{
        var id = item.getAttribute("data-id")
                || item.getAttribute("data-comment-id")
                || item.getAttribute("id")
                || "";
        if (!id) id = "comment_" + index;
        return id;
    }}

    function findContent(item) {{
        var candidates = [
            ".content", "[class*='content']",
            "[class*='text']", "[class*='body']",
            "span", "p"
        ];
        for (var k = 0; k < candidates.length; k++) {{
            var el = item.querySelector(candidates[k]);
            if (el) {{
                var t = el.textContent.trim();
                if (t.length > 2) return t;
            }}
        }}
        return item.textContent.trim().substring(0, 200);
    }}

    function findAuthor(item) {{
        var candidates = [
            "[class*='nick']", "[class*='name']",
            "[class*='author']", "[class*='user']",
            ".nickname", ".username"
        ];
        for (var k = 0; k < candidates.length; k++) {{
            var el = item.querySelector(candidates[k]);
            if (el) {{
                var t = el.textContent.trim();
                if (t.length > 0 && t.length < 50) return t;
            }}
        }}
        return "æœªçŸ¥ç”¨æˆ·";
    }}

    function findTime(item) {{
        var candidates = [
            "[class*='time']", "[class*='date']",
            ".date", ".time", "time"
        ];
        for (var k = 0; k < candidates.length; k++) {{
            var el = item.querySelector(candidates[k]);
            if (el) return el.textContent.trim();
        }}
        return "";
    }}

    function findLikes(item) {{
        var candidates = [
            "[class*='like']", "[class*='count']",
            "[class*='thumb']", "[class*='heart']"
        ];
        for (var k = 0; k < candidates.length; k++) {{
            var el = item.querySelector(candidates[k]);
            if (el) {{
                var num = parseInt(el.textContent.replace(/[^0-9]/g, ""), 10);
                if (!isNaN(num)) return num;
            }}
        }}
        return 0;
    }}

    function parseItem(item, index) {{
        var commentId = getUniqueId(item, index);
        if (seen[commentId]) return null;
        seen[commentId] = true;

        var content = findContent(item);
        if (!content || content.length < 1) return null;

        // æŸ¥æ‰¾çˆ¶è¯„è®ºIDï¼ˆæ”¯æŒå¤šç§å±æ€§æ ¼å¼ï¼‰
        var parentCommentId = item.getAttribute("data-parent-id")
                        || item.getAttribute("data-reply-to")
                        || item.getAttribute("data-root-id")
                        || "";

        // æŸ¥æ‰¾è¢«å›å¤è€…æ˜µç§°ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        var replyToEl = item.querySelector("[class*='reply-to'], [class*='at'], [class*='mention']");
        var replyToName = replyToEl ? replyToEl.textContent.replace(/@/g, '').trim() : "";

        return {{
            id:          commentId,
            parent_id:   parentCommentId,
            reply_to_name: replyToName,
            nickname:    findAuthor(item),
            content:     content,
            like_count:  findLikes(item),
            create_time: findTime(item)
        }};
    }}

    var rootItems = document.querySelectorAll("." + rootCls);
    var idx = 0;

    for (var i = 0; i < rootItems.length; i++) {{
        var root = rootItems[i];
        var data = parseItem(root, idx++);
        if (!data) continue;
        comments.push(data);
    }}

    return comments;
}})({json.dumps(root_cls)});
"""


# ==================== Cookie ç®¡ç† ====================


def load_cookies():
    cookie_file = Path("config/cookies.txt")
    if not cookie_file.exists():
        print("âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: config/cookies.txt")
        return None
    with open(cookie_file, "r", encoding="utf-8") as f:
        content = f.read()
    m = re.search(r"xiaohongshu_full=([^\n]+)", content)
    if m:
        return m.group(1)
    xhs_section = re.search(r"\[xiaohongshu\](.*?)(\[|$)", content, re.DOTALL)
    if xhs_section:
        section = xhs_section.group(1)
        cookies = []
        for line in section.split("\n"):
            line = line.strip()
            if "=" in line and not line.startswith("#"):
                key, value = line.split("=", 1)
                cookies.append(f"{key.strip()}={value.strip()}")
        return "; ".join(cookies)
    return None


def parse_cookies(cookie_str):
    cookies = []
    for item in cookie_str.split(";"):
        item = item.strip()
        if "=" in item:
            key, value = item.split("=", 1)
            cookies.append(
                {
                    "name": key.strip(),
                    "value": value.strip(),
                    "domain": ".xiaohongshu.com",
                    "path": "/",
                }
            )
    return cookies


def extract_note_id(url):
    if "/explore/" in url:
        m = re.search(r"/explore/([a-f0-9]{24})", url)
        if m:
            return m.group(1)
    m = re.search(r"([a-f0-9]{24})", url, re.IGNORECASE)
    if m:
        return m.group(0)
    return None


# ==================== è¯„è®ºæå–å™¨ ====================


class XHSCommentExtractor:
    def __init__(self, note_id: str):
        self.note_id = note_id
        self.root_cls: str | None = None

    async def diagnose_dom(self, page) -> bool:
        print("\n  ğŸ” è¯Šæ–­è¯„è®ºåŒº DOM ç»“æ„...")
        result = await page.evaluate(_JS_DIAGNOSE)

        classes = result.get("all_comment_related", [])
        if not classes:
            print("  âš ï¸ æœªæ‰¾åˆ°åŒ…å« 'comment' çš„ç±»åï¼Œå¯èƒ½æœªç™»å½•æˆ–è¯„è®ºåŒºæœªåŠ è½½")
            return False

        print("  ğŸ“‹ å« 'comment' çš„ç±»å (Top 20):")
        for item in classes[:20]:
            print(f"     .{item['cls']}  ({item['count']} ä¸ªå…ƒç´ )")

        # ç®€å•ç­–ç•¥ï¼šæ•°é‡æœ€å¤šçš„ç±»ä½œä¸ºæ ¹è¯„è®º
        self.root_cls = classes[0]["cls"]
        print(f"\n  âœ… é€‰å®šæ ¹è¯„è®ºç±»: .{self.root_cls} ({classes[0]['count']} ä¸ªå…ƒç´ )")

        if result.get("sample_html"):
            print("\n  ğŸ” ç¬¬ä¸€ä¸ªè¯„è®ºå…ƒç´  HTML ç‰‡æ®µï¼ˆå‰ 500 å­—ç¬¦ï¼‰:")
            print("  " + result["sample_html"][:500].replace("\n", "\n  "))

        return True

    async def extract_flat_comments(self, page):
        print("\n  ğŸ“ æå–æ‰å¹³è¯„è®ºåˆ—è¡¨...")
        if not self.root_cls:
            print("  âŒ æœªç¡®å®šæ ¹è¯„è®ºç±»ï¼Œæ— æ³•æå–")
            return []

        js = build_extract_js(self.root_cls)
        comments = await page.evaluate(js)
        if not comments:
            print("  âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è¯„è®º")
            return []

        print(f"  âœ… æ‰å¹³è¯„è®ºæ•°: {len(comments)}")
        return comments

    def build_comment_tree(self, comments):
        """
        æ„å»ºå¤šå±‚åµŒå¥—è¯„è®ºæ ‘

        æ”¯æŒ:
        - åŸºäº parent_id çš„ç›´æ¥å…³ç³»ï¼ˆä¼˜å…ˆï¼‰
        - åŸºäº reply_to_name çš„é—´æ¥å…³ç³»ï¼ˆå›é€€ï¼‰
        - åŸºäºæ–‡æœ¬è§„åˆ™è¯†åˆ«"å›å¤ XXX : â€¦"ï¼ˆæœ€åå›é€€ï¼‰

        comments: [{id, parent_id, reply_to_name, nickname, content, like_count, create_time}]
        """
        # æ„å»ºIDåˆ°è¯„è®ºçš„æ˜ å°„
        id2node: dict[str, dict] = {c['id']: c for c in comments}

        # ä½œè€… -> ç‚¹èµæœ€é«˜çš„é‚£æ¡è¯„è®ºï¼ˆä½œä¸ºè¢«å›å¤ anchorï¼‰
        by_author: dict[str, dict] = {}
        for c in comments:
            name = c["nickname"]
            if name not in by_author or c["like_count"] > by_author[name]["like_count"]:
                by_author[name] = c

        # åˆå§‹åŒ–æ‰€æœ‰è¯„è®ºçš„replies
        for c in comments:
            c['replies'] = []

        # æ„å»ºæ ‘ç»“æ„
        roots = []
        for c in comments:
            parent_id = c.get('parent_id', '')

            # ä¼˜å…ˆï¼šåŸºäºparent_idæ„å»ºç›´æ¥å…³ç³»
            if parent_id and parent_id in id2node:
                id2node[parent_id]['replies'].append(c)
                continue

            # å›é€€ï¼šåŸºäºreply_to_nameæ„å»ºï¼ˆå½“parent_idä¸å¯ç”¨æ—¶ï¼‰
            reply_to_name = c.get('reply_to_name', '')
            if reply_to_name:
                # æ‰¾åˆ°è¯¥ä½œè€…çš„è¯„è®ºä¸­ç‚¹èµæœ€é«˜çš„ä½œä¸ºçˆ¶è¯„è®º
                candidates = [n for n in comments if n['nickname'] == reply_to_name]
                if candidates:
                    best_parent = max(candidates, key=lambda x: x['like_count'])
                    best_parent['replies'].append(c)
                    continue

            # æœ€åå›é€€ï¼šåŸºäºæ–‡æœ¬è§„åˆ™è¯†åˆ«"å›å¤ XXX : â€¦"
            reply_pattern = re.compile(r"^å›å¤\s+(.+?)\s*[:ï¼š]")
            m = reply_pattern.match(c.get("content", ""))
            if m:
                target_name = m.group(1).strip()
                # é¿å…è‡ªå·±å›å¤è‡ªå·±
                if target_name != c["nickname"]:
                    anchor = by_author.get(target_name)
                    if anchor:
                        anchor['replies'].append(c)
                        continue

            # æ—¢æ²¡æœ‰parent_idä¹Ÿæ²¡æœ‰reply_to_nameä¹Ÿæ²¡æœ‰åŒ¹é…æ–‡æœ¬ï¼Œä½œä¸ºé¡¶çº§è¯„è®º
            roots.append(c)

        # é€’å½’æ’åºæ‰€æœ‰å›å¤
        for r in roots:
            self._sort_replies(r)

        return roots

    def _sort_replies(self, node):
        """é€’å½’æ’åºæ‰€æœ‰å›å¤"""
        node['replies'].sort(key=lambda x: x['like_count'], reverse=True)
        for reply in node['replies']:
            self._sort_replies(reply)

    def save_json(self, tree, total):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = OUTPUT_DIR / f"xhs_comments_{self.note_id}_{timestamp}.json"
        result = {
            "note_id": self.note_id,
            "total_comments": total,
            "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "comments": tree,
        }
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON å·²ä¿å­˜: {output_file}")
        return output_file


# ==================== ä¸»æµç¨‹ ====================


async def main_async(url: str | None = None, headless: bool = False):
    print("\n" + "=" * 80)
    print("å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (HTML ç‰ˆ - v5)")
    print("=" * 80)

    print("\n[æ­¥éª¤ 1] åŠ è½½ Cookie")
    cookie_str = load_cookies()
    if not cookie_str:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆ Cookie")
        return
    print("âœ… Cookie å·²åŠ è½½")

    print("\n[æ­¥éª¤ 2] è§£æç¬”è®°é“¾æ¥")
    if not url:
        print("è¯·è¾“å…¥å°çº¢ä¹¦ç¬”è®°é“¾æ¥:")
        url = input("ç¬”è®°é“¾æ¥: ").strip()

    note_id = extract_note_id(url)
    if not note_id:
        print(f"âŒ æ— æ³•ä»é“¾æ¥æå–ç¬”è®° ID: {url}")
        return
    print(f"âœ… ç¬”è®° ID: {note_id}")

    page_url = url if "?xsec_token=" in url else f"https://www.xiaohongshu.com/explore/{note_id}"
    print(f"ğŸ“ é¡µé¢ URL: {page_url}")

    print("\n[æ­¥éª¤ 3] è®¿é—®é¡µé¢å¹¶æå–è¯„è®º")
    print("-" * 80)
    print(f"æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if headless else 'æœ‰å¤´æ¨¡å¼'}")

    extractor = XHSCommentExtractor(note_id)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        cookies = parse_cookies(cookie_str)

        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1920, "height": 1080},
        )
        if cookies:
            await context.add_cookies(cookies)
            print(f"âœ… å·²è®¾ç½® {len(cookies)} ä¸ª Cookie")

        page = await context.new_page()
        print("\nğŸ“¡ æ­£åœ¨è®¿é—®ç¬”è®°é¡µé¢...")
        print(f"   {page_url}")

        try:
            await page.goto(page_url, wait_until="networkidle", timeout=60000)
            print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
            await asyncio.sleep(3)

            # è¯Šæ–­ DOM
            ok = await extractor.diagnose_dom(page)
            if not ok:
                html = await page.content()
                debug_file = OUTPUT_DIR / f"debug_{note_id}.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html)
                print(f"   è°ƒè¯• HTML å·²ä¿å­˜: {debug_file}")
                await browser.close()
                return

            # æ·±åº¦æ»šåŠ¨ + å±•å¼€
            print("\n  ğŸ”„ æ­£åœ¨æ·±åº¦åŠ è½½è¯„è®ºï¼ˆæ»šåŠ¨ + å±•å¼€æ‰€æœ‰å›å¤ï¼‰...")
            count_sel = f".{extractor.root_cls}"

            last_count = 0
            stable_rounds = 0
            MAX_STABLE = 5
            MAX_LOOPS = 60

            for loop_i in range(MAX_LOOPS):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(1.5)

                # å®‰å…¨ç‚¹å‡»"å±•å¼€/æ›´å¤šå›å¤"ï¼Œé¿å…è¯¯ç‚¹ a é“¾æ¥
                try:
                    clicked = await page.evaluate(
                        r"""
                    (function() {
                        var clicked = 0;
                        var keywords = ['å±•å¼€', 'æŸ¥çœ‹æ›´å¤šå›å¤', 'æ›´å¤šå›å¤', 'å±•å¼€å›å¤'];
                        var els = document.querySelectorAll('span, div, button');
                        for (var i = 0; i < els.length; i++) {
                            var el = els[i];
                            var txt = el.textContent.trim();
                            if (txt.length < 15 && keywords.some(function(k){ return txt.indexOf(k) !== -1; })) {
                                if (el.tagName !== 'A' && !el.closest('a') && !el.closest('[href]')) {
                                    try { el.click(); clicked++; } catch(e) {}
                                }
                            }
                        }
                        return clicked;
                    })()
                    """
                    )
                    if clicked > 0:
                        await asyncio.sleep(0.8)
                except Exception:
                    pass

                # é˜²æ­¢è·³è½¬åˆ°ç”¨æˆ·ä¸»é¡µ
                current_url = page.url
                if note_id not in current_url:
                    print(f"  âš ï¸ æ£€æµ‹åˆ°é¡µé¢è·³è½¬ ({current_url[:60]}...)ï¼Œæ­£åœ¨è¿”å›...")
                    await page.goto(page_url, wait_until="networkidle", timeout=30000)
                    await asyncio.sleep(2)

                current_count = await page.evaluate(
                    f"document.querySelectorAll({json.dumps(count_sel)}).length"
                )

                if current_count > last_count:
                    print(f"     [{loop_i+1}] å·²å‘ç° {current_count} æ¡è¯„è®ºé¡¹...")
                    last_count = current_count
                    stable_rounds = 0
                else:
                    stable_rounds += 1
                    if stable_rounds >= MAX_STABLE:
                        print(f"     è¿ç»­ {MAX_STABLE} æ¬¡æ— æ–°å¢ï¼Œåˆ¤å®šåŠ è½½å®Œæ¯•")
                        break

            print(f"  âœ… æ»šåŠ¨å®Œæˆï¼Œå…± {last_count} æ¡è¯„è®ºé¡¹")

            # æ‰å¹³æå–
            flat_comments = await extractor.extract_flat_comments(page)
            if not flat_comments:
                html = await page.content()
                debug_file = OUTPUT_DIR / f"debug_{note_id}.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html)
                print(f"âš ï¸ æœªæå–åˆ°è¯„è®ºï¼Œè°ƒè¯• HTML å·²ä¿å­˜: {debug_file}")
                await browser.close()
                return

            # æ„å»ºæ ‘
            print("\n  ğŸŒ³ æ„å»ºè¯„è®ºæ ‘ï¼ˆæŒ‰èµæ•°æ’åº + å›å¤æŒ‚è½½ï¼‰...")
            tree = extractor.build_comment_tree(flat_comments)
            print("  âœ… è¯„è®ºæ ‘æ„å»ºå®Œæˆ")

            # ä¿å­˜ JSON
            print("\n[æ­¥éª¤ 4] ä¿å­˜ç»“æœ")
            extractor.save_json(tree, len(flat_comments))

            # ç®€è¦ç»Ÿè®¡
            print("\n[æ­¥éª¤ 5] ç»Ÿè®¡æ‘˜è¦")
            top_level = len(tree)
            reply_count = sum(len(t["replies"]) for t in tree)
            print("-" * 80)
            print(f"  æ€»è¯„è®ºæ•°ï¼ˆå«å›å¤ï¼‰: {len(flat_comments)}")
            print(f"  é¡¶çº§è¯„è®ºæ•°        : {top_level}")
            print(f"  å›å¤è¯„è®ºæ•°        : {reply_count}")
            print("-" * 80)

            await browser.close()

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            try:
                await browser.close()
            except Exception:
                pass

    print("\n" + "=" * 80)
    print("âœ… å®Œæˆï¼")
    print("=" * 80 + "\n")


def main(url: str | None = None, headless: bool = False):
    asyncio.run(main_async(url, headless))


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (HTML ç‰ˆ - v5)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("url", nargs="?", help="å°çº¢ä¹¦ç¬”è®°é“¾æ¥")
    parser.add_argument("--headless", action="store_true", help="æ— å¤´æ¨¡å¼")
    args = parser.parse_args()

    try:
        main(args.url, args.headless)
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
