# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (HTML çˆ¬å–ç‰ˆ)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Cookie ç›´æ¥è®¿é—®ç¬”è®°é¡µé¢
2. ä» HTML ä¸­æå–è¯„è®ºæ•°æ®
3. ç”Ÿæˆ JSON å±‚çº§æ–‡ä»¶
4. åŒ…å«å›å¤å…³ç³»ï¼šè°å›å¤äº†è°ï¼Œè°å‘è¨€äº†

ä½¿ç”¨æ–¹æ³•:
    python fetch_xhs_comments.py "ç¬”è®°é“¾æ¥"

ç¤ºä¾‹:
    python fetch_xhs_comments.py "https://www.xiaohongshu.com/explore/694f9e5300000001e013674"
"""

import asyncio
import json
import sys
import re
from pathlib import Path
from datetime import datetime

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from playwright.async_api import async_playwright


# ==================== é…ç½® ====================
OUTPUT_DIR = Path("xhs_comments_output")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# ==================== Cookie ç®¡ç† ====================

def load_cookies():
    """ä» config/cookies.txt è¯»å– Cookie"""
    cookie_file = Path("config/cookies.txt")

    if not cookie_file.exists():
        print("âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: config/cookies.txt")
        return None

    with open(cookie_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾ xiaohongshu_full= æ ¼å¼
    match = re.search(r'xiaohongshu_full=([^\n]+)', content)
    if match:
        return match.group(1)

    # æŸ¥æ‰¾ [xiaohongshu] éƒ¨åˆ†
    xhs_section = re.search(r'\[xiaohongshu\](.*?)\[', content, re.DOTALL)
    if xhs_section:
        section = xhs_section.group(1)
        cookies = []
        for line in section.split('\n'):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                cookies.append(f"{key.strip()}={value.strip()}")
        return '; '.join(cookies)

    return None


def extract_note_id(url):
    """ä» URL ä¸­æå–ç¬”è®° ID"""
    if '/explore/' in url:
        match = re.search(r'/explore/([a-f0-9]{24})', url)
        if match:
            return match.group(1)
    match = re.search(r'([a-f0-9]{24})', url, re.IGNORECASE)
    if match:
        return match.group(0)
    return None


def parse_cookies(cookie_str):
    """å°† Cookie å­—ç¬¦ä¸²è½¬æ¢ä¸º Playwright æ ¼å¼"""
    cookies = []
    for item in cookie_str.split(';'):
        item = item.strip()
        if '=' in item:
            key, value = item.split('=', 1)
            cookies.append({
                'name': key.strip(),
                'value': value.strip(),
                'domain': '.xiaohongshu.com',
                'path': '/'
            })
    return cookies


# ==================== è¯„è®ºæå– ====================

class XHSCommentExtractor:
    """å°çº¢ä¹¦è¯„è®ºæå–å™¨"""

    def __init__(self, note_id):
        self.note_id = note_id
        self.all_comments = {}  # {comment_id: comment_data}
        self.comment_tree = []  # æ ‘å½¢ç»“æ„

    async def extract_comments_from_html(self, page):
        """ä» HTML é¡µé¢æå–è¯„è®º"""
        print("\n  ğŸ“ æ­£åœ¨æå–è¯„è®ºæ•°æ®...")

        # ä½¿ç”¨ JavaScript åœ¨é¡µé¢ä¸­æ‰§è¡Œï¼Œæå–è¯„è®ºæ•°æ®
        comments_data = await page.evaluate('''
            () => {
                const comments = [];
                const seen = new Set();

                // æŸ¥æ‰¾æ‰€æœ‰è¯„è®ºå®¹å™¨
                const commentItems = document.querySelectorAll('[class*="comment"], [class*="Comment"]');

                for (const item of commentItems) {
                    // è·³è¿‡æ²¡æœ‰å†…å®¹çš„
                    const contentEl = item.querySelector('[class*="content"], [class*="text"]');
                    if (!contentEl || !contentEl.textContent.trim()) continue;

                    // æå–è¯„è®º ID
                    let commentId = item.getAttribute('data-id') ||
                                     item.getAttribute('data-comment-id') ||
                                     item.querySelector('[class*="id"]')?.textContent ||
                                     Math.random().toString(36).substr(2, 9);

                    if (seen.has(commentId)) continue;
                    seen.add(commentId);

                    // æå–å†…å®¹
                    const content = contentEl.textContent.trim();

                    // æå–ç‚¹èµæ•°
                    let likes = 0;
                    const likeEl = item.querySelector('[class*="like"], [class*="count"], [class*="num"]');
                    if (likeEl) {
                        const text = likeEl.textContent.trim();
                        const num = parseInt(text.replace(/\\D/g, ''));
                        if (!isNaN(num)) likes = num;
                    }

                    // æå–ä½œè€…ä¿¡æ¯
                    const authorEl = item.querySelector('[class*="author"], [class*="user"], a[href*="/user/profile/"]');
                    const author = {
                        nickname: authorEl?.textContent?.trim() || 'æœªçŸ¥ç”¨æˆ·',
                        avatar: authorEl?.querySelector('img')?.src || ''
                    };

                    // æå–æ—¶é—´
                    let createTime = '';
                    const timeEl = item.querySelector('[class*="time"], time, [datetime]');
                    if (timeEl) {
                        createTime = timeEl.textContent.trim() || timeEl.getAttribute('datetime') || '';
                    }

                    // æ£€æµ‹æ˜¯å¦æ˜¯å›å¤ï¼ˆé€šè¿‡æ ·å¼æˆ–ç»“æ„åˆ¤æ–­ï¼‰
                    let isReply = false;
                    let parentId = null;

                    const parentComment = item.closest('[class*="reply"], [class*="sub"]');
                    if (parentComment) {
                        isReply = true;
                        // å°è¯•æ‰¾åˆ°çˆ¶è¯„è®ºID
                        const parentContainer = parentComment.closest('[class*="comment"]');
                        if (parentContainer) {
                            parentId = parentContainer.getAttribute('data-id') ||
                                         parentContainer.getAttribute('data-comment-id');
                        }
                    }

                    comments.push({
                        id: commentId,
                        parent_id: parentId,
                        depth: isReply ? 1 : 0,
                        content: content,
                        like_count: likes,
                        author: author,
                        create_time: createTime,
                        is_reply: isReply
                    });
                }

                return comments;
            }
        ''')

        if not comments_data:
            print("  âš ï¸  æœªæ‰¾åˆ°è¯„è®ºæ•°æ®")
        else:
            print(f"  âœ… æå–åˆ° {len(comments_data)} æ¡è¯„è®º")

        return comments_data

    def build_comment_tree(self, comments):
        """æ„å»ºè¯„è®ºæ ‘"""
        # ç¬¬ä¸€å±‚ï¼šé¡¶çº§è¯„è®º
        top_level = [c for c in comments if c['depth'] == 0]

        # æ„å»ºæ ‘å½¢ç»“æ„
        tree = []
        for comment in top_level:
            node = {
                'comment': comment,
                'replies': self._build_replies(comment['id'], comments)
            }
            tree.append(node)

        return tree

    def _build_replies(self, parent_id, comments):
        """é€’å½’æ„å»ºå­è¯„è®º"""
        replies = [c for c in comments if c.get('parent_id') == parent_id]

        result = []
        for reply in replies:
            node = {
                'comment': reply,
                'replies': self._build_replies(reply['id'], comments)
            }
            result.append(node)

        return result

    def save_json(self, tree, comments_count):
        """ä¿å­˜ä¸º JSON"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = OUTPUT_DIR / f"xhs_comments_{self.note_id}_{timestamp}.json"

        result = {
            'note_id': self.note_id,
            'total_comments': comments_count,
            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'comments': tree
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ JSON å·²ä¿å­˜: {output_file}")
        return output_file

    def generate_summary(self, comments):
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        if not comments:
            return {}

        total = len(comments)
        top_level = sum(1 for c in comments if c['depth'] == 0)
        replies = total - top_level

        # ä½œè€…ç»Ÿè®¡
        authors = {}
        for comment in comments:
            nickname = comment['author']['nickname']
            if nickname not in authors:
                authors[nickname] = {
                    'count': 0,
                    'likes': 0,
                    'comments': []
                }
            authors[nickname]['count'] += 1
            authors[nickname]['likes'] += comment['like_count']
            authors[nickname]['comments'].append(comment['id'])

        # æ´»è·ƒä½œè€… Top 5
        sorted_authors = sorted(
            authors.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )[:5]

        # å›å¤å…³ç³»ç»Ÿè®¡
        reply_relations = []
        for comment in comments:
            if comment['depth'] == 1 and comment['parent_id']:
                # æ‰¾åˆ°çˆ¶è¯„è®º
                parent = next((c for c in comments if c['id'] == comment['parent_id']), None)
                if parent:
                    reply_relations.append({
                        'from': comment['author']['nickname'],
                        'to': parent['author']['nickname'],
                        'content': comment['content'][:50],
                        'time': comment['create_time']
                    })

        summary = {
            'total_comments': total,
            'top_level_comments': top_level,
            'reply_comments': replies,
            'unique_authors': len(authors),
            'top_authors': [
                {
                    'nickname': name,
                    'comment_count': data['count'],
                    'total_likes': data['likes']
                }
                for name, data in sorted_authors
            ],
            'sample_reply_relations': reply_relations[:10] if reply_relations else []
        }

        return summary


# ==================== ä¸»ç¨‹åº ====================

async def main_async(url: str = None):
    """å¼‚æ­¥ä¸»ç¨‹åº"""

    print(f"\n{'='*80}")
    print(f"å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (HTML ç‰ˆ)")
    print(f"{'='*80}")

    # è·å– Cookie
    print("\n[æ­¥éª¤ 1] åŠ è½½ Cookie")
    cookie_str = load_cookies()
    if not cookie_str:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆ Cookie")
        return

    print("âœ… Cookie å·²åŠ è½½")

    # æå–ç¬”è®° ID
    print("\n[æ­¥éª¤ 2] è§£æç¬”è®°é“¾æ¥")
    if not url:
        print("è¯·è¾“å…¥å°çº¢ä¹¦ç¬”è®°é“¾æ¥:")
        url = input("ç¬”è®°é“¾æ¥: ").strip()

    note_id = extract_note_id(url)
    if not note_id:
        print(f"âŒ æ— æ³•ä»é“¾æ¥æå–ç¬”è®° ID: {url}")
        return

    print(f"âœ… ç¬”è®° ID: {note_id}")

    # æ„å»ºé¡µé¢ URLï¼ˆä¿ç•™å®Œæ•´ URLï¼ŒåŒ…æ‹¬ xsec_tokenï¼‰
    # å¦‚æœåŸé“¾æ¥åŒ…å« xsec_tokenï¼Œåˆ™ä½¿ç”¨åŸé“¾æ¥
    if '?xsec_token=' in url:
        page_url = url
    else:
        page_url = f"https://www.xiaohongshu.com/explore/{note_id}"

    print(f"ğŸ“ é¡µé¢ URL: {page_url}")

    # çˆ¬å–è¯„è®º
    print("\n[æ­¥éª¤ 3] è®¿é—®é¡µé¢å¹¶æå–è¯„è®º")
    print("-" * 80)

    extractor = XHSCommentExtractor(note_id)

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(headless=False)

        # è§£æ Cookie
        cookies = parse_cookies(cookie_str)

        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            viewport={'width': 1920, 'height': 1080}
        )

        # æ·»åŠ  Cookie
        if cookies:
            await context.add_cookies(cookies)
            print(f"âœ… å·²è®¾ç½® {len(cookies)} ä¸ª Cookie")

        page = await context.new_page()

        # è®¿é—®é¡µé¢
        print(f"\nğŸ“¡ æ­£åœ¨è®¿é—®ç¬”è®°é¡µé¢...")
        print(f"   {page_url}")

        try:
            await page.goto(page_url, wait_until='networkidle', timeout=60000)
            print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")

            # æ»šåŠ¨åŠ è½½è¯„è®º
            print("\n  ğŸ”„ æ»šåŠ¨åŠ è½½è¯„è®º...")
            await asyncio.sleep(2)  # ç­‰å¾…åˆå§‹åŠ è½½

            # æ»šåŠ¨åˆ°åº•éƒ¨å¤šæ¬¡ï¼Œç¡®ä¿è¯„è®ºåŠ è½½
            for i in range(3):
                await page.evaluate('window.scrollBy(0, window.innerHeight)')
                await asyncio.sleep(1.5)

            print("  âœ… æ»šåŠ¨å®Œæˆ")

            # æå–è¯„è®º
            comments = await extractor.extract_comments_from_html(page)

            if not comments:
                print("\nâŒ æœªæå–åˆ°ä»»ä½•è¯„è®º")
                await browser.close()
                return

            # æ„å»ºæ ‘å½¢ç»“æ„
            print("\n  ğŸŒ³ æ„å»ºè¯„è®ºæ ‘...")
            tree = extractor.build_comment_tree(comments)
            print("  âœ… è¯„è®ºæ ‘æ„å»ºå®Œæˆ")

            # ä¿å­˜ JSON
            print("\n[æ­¥éª¤ 4] ä¿å­˜ç»“æœ")
            json_file = extractor.save_json(tree, len(comments))

            # ç”Ÿæˆæ‘˜è¦
            print("\n[æ­¥éª¤ 5] ç”Ÿæˆç»Ÿè®¡æ‘˜è¦")
            summary = extractor.generate_summary(comments)

            print(f"\n{'-'*80}")
            print(f"ç»Ÿè®¡æ‘˜è¦:")
            print(f"{'-'*80}")
            print(f"  æ€»è¯„è®ºæ•°: {summary['total_comments']}")
            print(f"  é¡¶çº§è¯„è®º: {summary['top_level_comments']}")
            print(f"  å›å¤è¯„è®º: {summary['reply_comments']}")
            print(f"  ç‹¬ä¸€ä½œè€…: {summary['unique_authors']}")
            print(f"\n  æ´»è·ƒä½œè€… Top 5:")
            for i, author in enumerate(summary['top_authors'], 1):
                print(f"    {i}. {author['nickname']} - {author['comment_count']} æ¡è¯„è®ºï¼Œ{author['total_likes']} åµŒ")

            if summary['sample_reply_relations']:
                print(f"\n  å›å¤å…³ç³»ç¤ºä¾‹:")
                for i, reply in enumerate(summary['sample_reply_relations'][:5], 1):
                    print(f"    {i}. {reply['from']} å›å¤ {reply['to']}")
                    print(f"       \"{reply['content']}\"")
                    print(f"       æ—¶é—´: {reply['create_time']}")

            # ä¿å­˜æ‘˜è¦
            summary_file = json_file.with_suffix('.summary.json')
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æ‘˜è¦å·²ä¿å­˜: {summary_file}")

            await browser.close()

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            await browser.close()
            return

    print(f"\n{'='*80}")
    print(f"âœ… å®Œæˆï¼")
    print(f"{'='*80}\n")


def main(url: str = None):
    """åŒæ­¥å…¥å£"""
    asyncio.run(main_async(url))


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦ç¬”è®°è¯„è®ºçˆ¬å–å·¥å…· (HTML ç‰ˆ)",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('url', nargs='?', help='å°çº¢ä¹¦ç¬”è®°é“¾æ¥')

    args = parser.parse_args()

    try:
        main(args.url)
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
