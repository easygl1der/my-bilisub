#!/usr/bin/env python3
"""
å°çº¢ä¹¦å›¾ç‰‡ä¸‹è½½å™¨ - åªä¸‹è½½ç¬”è®°çš„å®é™…å†…å®¹å›¾ç‰‡
éœ€è¦æä¾›å®Œæ•´çš„å°çº¢ä¹¦é“¾æ¥ï¼ˆå¸¦xsec_tokenï¼‰
ç”¨æ³•: python download_xhs_images.py "å°çº¢ä¹¦å®Œæ•´é“¾æ¥"
"""

import os
import sys
import re
import json
import time
import requests
from pathlib import Path
from urllib.parse import urlparse, parse_qs

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def extract_username_from_html(html):
    """ä» HTML ä¸­ç›´æ¥æœç´¢æå–ç”¨æˆ·åï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
    username = "å°çº¢ä¹¦ç”¨æˆ·"
    # æœç´¢å¸¸è§çš„ç”¨æˆ·åå­—æ®µ - ä¼˜å…ˆæŸ¥æ‰¾ user å¯¹è±¡å†…çš„ nickname
    user_patterns = [
        r'"user":\{[^}]*"nickname":"([^"]+)"',  # user å¯¹è±¡å†…çš„ nickname
        r'"nickname":"([^"]+)"',  # ä»»ä½• nickname
        r'"nickName":"([^"]+)"',
        r'"name":"([^"]+)"',
    ]
    for pattern in user_patterns:
        match = re.search(pattern, html)
        if match:
            try:
                potential_name = match.group(1).encode('raw_unicode_escape').decode('unicode_escape')
                # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯ç”¨æˆ·åçš„å€¼
                if potential_name and len(potential_name) > 1 and len(potential_name) < 30:
                    # æ’é™¤ä¸€äº›æ˜æ˜¾ä¸æ˜¯æ˜µç§°çš„å€¼
                    if potential_name not in ['åˆ†äº«', 'æ¨è', 'å…³æ³¨', 'ç²‰ä¸']:
                        username = potential_name
                        break
            except:
                try:
                    potential_name = match.group(1).encode('latin1').decode('utf-8')
                    if potential_name and len(potential_name) > 1 and len(potential_name) < 30:
                        if potential_name not in ['åˆ†äº«', 'æ¨è', 'å…³æ³¨', 'ç²‰ä¸']:
                            username = potential_name
                            break
                except:
                    pass
    return username


def extract_xhs_images(url):
    """
    ä»å°çº¢ä¹¦é“¾æ¥æå–ç¬”è®°çš„å®Œæ•´ä¿¡æ¯ï¼ˆç”¨æˆ·åã€æ ‡é¢˜ã€æ–‡æ¡ˆã€å›¾ç‰‡URLã€ç¬”è®°IDã€ç”¨æˆ·IDï¼‰

    Returns:
        (ç”¨æˆ·å, æ ‡é¢˜, æ–‡æ¡ˆ, å›¾ç‰‡URLåˆ—è¡¨, ç¬”è®°URL, ç”¨æˆ·ä¸»é¡µURL)
    """

    # é‡è¦çš„ï¼šå¿…é¡»ä½¿ç”¨å®Œæ•´çš„ URLï¼ˆåŒ…å« xsec_tokenï¼‰
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.xiaohongshu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
    }

    print(f"ğŸ“¡ è¯·æ±‚é¡µé¢...")
    print(f"   URL: {url[:80]}...")

    response = requests.get(url, headers=headers, timeout=30, allow_redirects=True)

    print(f"   çŠ¶æ€ç : {response.status_code}")
    print(f"   æœ€ç»ˆURL: {response.url[:80]}...")

    if response.status_code != 200:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
        return None, None, [], '', ''

    # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°404
    if '/404?' in response.url or 'ä½ è®¿é—®çš„é¡µé¢ä¸è§äº†' in response.text:
        print(f"âŒ é¡µé¢æ— æ³•è®¿é—®ï¼ˆåçˆ¬è™«ä¿æŠ¤ï¼‰")
        print(f"   åŸå› ï¼š")
        print(f"   1. é“¾æ¥ç¼ºå°‘ xsec_token å‚æ•°")
        print(f"   2. é“¾æ¥å·²è¿‡æœŸæˆ–å¤±æ•ˆ")
        print(f"   3. éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹")
        return None, None, [], '', ''

    html = response.text

    # æå–ç¬”è®° ID å’Œç”¨æˆ· ID
    note_url = response.url  # ä½¿ç”¨æœ€ç»ˆé‡å®šå‘åçš„ URL
    user_homepage = ''

    # ä» URL æˆ– HTML ä¸­æå–ä¿¡æ¯
    try:
        # å°è¯•ä» URL ä¸­æå–ç¬”è®° ID
        # æ ¼å¼: https://www.xiaohongshu.com/explore/ç¬”è®°ID?...
        url_match = re.search(r'/explore/([^/?]+)', note_url)
        if url_match:
            note_id = url_match.group(1)
        else:
            note_id = ''

        # å°è¯•ä» JSON ä¸­æå–ç”¨æˆ· ID æ¥æ„å»ºä¸»é¡µé“¾æ¥
        start_idx = html.find('window.__INITIAL_STATE__=')
        if start_idx != -1:
            start_idx += len('window.__INITIAL_STATE__=')
            end_idx = html.find('</script>', start_idx)
            json_str = html[start_idx:end_idx]

            try:
                data = json.loads(json_str)

                # å°è¯•å¤šä¸ªè·¯å¾„è·å–ç”¨æˆ· ID
                user_id = None
                user = data.get('user', {}).get('user', {})
                if not user or not user.get('user_id'):
                    user = data.get('user', {}).get('userPageInfo', {}).get('user', {})
                if not user or not user.get('user_id'):
                    note = data.get('note', {})
                    note_detail = note.get('noteDetail', {})
                    user = note_detail.get('user', {})

                if user:
                    user_id = (user.get('user_id') or
                              user.get('userId') or
                              user.get('webId'))

                if user_id:
                    user_homepage = f"https://www.xiaohongshu.com/user/profile/{user_id}"
            except:
                pass
    except:
        pass

    print(f"âœ… é¡µé¢è·å–æˆåŠŸ (é•¿åº¦: {len(html)})")

    # æå–æ ‡é¢˜ - å¤„ç† Unicode è½¬ä¹‰
    title = "å°çº¢ä¹¦ç¬”è®°"
    title_match = re.search(r'<title[^>]*>(.+?)</title>', html)
    if title_match:
        title = title_match.group(1).replace(' - å°çº¢ä¹¦', '').strip()
        # å¤„ç† Unicode è½¬ä¹‰ (å¦‚ \u0040)
        try:
            title = title.encode('raw_unicode_escape').decode('unicode_escape')
        except:
            try:
                title = title.encode('latin1').decode('utf-8')
            except:
                pass
    print(f"ğŸ“ æ ‡é¢˜: {title[:50]}...")

    # æå–æ–‡æ¡ˆ/æè¿° - åŒæ ·å¤„ç†è½¬ä¹‰
    desc = ""
    try:
        # æ–¹æ³•1: ä» desc å­—æ®µæå–
        desc_patterns = [
            r'"desc":"([^"]+)"',
            r'"desc":\s*"([^"]+)"',
        ]
        for pattern in desc_patterns:
            desc_match = re.search(pattern, html)
            if desc_match:
                try:
                    desc = desc_match.group(1).encode('raw_unicode_escape').decode('unicode_escape')
                except:
                    try:
                        desc = desc_match.group(1).encode('latin1').decode('utf-8')
                    except:
                        desc = desc_match.group(1)
                if desc:
                    break
    except:
        pass

    print(f"ğŸ“„ æ–‡æ¡ˆ: {desc[:100] if desc else '(æ— )'}...")

    print(f"\nğŸ” æ­£åœ¨æå–ç¬”è®°å›¾ç‰‡...")

    # æŸ¥æ‰¾ __INITIAL_STATE__ JSON æ•°æ®
    start_idx = html.find('window.__INITIAL_STATE__=')
    if start_idx == -1:
        print(f"âŒ æœªæ‰¾åˆ° __INITIAL_STATE__")
        # ä» HTML ç›´æ¥æå–ç”¨æˆ·åï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        username = extract_username_from_html(html)
        print(f"ğŸ‘¤ ç”¨æˆ·å: {username}")
        return username, title, desc, []

    start_idx += len('window.__INITIAL_STATE__=')
    end_idx = html.find('</script>', start_idx)
    json_str = html[start_idx:end_idx]

    # å°è¯•è§£æ JSON
    data = None
    try:
        data = json.loads(json_str)
        print(f"âœ… JSONè§£ææˆåŠŸ")
    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™æœç´¢...")

    # æå–ç”¨æˆ·å - å‚è€ƒ fetch_xhs_videos_complete.py çš„ get_user_nickname æ–¹æ³•
    username = "å°çº¢ä¹¦ç”¨æˆ·"
    if data:
        # å°è¯•å¤šä¸ªè·¯å¾„è·å–ç”¨æˆ·å
        user = data.get('user', {}).get('user', {})
        if not user or not user.get('nickname'):
            user = data.get('user', {}).get('userPageInfo', {}).get('user', {})
        if not user or not user.get('nickname'):
            # ä» note.noteDetail æå–
            note = data.get('note', {})
            note_detail = note.get('noteDetail', {})
            user = note_detail.get('user', {})

        # è·å– nickname
        if user and user.get('nickname'):
            username = user.get('nickname')
        elif user:
            # å°è¯•å…¶ä»–å­—æ®µ
            username = (user.get('name') or
                       user.get('nickName') or
                       user.get('username') or "å°çº¢ä¹¦ç”¨æˆ·")

    # å¦‚æœ JSON æ²¡æ‰¾åˆ°ï¼Œä» HTML ç›´æ¥æœç´¢
    if username == "å°çº¢ä¹¦ç”¨æˆ·":
        username = extract_username_from_html(html)

    print(f"ğŸ‘¤ ç”¨æˆ·å: {username}")

    image_urls = []

    # æ–¹æ³•1: ä»è§£æå¥½çš„ JSON ä¸­æå–
    if data:
        try:
            # è·¯å¾„: note.noteDetail.imageList
            note = data.get('note', {})
            note_detail = note.get('noteDetail', {})
            image_list = note_detail.get('imageList', [])

            if image_list:
                print(f"âœ… ä» note.noteDetail.imageList æ‰¾åˆ° {len(image_list)} å¼ å›¾ç‰‡")
                for img_obj in image_list:
                    if isinstance(img_obj, dict):
                        # å°è¯•å¤šä¸ªå­—æ®µ
                        url = (img_obj.get('urlDefault') or
                               img_obj.get('url_default') or
                               img_obj.get('url') or
                               img_obj.get('infoList', [{}])[0].get('url') if isinstance(img_obj.get('infoList'), list) else None)
                        if url:
                            image_urls.append(url)
        except Exception as e:
            print(f"âš ï¸  æ–¹æ³•1å¤±è´¥: {e}")

    # æ–¹æ³•2: ç›´æ¥åœ¨ JSON å­—ç¬¦ä¸²ä¸­æœç´¢ imageList
    if not image_urls:
        print(f"ğŸ” å°è¯•ç›´æ¥æœç´¢ imageList...")

        # æ‰¾åˆ° imageList: [...] éƒ¨åˆ† - ä½¿ç”¨è®¡æ•°å™¨åŒ¹é…å®Œæ•´çš„æ•°ç»„
        start = json_str.find('"imageList"')
        if start >= 0:
            # æ‰¾åˆ° [
            bracket_start = json_str.find('[', start)
            if bracket_start >= 0:
                # æ‰‹åŠ¨åŒ¹é…å¯¹åº”çš„ ]
                depth = 0
                i = bracket_start
                while i < len(json_str):
                    if json_str[i] == '[':
                        depth += 1
                    elif json_str[i] == ']':
                        depth -= 1
                        if depth == 0:
                            bracket_end = i
                            break
                    i += 1

                list_content = json_str[bracket_start+1:bracket_end]
                print(f"âœ… æ‰¾åˆ° imageListï¼Œå†…å®¹é•¿åº¦: {len(list_content)}")

                # åªæå– urlDefaultï¼ˆé»˜è®¤/åŸå›¾ï¼‰ï¼Œè·³è¿‡ urlPre å’Œ infoList
                # æ¯ä¸ª image å¯¹è±¡åªå–ä¸€ä¸ª urlDefault
                url_pattern = r'"urlDefault":"([^"]+)"'
                for match in re.finditer(url_pattern, list_content):
                    url = match.group(1)
                    if url:
                        image_urls.append(url)

                if image_urls:
                    print(f"âœ… æå–åˆ° {len(image_urls)} å¼ å›¾ç‰‡")

    # æ–¹æ³•3: ä»æ•´ä¸ª HTML ä¸­æœç´¢ sns-webpic å›¾ç‰‡URLï¼ˆå¤‡ç”¨ï¼‰
    if not image_urls:
        print(f"ğŸ” å°è¯•ä»HTMLç›´æ¥æå–...")
        # æŸ¥æ‰¾æ‰€æœ‰ sns-webpic é“¾æ¥
        all_urls = re.findall(r'(https://sns-webpic[^\"\s\'<>]+)', html)
        # å»é‡
        unique_urls = list(set(all_urls))
        if unique_urls:
            print(f"âœ… æ‰¾åˆ° {len(unique_urls)} ä¸ª sns-webpic URL")
            image_urls = unique_urls[:10]  # é™åˆ¶æ•°é‡

    # å»é‡å¹¶æ¸…ç†
    seen = set()
    unique_urls = []
    for url in image_urls:
        # æ¸…ç† URL
        url = url.split('?')[0]
        # è§£ç  Unicode è½¬ä¹‰ (å¦‚ \u002F -> /)
        try:
            url = url.encode('utf-8').decode('unicode_escape')
        except:
            pass
        # å†æ¬¡æ¸…ç†å¯èƒ½å¼•å…¥çš„é—®é¢˜
        url = url.replace(r'\/', '/')
        # ç¡®ä¿ https åè®®
        if url.startswith('http://'):
            url = 'https://' + url[7:]
        elif not url.startswith('https://'):
            continue
        if url not in seen and 'xhscdn' in url:
            seen.add(url)
            unique_urls.append(url)

    # æ˜¾ç¤ºæ‰¾åˆ°çš„URLç”¨äºè°ƒè¯•
    print(f"ğŸ“‹ å›¾ç‰‡URLåˆ—è¡¨:")
    for i, u in enumerate(unique_urls, 1):
        print(f"   {i}. {u[:80]}...")

    print(f"ğŸ“ ç¬”è®°é“¾æ¥: {note_url[:80]}...")
    if user_homepage:
        print(f"ğŸ‘¤ ç”¨æˆ·ä¸»é¡µ: {user_homepage}")

    return username, title, desc, unique_urls, note_url, user_homepage


def download_images(url, output_dir="xhs_images"):
    """
    ä¸‹è½½æ‰€æœ‰å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•ï¼ŒåŒæ—¶ä¿å­˜æ–‡æ¡ˆå’Œé“¾æ¥ä¿¡æ¯

    æ–‡ä»¶ç»“æ„:
    xhs_images/
    â””â”€â”€ ç”¨æˆ·å/
        â””â”€â”€ ç¬”è®°æ ‡é¢˜/
            â”œâ”€â”€ content.txt  # æ–‡æ¡ˆï¼ˆåŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€ç”¨æˆ·ä¸»é¡µï¼‰
            â”œâ”€â”€ image_01.jpg
            â”œâ”€â”€ image_02.jpg
            â””â”€â”€ ...
    """

    result = extract_xhs_images(url)

    if not result or len(result) < 4:
        print(f"\nâŒ æå–å¤±è´¥")
        return False

    username, title, desc, image_urls, note_url, user_homepage = result

    if not image_urls:
        print(f"\nâŒ æœªæ‰¾åˆ°å›¾ç‰‡")
        return False

    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„: xhs_images/ç”¨æˆ·å/ç¬”è®°æ ‡é¢˜/
    safe_user = re.sub(r'[<>:"/\\|?*]', '_', username)[:30]
    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]

    note_path = Path(output_dir) / safe_user / safe_title
    note_path.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜æ–‡æ¡ˆåˆ° content.txtï¼ˆåŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€ç”¨æˆ·ä¸»é¡µï¼‰
    content_file = note_path / "content.txt"
    with open(content_file, 'w', encoding='utf-8') as f:
        f.write(f"æ ‡é¢˜: {title}\n")
        if note_url:
            f.write(f"é“¾æ¥: {note_url}\n")
        if user_homepage:
            f.write(f"ç”¨æˆ·ä¸»é¡µ: {user_homepage}\n")
        f.write(f"\næ–‡æ¡ˆ:\n{desc}\n")
    print(f"ğŸ“„ æ–‡æ¡ˆå·²ä¿å­˜: content.txt")

    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡...")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {note_path}")
    print(f"{'='*60}")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.xiaohongshu.com/',
    }

    success_count = 0
    failed_count = 0

    for i, img_url in enumerate(image_urls, 1):
        try:
            print(f"[{i}/{len(image_urls)}] ", end='', flush=True)

            img_response = requests.get(img_url, headers=headers, timeout=30)

            if img_response.status_code == 200:
                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                content_type = img_response.headers.get('Content-Type', '')
                if 'png' in content_type or img_url.endswith('.png'):
                    ext = '.png'
                elif 'webp' in content_type or img_url.endswith('.webp'):
                    ext = '.webp'
                else:
                    ext = '.jpg'

                filename = f"image_{i:02d}{ext}"
                filepath = note_path / filename

                with open(filepath, 'wb') as f:
                    f.write(img_response.content)

                size = len(img_response.content) / 1024
                print(f"âœ… {size:.1f}KB")
                success_count += 1
            else:
                print(f"âŒ HTTP {img_response.status_code}")
                failed_count += 1

        except Exception as e:
            print(f"âŒ {str(e)[:30]}")
            failed_count += 1

        time.sleep(0.3)

    print(f"{'='*60}")
    print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ!")
    print(f"   å›¾ç‰‡: {success_count} æˆåŠŸ | {failed_count} å¤±è´¥")
    print(f"   æ–‡æ¡ˆ: {'å·²ä¿å­˜' if desc else '(æ— )'}")
    print(f"   ä½ç½®: {note_path.absolute()}")

    # è¿”å›ä¸‹è½½ç›®å½•è·¯å¾„
    return note_path if success_count > 0 else None


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python download_xhs_images.py \"å°çº¢ä¹¦å®Œæ•´é“¾æ¥\"")
        print("\næ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨å®Œæ•´çš„é“¾æ¥ï¼ˆåŒ…å« xsec_token å‚æ•°ï¼‰")
        print("ä»å°çº¢ä¹¦åˆ†äº«æˆ–å¤åˆ¶é“¾æ¥è·å–å®Œæ•´URL")
        sys.exit(1)

    url = sys.argv[1]
    download_images(url)
