#!/usr/bin/env python3
"""
å°çº¢ä¹¦ç”¨æˆ·è§†é¢‘ç¬”è®°è·å–å·¥å…·
è·å–æŒ‡å®šå°çº¢ä¹¦ç”¨æˆ·çš„æ‰€æœ‰è§†é¢‘ç¬”è®°

ä½¿ç”¨æ–¹æ³•:
    python fetch_xhs_videos.py

åŠŸèƒ½:
    1. è·å–æŒ‡å®šå°çº¢ä¹¦ç”¨æˆ·çš„æ‰€æœ‰è§†é¢‘ç¬”è®°
    2. ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆç”¨äºåç»­å·¥ä½œæµï¼‰
    3. æ”¯æŒå¢é‡æ›´æ–°ï¼ˆè®°å½•å·²å¤„ç†çš„ç¬”è®°ï¼‰

ä¾èµ–:
    - MediaCrawler/media_platform/xhs/ (å°çº¢ä¹¦çˆ¬è™«API)
    - config/cookies.txt (å°çº¢ä¹¦Cookie)
"""

import os
import sys
import csv
import json
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ==================== é…ç½® ====================

OUTPUT_DIR = PROJECT_ROOT / "output" / "xhs_videos"
COOKIE_FILE = PROJECT_ROOT / "config" / "cookies.txt"

# CSVåˆ—å®šä¹‰
CSV_COLUMNS = [
    'åºå·',
    'æ ‡é¢˜',
    'é“¾æ¥',
    'ç¬”è®°ID',
    'ç±»å‹',
    'å‘å¸ƒæ—¶é—´',
    'ç‚¹èµæ•°',
    'æ”¶è—æ•°',
    'è¯„è®ºæ•°',
    'å­—å¹•çŠ¶æ€',
    'åˆ†æçŠ¶æ€',
]

# ==================== å·¥å…·å‡½æ•° ====================

def extract_user_info_from_url(url: str) -> Optional[Dict]:
    """
    ä»å°çº¢ä¹¦ç”¨æˆ·é“¾æ¥ä¸­æå–ç”¨æˆ·ä¿¡æ¯

    æ”¯æŒçš„URLæ ¼å¼:
    - https://www.xiaohongshu.com/user/profile/5f3e2c1d2e3a4b5c
    - https://www.xiaohongshu.com/user/profile/5f3e2c1d2e3a4b5c?xhsshare=...
    """
    print("\n" + "="*70)
    print("æ­¥éª¤1: è§£æç”¨æˆ·é“¾æ¥")
    print("="*70)

    try:
        # ç§»é™¤æŸ¥è¯¢å‚æ•°
        if '?' in url:
            url = url.split('?')[0]

        # æå–ç”¨æˆ·ID
        if 'user/profile/' in url:
            user_id = url.split('user/profile/')[-1].strip('/')
            print(f"âœ… æå–åˆ°ç”¨æˆ·ID: {user_id}")
            return {
                'user_id': user_id,
                'url': url,
                'profile_url': f"https://www.xiaohongshu.com/user/profile/{user_id}"
            }
        else:
            print("âŒ æ— æ³•è¯†åˆ«çš„å°çº¢ä¹¦ç”¨æˆ·é“¾æ¥æ ¼å¼")
            return None
    except Exception as e:
        print(f"âŒ è§£æé“¾æ¥å¤±è´¥: {e}")
        return None


def load_cookie() -> Optional[str]:
    """åŠ è½½å°çº¢ä¹¦Cookie"""
    if not COOKIE_FILE.exists():
        print(f"âš ï¸  Cookieæ–‡ä»¶ä¸å­˜åœ¨: {COOKIE_FILE}")
        print(f"   è¯·åˆ›å»ºè¯¥æ–‡ä»¶å¹¶å¡«å…¥å°çº¢ä¹¦Cookie")
        return None

    try:
        with open(COOKIE_FILE, 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
        if cookie:
            print(f"âœ… Cookieå·²åŠ è½½")
            return cookie
        else:
            print(f"âš ï¸  Cookieæ–‡ä»¶ä¸ºç©º")
            return None
    except Exception as e:
        print(f"âŒ è¯»å–Cookieå¤±è´¥: {e}")
        return None


# ==================== MediaCrawleré›†æˆ ====================

async def fetch_user_notes_with_mediacrawler(user_id: str, cookie: str) -> List[Dict]:
    """
    ä½¿ç”¨MediaCrawler APIè·å–ç”¨æˆ·ç¬”è®°

    Args:
        user_id: å°çº¢ä¹¦ç”¨æˆ·ID
        cookie: å°çº¢ä¹¦Cookie

    Returns:
        ç¬”è®°åˆ—è¡¨
    """
    try:
        # å¯¼å…¥MediaCrawleræ¨¡å—
        from MediaCrawler.media_platform.xhs.client import XiaoHongShuClient
        from MediaCrawler.media_platform.xhs.help import parse_creator_info_from_url
        import aiohttp

        print("\n" + "="*70)
        print("æ­¥éª¤2: è·å–ç”¨æˆ·ç¬”è®°åˆ—è¡¨")
        print("="*70)

        # åˆ›å»ºHTTPå®¢æˆ·ç«¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Cookie': cookie,
        }

        # TODO: è¿™é‡Œéœ€è¦å®ç°MediaCrawlerçš„APIè°ƒç”¨
        # ç”±äºMediaCrawlerçš„å¤æ‚æ€§ï¼Œè¿™é‡Œæä¾›ç®€åŒ–ç‰ˆæœ¬
        # å®é™…å®ç°éœ€è¦è°ƒç”¨ XiaoHongShuClient çš„æ–¹æ³•

        print("âš ï¸  å®Œæ•´çš„MediaCrawleré›†æˆéœ€è¦é¢å¤–é…ç½®")
        print("   å½“å‰æä¾›ç®€åŒ–ç‰ˆæœ¬ç”¨äºæµ‹è¯•")

        # è¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        return []

    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥MediaCrawleræ¨¡å—: {e}")
        print(f"   è¯·ç¡®ä¿ MediaCrawler å­æ¨¡å—å·²æ­£ç¡®åˆå§‹åŒ–")
        return []
    except Exception as e:
        print(f"âŒ è·å–ç¬”è®°å¤±è´¥: {e}")
        return []


# ==================== ç®€åŒ–ç‰ˆæœ¬ï¼ˆä½¿ç”¨yt-dlpï¼‰ ====================

def fetch_user_notes_simple(user_id: str, max_count: int = 30) -> List[Dict]:
    """
    ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨yt-dlpæˆ–å…¶ä»–æ–¹å¼è·å–ç”¨æˆ·è§†é¢‘ç¬”è®°

    è¿™æ˜¯ä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆï¼Œå½“MediaCrawlerä¸å¯ç”¨æ—¶ä½¿ç”¨

    Args:
        user_id: å°çº¢ä¹¦ç”¨æˆ·ID
        max_count: æœ€å¤§è·å–æ•°é‡

    Returns:
        ç¬”è®°åˆ—è¡¨
    """
    print("\n" + "="*70)
    print("æ­¥éª¤2: è·å–ç”¨æˆ·ç¬”è®°åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰")
    print("="*70)
    print(f"âš ï¸  å½“å‰ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    print(f"   éœ€è¦æ‰‹åŠ¨æä¾›è§†é¢‘é“¾æ¥åˆ—è¡¨")

    # è¿”å›ç©ºåˆ—è¡¨ï¼Œæç¤ºç”¨æˆ·æä¾›CSV
    return []


# ==================== æ•°æ®ä¿å­˜ ====================

def filter_video_notes(notes: List[Dict]) -> List[Dict]:
    """è¿‡æ»¤å‡ºè§†é¢‘ç¬”è®°ï¼ˆæ’é™¤å›¾æ–‡ï¼‰"""
    video_notes = []
    for note in notes:
        # æ ¹æ®ç¬”è®°ç±»å‹è¿‡æ»¤
        # å°çº¢ä¹¦ç¬”è®°ç±»å‹ï¼švideo=è§†é¢‘ï¼Œnormal=å›¾æ–‡
        note_type = note.get('type', '')
        if note_type == 'video' or note.get('video_url'):
            video_notes.append(note)

    print(f"âœ… ç­›é€‰å‡ºè§†é¢‘ç¬”è®°: {len(video_notes)}/{len(notes)}")
    return video_notes


def save_to_csv(notes: List[Dict], user_id: str, output_dir: Path) -> Optional[Path]:
    """ä¿å­˜ç¬”è®°åˆ—è¡¨åˆ°CSVæ–‡ä»¶"""
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = output_dir / f"xhs_videos_{user_id}_{timestamp}.csv"

    try:
        with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS)
            writer.writeheader()

            for i, note in enumerate(notes, 1):
                row = {
                    'åºå·': i,
                    'æ ‡é¢˜': note.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                    'é“¾æ¥': note.get('url', ''),
                    'ç¬”è®°ID': note.get('note_id', ''),
                    'ç±»å‹': note.get('type', 'video'),
                    'å‘å¸ƒæ—¶é—´': note.get('publish_time', ''),
                    'ç‚¹èµæ•°': note.get('like_count', 0),
                    'æ”¶è—æ•°': note.get('collect_count', 0),
                    'è¯„è®ºæ•°': note.get('comment_count', 0),
                    'å­—å¹•çŠ¶æ€': 'pending',
                    'åˆ†æçŠ¶æ€': 'pending',
                }
                writer.writerow(row)

        print(f"\nâœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        print(f"   å…±ä¿å­˜ {len(notes)} ä¸ªè§†é¢‘ç¬”è®°")
        return csv_file

    except Exception as e:
        print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        return None


def save_to_markdown(notes: List[Dict], user_id: str, output_dir: Path) -> Optional[Path]:
    """ä¿å­˜ç¬”è®°åˆ—è¡¨åˆ°Markdownæ–‡ä»¶"""
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    md_file = output_dir / f"xhs_videos_{user_id}_{timestamp}.md"

    try:
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# å°çº¢ä¹¦ç”¨æˆ·è§†é¢‘ç¬”è®°æ±‡æ€»\n\n")
            f.write(f"**ç”¨æˆ·ID**: {user_id}\n\n")
            f.write(f"**è·å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**è§†é¢‘æ•°é‡**: {len(notes)}\n\n")
            f.write("---\n\n")
            f.write(f"## è§†é¢‘åˆ—è¡¨\n\n")
            f.write(f"| åºå· | æ ‡é¢˜ | é“¾æ¥ | ç¬”è®°ID | å‘å¸ƒæ—¶é—´ | ç‚¹èµ | æ”¶è— | è¯„è®º |\n")
            f.write(f"|------|------|------|--------|----------|------|------|------|\n")

            for i, note in enumerate(notes, 1):
                title = note.get('title', 'æœªçŸ¥æ ‡é¢˜')[:50]
                url = note.get('url', '')
                note_id = note.get('note_id', '')
                publish_time = note.get('publish_time', '')
                like_count = note.get('like_count', 0)
                collect_count = note.get('collect_count', 0)
                comment_count = note.get('comment_count', 0)

                f.write(f"| {i} | {title} | [é“¾æ¥]({url}) | {note_id} | {publish_time} | {like_count} | {collect_count} | {comment_count} |\n")

        print(f"âœ… Markdownæ–‡ä»¶å·²ä¿å­˜: {md_file}")
        return md_file

    except Exception as e:
        print(f"âŒ ä¿å­˜Markdownå¤±è´¥: {e}")
        return None


# ==================== ä¸»ç¨‹åº ====================

async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("å°çº¢ä¹¦ç”¨æˆ·è§†é¢‘ç¬”è®°è·å–å·¥å…·")
    print("="*70)

    # æ­¥éª¤1: è§£æç”¨æˆ·é“¾æ¥
    user_info = extract_user_info_from_url(args.url)
    if not user_info:
        return False

    user_id = user_info['user_id']

    # æ­¥éª¤2: åŠ è½½Cookie
    cookie = load_cookie() if not args.no_cookie else None
    if not cookie and not args.no_cookie:
        print("\nğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ --no-cookie è·³è¿‡Cookieæ£€æŸ¥ï¼ˆå¯èƒ½æ— æ³•è·å–å®Œæ•´æ•°æ®ï¼‰")

    # æ­¥éª¤3: è·å–ç”¨æˆ·ç¬”è®°
    if args.use_mediacrawler:
        # ä½¿ç”¨MediaCrawler API
        notes = await fetch_user_notes_with_mediacrawler(user_id, cookie)
    else:
        # ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        notes = fetch_user_notes_simple(user_id, args.count)

    if not notes:
        print("\nâŒ æœªè·å–åˆ°ç¬”è®°")
        print("\nğŸ’¡ å»ºè®®:")
        print("   1. æ‰‹åŠ¨æä¾›è§†é¢‘é“¾æ¥åˆ—è¡¨CSVæ–‡ä»¶")
        print("   2. æˆ–è€…é…ç½®MediaCrawlerä»¥ä½¿ç”¨å®Œæ•´API")
        return False

    # æ­¥éª¤4: è¿‡æ»¤è§†é¢‘ç¬”è®°
    video_notes = filter_video_notes(notes)

    if not video_notes:
        print("\nâŒ æœªæ‰¾åˆ°è§†é¢‘ç¬”è®°")
        return False

    # é™åˆ¶æ•°é‡
    if args.count and len(video_notes) > args.count:
        video_notes = video_notes[:args.count]
        print(f"\nğŸ“Š é™åˆ¶å¤„ç†æ•°é‡: {args.count}")

    # æ­¥éª¤5: ä¿å­˜ç»“æœ
    print("\n" + "="*70)
    print("æ­¥éª¤3: ä¿å­˜ç»“æœ")
    print("="*70)

    csv_file = save_to_csv(video_notes, user_id, OUTPUT_DIR)
    md_file = save_to_markdown(video_notes, user_id, OUTPUT_DIR)

    # å®Œæˆ
    print("\n" + "="*70)
    print("âœ… è·å–å®Œæˆ!")
    print("="*70)
    print(f"ç”¨æˆ·ID: {user_id}")
    print(f"è§†é¢‘ç¬”è®°æ•°: {len(video_notes)}")
    if csv_file:
        print(f"CSVæ–‡ä»¶: {csv_file}")
    if md_file:
        print(f"Markdownæ–‡ä»¶: {md_file}")

    return True


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦ç”¨æˆ·è§†é¢‘ç¬”è®°è·å–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # åŸºæœ¬ä½¿ç”¨
    python fetch_xhs_videos.py --url "https://www.xiaohongshu.com/user/profile/5f3e2c1d2e3a4b5c"

    # é™åˆ¶è·å–æ•°é‡
    python fetch_xhs_videos.py --url "ç”¨æˆ·ä¸»é¡µé“¾æ¥" --count 20

    # ä½¿ç”¨MediaCrawler APIï¼ˆéœ€è¦é…ç½®ï¼‰
    python fetch_xhs_videos.py --url "ç”¨æˆ·ä¸»é¡µé“¾æ¥" --use-mediacrawler

    # è·³è¿‡Cookieæ£€æŸ¥
    python fetch_xhs_videos.py --url "ç”¨æˆ·ä¸»é¡µé“¾æ¥" --no-cookie
        """
    )

    parser.add_argument('-u', '--url', required=True,
                       help='å°çº¢ä¹¦ç”¨æˆ·ä¸»é¡µé“¾æ¥')
    parser.add_argument('-c', '--count', type=int, default=None,
                       help='æœ€å¤§è·å–æ•°é‡ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰')
    parser.add_argument('--use-mediacrawler', action='store_true',
                       help='ä½¿ç”¨MediaCrawler APIï¼ˆéœ€è¦å®Œæ•´é…ç½®ï¼‰')
    parser.add_argument('--no-cookie', action='store_true',
                       help='è·³è¿‡Cookieæ£€æŸ¥')

    args = parser.parse_args()

    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    try:
        success = asyncio.run(main_async(args))
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
