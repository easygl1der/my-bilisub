#!/usr/bin/env python3
"""
å°çº¢ä¹¦å›¾æ–‡ç¬”è®°åˆ†æå·¥å…· - ç®€åŒ–ç‰ˆ

åŠŸèƒ½ï¼š
1. ä¸Šä¼ å›¾ç‰‡åˆ° Gemini
2. ä½¿ç”¨ä¸€ä¸ªé€šç”¨æç¤ºè¯ï¼Œè®© AI è‡ªåŠ¨è¯†åˆ«é£æ ¼å¹¶åˆ†æ
3. è¾“å‡ºç»“æ„åŒ–çš„ Markdown æŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹:
    python xhs_simple_analysis.py --dir "xhs_images/ç”¨æˆ·å/ç¬”è®°æ ‡é¢˜"
    python xhs_simple_analysis.py --user-dir "xhs_images/ç”¨æˆ·å"
"""

import os
import sys
import time
from pathlib import Path
from typing import List

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

try:
    import google.generativeai as genai
except ImportError:
    print("âŒ æœªå®‰è£… google-generativeai åº“")
    print("è¯·è¿è¡Œ: pip install google-generativeai")
    sys.exit(1)


# ==================== é€šç”¨æç¤ºè¯ ====================

UNIVERSAL_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å›¾æ–‡ç¬”è®°åˆ†æå¸ˆã€‚è¯·åˆ†æè¿™ç»„å›¾æ–‡ç¬”è®°ï¼Œè¾“å‡ºç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚

## ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«ç¬”è®°ç±»å‹

è¯·é¦–å…ˆåˆ¤æ–­è¿™ç»„ç¬”è®°å±äºå“ªç§ç±»å‹ï¼š
- **ç”Ÿæ´»è®°å½•**: æ—¥å¸¸ç”Ÿæ´»ã€å¿ƒæƒ…éšæƒ³ã€vlogã€å¤–å‡ºè®°å½•
- **é‡‘å¥é“ç†**: ä»¥æ–‡å­—ä¸ºä¸»ï¼Œåˆ†äº«äººç”Ÿæ„Ÿæ‚Ÿã€é“ç†ã€è¯­å½•ã€æ–‡æ¡ˆ
- **æ–°é—»ç§‘æ™®**: æ–°é—»äº‹ä»¶ã€ç§‘æ™®çŸ¥è¯†ã€è¡Œä¸šåŠ¨æ€
- **ç©¿æ­ç¾å¦†**: æœè£…æ­é…ã€ç¾å¦†æ•™ç¨‹ã€OOTDã€é¢œå€¼åˆ†äº«
- **ç¾é£Ÿæ¢åº—**: é¤å…æ¢åº—ã€ç¾é£Ÿåˆ¶ä½œã€é£Ÿè°±åˆ†äº«
- **æ—…è¡Œæ”»ç•¥**: æ—…è¡Œæ”»ç•¥ã€æ™¯ç‚¹æ¨èã€è¡Œç¨‹åˆ†äº«
- **æ•°ç æµ‹è¯„**: æ•°ç äº§å“æµ‹è¯„ã€å¼€ç®±ã€ä½¿ç”¨ä½“éªŒ
- **å­¦ä¹ ç¬”è®°**: å­¦ä¹ ç¬”è®°ã€æ•™ç¨‹ã€å¹²è´§åˆ†äº«
- **å¥èº«è¿åŠ¨**: å¥èº«æ•™ç¨‹ã€è¿åŠ¨æ‰“å¡ã€å‡è‚¥å¡‘å½¢
- **æƒ…æ„Ÿå…³ç³»**: æ‹çˆ±æ„Ÿæ‚Ÿã€æƒ…æ„Ÿåˆ†æã€å…³ç³»å»ºè®®

## ç¬¬äºŒæ­¥ï¼šæ ¹æ®ç±»å‹è¾“å‡ºå¯¹åº”åˆ†æ

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼ˆä¿æŒæ‰€æœ‰æ ‡é¢˜å’Œç¬¦å·ï¼‰ï¼š

## ğŸ“‹ ç¬”è®°åŸºæœ¬ä¿¡æ¯
- **ç¬”è®°ç±»å‹**: [è¯†åˆ«å‡ºçš„ç±»å‹]
- **æ ¸å¿ƒä¸»é¢˜**: [ä¸€å¥è¯æ¦‚æ‹¬ä¸»é¢˜]
- **ä½œè€…**: [å¦‚æœæœ‰ä½œè€…ä¿¡æ¯]

## ğŸ“– å†…å®¹æ¦‚è¦ï¼ˆ150-250å­—ï¼‰
[ç»“åˆå›¾ç‰‡å’Œæ–‡å­—ï¼Œç”¨ç²¾ç‚¼çš„è¯­è¨€æ¦‚æ‹¬ç¬”è®°æ ¸å¿ƒå†…å®¹]

## ğŸ¯ æ ¸å¿ƒä¿¡æ¯æå–
æ ¹æ®ç¬”è®°ç±»å‹ï¼Œæå–å¯¹åº”çš„å…³é”®ä¿¡æ¯ï¼š

### å¦‚æœæ˜¯ç”Ÿæ´»è®°å½•ï¼š
- **åœºæ™¯**: [è®°å½•çš„åœºæ™¯/ç¯å¢ƒ]
- **æƒ…ç»ª**: [ä½œè€…çš„æƒ…ç»ªçŠ¶æ€]
- **ç”Ÿæ´»ç»†èŠ‚**: [å…³é”®å…ƒç´ ã€äº‹ä»¶]

### å¦‚æœæ˜¯é‡‘å¥é“ç†ï¼š
- **æ ¸å¿ƒè§‚ç‚¹**: [ä¸»è¦è®ºç‚¹]
- **é‡‘å¥æå–**: [å€¼å¾—å¼•ç”¨çš„å¥å­]
- **ä½¿ç”¨å»ºè®®**: [é€‚ç”¨åœºæ™¯]

### å¦‚æœæ˜¯ç©¿æ­ç¾å¦†ï¼š
- **å•å“æ¸…å•**: [ä¸Šè£…/ä¸‹è£…/é‹å­/é…é¥°]
- **æ­é…æŠ€å·§**: [è‰²å½©/å±‚æ¬¡/æ¯”ä¾‹]
- **é€‚åˆäººç¾¤**: [ä½“å‹/è‚¤è‰²/é£æ ¼]

### å¦‚æœæ˜¯ç¾é£Ÿæ¢åº—ï¼š
- **åº—é“ºä¿¡æ¯**: [åº—å/ä½ç½®/äººå‡]
- **èœå“æµ‹è¯„**: [æ¨èèœå“/å£å‘³è¯„ä»·]
- **ä½“éªŒæ„Ÿå—**: [ç¯å¢ƒ/æœåŠ¡/æ€§ä»·æ¯”]

### å¦‚æœæ˜¯æ—…è¡Œæ”»ç•¥ï¼š
- **ç›®çš„åœ°**: [åœ°ç‚¹]
- **è¡Œç¨‹å»ºè®®**: [æ™¯ç‚¹/è·¯çº¿/æ—¶é—´]
- **å®ç”¨ä¿¡æ¯**: [äº¤é€š/ä½å®¿/æ³¨æ„äº‹é¡¹]

### å¦‚æœæ˜¯æ•°ç æµ‹è¯„ï¼š
- **äº§å“ä¿¡æ¯**: [åç§°/å‹å·/ä»·æ ¼]
- **ä¼˜ç¼ºç‚¹**: [ä¼˜ç‚¹å’Œç¼ºç‚¹]
- **è´­ä¹°å»ºè®®**: [æ˜¯å¦æ¨èã€é€‚åˆäººç¾¤]

### å¦‚æœæ˜¯å­¦ä¹ ç¬”è®°ï¼š
- **çŸ¥è¯†ç‚¹**: [æ ¸å¿ƒçŸ¥è¯†è¦ç‚¹]
- **æ–¹æ³•æŠ€å·§**: [å…·ä½“çš„å­¦ä¹ æ–¹æ³•]
- **é€‚ç”¨äººç¾¤**: [é€‚åˆä»€ä¹ˆäºº]

### å¦‚æœæ˜¯å¥èº«è¿åŠ¨ï¼š
- **è®­ç»ƒå†…å®¹**: [åŠ¨ä½œ/è®¡åˆ’/å¼ºåº¦]
- **é¥®é£Ÿå»ºè®®**: [é¥®é£Ÿ/å‡è„‚/å¢è‚Œ]
- **æ³¨æ„äº‹é¡¹**: [å®‰å…¨æé†’]

### å¦‚æœæ˜¯æƒ…æ„Ÿå…³ç³»ï¼š
- **æ ¸å¿ƒè§‚ç‚¹**: [æƒ…æ„Ÿè§‚ç‚¹/åˆ†æ]
- **è¡ŒåŠ¨å»ºè®®**: [å¯æ“ä½œå»ºè®®]
- **æƒ…æ„Ÿä»·å€¼**: [å…±é¸£ç‚¹/å¯å‘]

### å¦‚æœæ˜¯æ–°é—»ç§‘æ™®ï¼š
- **å…³é”®äº‹å®**: [æ ¸å¿ƒäº‹å®ä¿¡æ¯]
- **çŸ¥è¯†ä»·å€¼**: [æœ‰ä»€ä¹ˆå€¼å¾—å­¦ä¹ çš„]
- **å¯é æ€§**: [ä¿¡æ¯æ¥æº/å¯ä¿¡åº¦]

## ğŸ“¸ è§†è§‰åˆ†æ
- **å›¾ç‰‡æ•°é‡**: {image_count}å¼ 
- **å›¾ç‰‡é£æ ¼**: [æè¿°å›¾ç‰‡çš„æ•´ä½“é£æ ¼]
- **å›¾æ–‡é…åˆ**: [æ–‡å­—å’Œå›¾ç‰‡å¦‚ä½•é…åˆ]

## ğŸ’¡ äº®ç‚¹ä¸ä»·å€¼
- **å†…å®¹äº®ç‚¹**: [è¿™ç¯‡ç¬”è®°çš„äº®ç‚¹]
- **å®ç”¨ä»·å€¼**: [â˜…â˜…â˜…â˜…â˜…] - [å®ç”¨æ€§è¯„ä¼°]
- **æ–°é¢–æ€§**: [â˜…â˜…â˜…â˜…â˜…] - [å†…å®¹æœ‰å¤šæ–°]

## ğŸ“ æ€»ç»“è¯„ä»·
[ä¸€å¥è¯æ€»ç»“è¿™ç¯‡ç¬”è®°çš„ä»·å€¼å’Œç‰¹è‰²]

---

## åŸå§‹æ–‡æ¡ˆå†…å®¹:
{text}

---

è¯·ç¡®ä¿ï¼š
1. é¦–å…ˆæ­£ç¡®è¯†åˆ«ç¬”è®°ç±»å‹
2. æ ¹æ®ç±»å‹è¾“å‡ºå¯¹åº”çš„"æ ¸å¿ƒä¿¡æ¯æå–"éƒ¨åˆ†
3. å…¶ä»–éƒ¨åˆ†å¯¹æ‰€æœ‰ç±»å‹éƒ½é€‚ç”¨
4. å¦‚æœæŸéƒ¨åˆ†ä¸é€‚ç”¨ï¼Œæ ‡æ³¨"[ä¸é€‚ç”¨]"
"""


# ==================== API é…ç½® ====================

def get_api_key() -> str:
    """è·å– Gemini API Key"""
    api_key = os.environ.get('GEMINI_API_KEY')
    if api_key:
        return api_key

    try:
        sys.path.insert(0, str(Path(__file__).parent))
        from config_api import API_CONFIG
        api_key = API_CONFIG.get('gemini', {}).get('api_key')
        if api_key:
            return api_key
    except ImportError:
        pass

    return None


def configure_gemini(api_key: str = None) -> bool:
    """é…ç½® Gemini API"""
    if not api_key:
        api_key = get_api_key()

    if not api_key:
        print("âŒ æœªæ‰¾åˆ° Gemini API Key")
        print("\nè¯·é…ç½®ç¯å¢ƒå˜é‡: set GEMINI_API_KEY='your-key'")
        return False

    try:
        genai.configure(api_key=api_key)
        return True
    except Exception as e:
        print(f"âŒ Gemini API é…ç½®å¤±è´¥: {e}")
        return False


# ==================== åˆ†æå™¨ ====================

class XHSAnalyzer:
    """å°çº¢ä¹¦å›¾æ–‡åˆ†æå™¨ - ç®€åŒ–ç‰ˆ"""

    def __init__(self, model: str = 'flash-lite', api_key: str = None):
        self.api_key = api_key or get_api_key()
        self.model_name = f'gemini-2.5-flash-lite' if model == 'flash-lite' else f'gemini-2.5-{model}'

        if not configure_gemini(self.api_key):
            raise ValueError("æ— æ³•é…ç½® Gemini API")

    def upload_images(self, image_paths: List[Path]) -> List:
        """ä¸Šä¼ å›¾ç‰‡åˆ° Gemini"""
        uploaded_files = []

        print(f"\nğŸ“¤ ä¸Šä¼ å›¾ç‰‡åˆ° Gemini...")
        print(f"{'='*60}")

        for i, img_path in enumerate(image_paths, 1):
            print(f"[{i}/{len(image_paths)}] {img_path.name}... ", end='', flush=True)

            try:
                img_file = genai.upload_file(
                    path=str(img_path),
                    display_name=img_path.name
                )

                # ç­‰å¾…å¤„ç†å®Œæˆ
                while img_file.state.name == "PROCESSING":
                    time.sleep(1)
                    img_file = genai.get_file(img_file.name)

                if img_file.state.name == "ACTIVE":
                    size_mb = img_path.stat().st_size / (1024 * 1024)
                    print(f"âœ… ({size_mb:.2f}MB)")
                    uploaded_files.append(img_file)
                else:
                    print(f"âŒ çŠ¶æ€: {img_file.state.name}")

            except Exception as e:
                print(f"âŒ {e}")

        print(f"{'='*60}")
        print(f"âœ… æˆåŠŸä¸Šä¼  {len(uploaded_files)}/{len(image_paths)} å¼ å›¾ç‰‡\n")

        return uploaded_files

    def analyze(self, text: str, image_files: List) -> tuple:
        """åˆ†æå›¾æ–‡å†…å®¹"""
        prompt = UNIVERSAL_PROMPT.format(text=text, image_count=len(image_files))

        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        print(f"ğŸ”„ æ­£åœ¨åˆ†æ...")

        try:
            model = genai.GenerativeModel(self.model_name)
            contents = image_files + [prompt]

            start_time = time.time()
            response = model.generate_content(contents)
            elapsed = time.time() - start_time

            print(f"âœ… åˆ†æå®Œæˆ! ({elapsed:.1f}ç§’)\n")

            # æå– token ä¿¡æ¯
            token_info = {
                'prompt_tokens': 0,
                'candidates_tokens': 0,
                'total_tokens': 0
            }
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                token_info['prompt_tokens'] = response.usage_metadata.prompt_token_count or 0
                token_info['candidates_tokens'] = response.usage_metadata.candidates_token_count or 0
                token_info['total_tokens'] = response.usage_metadata.total_token_count or 0

            return response.text, token_info

        except Exception as e:
            error_msg = f"âŒ åˆ†æå¤±è´¥: {e}"
            print(error_msg)
            return error_msg, {}

    def delete_files(self, files: List):
        """åˆ é™¤å·²ä¸Šä¼ çš„æ–‡ä»¶"""
        for f in files:
            try:
                genai.delete_file(f.name)
            except:
                pass


# ==================== æ–‡ä»¶æ“ä½œ ====================

def get_image_files(image_dir: Path) -> List[Path]:
    """è·å–ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
    image_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']
    image_paths = set()

    for ext in image_extensions:
        image_paths.update(image_dir.glob(f"*{ext}"))

    return sorted(image_paths)


def load_text_content(image_dir: Path) -> tuple:
    """ä»ç›®å½•åŠ è½½æ–‡å­—å†…å®¹"""
    # å°è¯•è¯»å– content.txt
    text_path = image_dir / "content.txt"

    username = image_dir.parent.name  # ä»ç›®å½•åè·å–ç”¨æˆ·å
    text_content = ""

    if text_path.exists():
        with open(text_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–çº¯æ–‡æ¡ˆå†…å®¹
        lines = content.split('\n')
        content_lines = []
        in_content = False
        for line in lines:
            if 'æ–‡æ¡ˆ:' in line or 'desc:' in line.lower():
                in_content = True
                continue
            if in_content:
                content_lines.append(line)

        text_content = '\n'.join(content_lines).strip()

        # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹
        if not text_content:
            text_content = content

    return username, text_content


def save_result(title: str, username: str, text: str,
                result: str, model: str, token_info: dict,
                image_count: int, output_dir: str = "xhs_analysis") -> Path:
    """ä¿å­˜åˆ†æç»“æœ"""
    output_path = Path(output_dir)
    safe_username = username[:30]
    user_output = output_path / safe_username
    user_output.mkdir(parents=True, exist_ok=True)

    safe_title = title[:50].replace('/', '_').replace('\\', '_').replace(':', '_')
    timestamp = time.strftime('%Y%m%d_%H%M%S')

    result_file = user_output / f"{safe_title}_{timestamp}.md"

    with open(result_file, 'w', encoding='utf-8') as f:
        f.write(f"# {title}\n\n")
        f.write(f"## ğŸ“Œ å…ƒä¿¡æ¯\n\n")
        f.write(f"| é¡¹ç›® | å†…å®¹ |\n")
        f.write(f"|------|------|\n")
        f.write(f"| **ä½œè€…** | {username} |\n")
        f.write(f"| **åˆ†ææ—¶é—´** | {time.strftime('%Y-%m-%d %H:%M:%S')} |\n")
        f.write(f"| **ä½¿ç”¨æ¨¡å‹** | {model} |\n")
        f.write(f"| **å›¾ç‰‡æ•°é‡** | {image_count} |\n")

        if token_info and token_info.get('total_tokens', 0) > 0:
            f.write(f"| **Token ä½¿ç”¨** | è¾“å…¥: {token_info.get('prompt_tokens', 0):,} | è¾“å‡º: {token_info.get('candidates_tokens', 0):,} | æ€»è®¡: {token_info.get('total_tokens', 0):,} |\n")

        f.write(f"\n---\n\n")
        f.write(f"## ğŸ“„ åŸå§‹æ–‡å­—\n\n{text}\n\n")
        f.write(f"---\n\n")
        f.write(f"## ğŸ¤– AI åˆ†æç»“æœ\n\n{result}")

    return result_file


# ==================== ä¸»å¤„ç†æµç¨‹ ====================

def process_single_note(image_dir: str, analyzer: XHSAnalyzer,
                        output_dir: str = "xhs_analysis") -> bool:
    """å¤„ç†å•ä¸ªç¬”è®°ç›®å½•"""
    image_dir = Path(image_dir)

    if not image_dir.is_dir():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {image_dir}")
        return False

    # è·å–å›¾ç‰‡æ–‡ä»¶
    image_paths = get_image_files(image_dir)

    if not image_paths:
        print(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return False

    print(f"\n{'='*80}")
    print(f"ğŸ“ ç¬”è®°: {image_dir.name}")
    print(f"ğŸ‘¤ ä½œè€…: {image_dir.parent.name}")
    print(f"ğŸ“¸ å›¾ç‰‡: {len(image_paths)} å¼ ")
    print(f"{'='*80}")

    # åŠ è½½æ–‡å­—å†…å®¹
    username, text_content = load_text_content(image_dir)
    print(f"ğŸ“„ æ–‡å­—: {len(text_content)} å­—ç¬¦\n")

    # ä¸Šä¼ å›¾ç‰‡
    uploaded_files = analyzer.upload_images(image_paths)

    if not uploaded_files:
        print(f"âŒ å›¾ç‰‡ä¸Šä¼ å¤±è´¥")
        return False

    try:
        # åˆ†æ
        result, token_info = analyzer.analyze(text_content, uploaded_files)

        # åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
        analyzer.delete_files(uploaded_files)

        # ä¿å­˜ç»“æœ
        if result and not result.startswith("âŒ"):
            result_file = save_result(
                title=image_dir.name,
                username=username,
                text=text_content,
                result=result,
                model=analyzer.model_name,
                token_info=token_info,
                image_count=len(uploaded_files),
                output_dir=output_dir
            )
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file.name}")

            if token_info and token_info.get('total_tokens', 0) > 0:
                print(f"ğŸ“Š Token: è¾“å…¥ {token_info.get('prompt_tokens', 0):,} | è¾“å‡º {token_info.get('candidates_tokens', 0):,} | æ€»è®¡ {token_info.get('total_tokens', 0):,}")

            return True
        else:
            print(f"âŒ åˆ†æå¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return False


def batch_process_user(user_dir: str, analyzer: XHSAnalyzer,
                       output_dir: str = "xhs_analysis") -> dict:
    """æ‰¹é‡å¤„ç†ç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°"""
    user_path = Path(user_dir)

    if not user_path.is_dir():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {user_dir}")
        return {'total': 0, 'success': 0, 'fail': 0}

    # æŸ¥æ‰¾æ‰€æœ‰ç¬”è®°ç›®å½•
    note_dirs = []
    for item in user_path.iterdir():
        if item.is_dir():
            if get_image_files(item):
                note_dirs.append(item)

    if not note_dirs:
        print(f"âŒ æœªæ‰¾åˆ°åŒ…å«å›¾ç‰‡çš„ç¬”è®°ç›®å½•")
        return {'total': 0, 'success': 0, 'fail': 0}

    print(f"\nğŸ“Š æ‰¾åˆ° {len(note_dirs)} ä¸ªç¬”è®°\n")

    stats = {'total': len(note_dirs), 'success': 0, 'fail': 0}

    for i, note_dir in enumerate(note_dirs, 1):
        print(f"\n[{i}/{len(note_dirs)}] {note_dir.name}")
        print(f"{'='*60}")

        if process_single_note(note_dir, analyzer, output_dir):
            stats['success'] += 1
        else:
            stats['fail'] += 1

        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

    print(f"\n{'='*60}")
    print(f"ğŸ“Š å®Œæˆ")
    print(f"æ€»è®¡: {stats['total']} | æˆåŠŸ: {stats['success']} | å¤±è´¥: {stats['fail']}")

    return stats


# ==================== ä¸»ç¨‹åº ====================

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦å›¾æ–‡ç¬”è®°åˆ†æå·¥å…· - ç®€åŒ–ç‰ˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

1. åˆ†æå•ä¸ªç¬”è®°:
   python xhs_simple_analysis.py --dir "xhs_images/ç”¨æˆ·å/ç¬”è®°æ ‡é¢˜"

2. æ‰¹é‡åˆ†æç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°:
   python xhs_simple_analysis.py --user-dir "xhs_images/ç”¨æˆ·å"

3. æŒ‡å®šæ¨¡å‹:
   python xhs_simple_analysis.py --dir "images" --model flash
        """
    )

    parser.add_argument('--dir', help='å•ä¸ªç¬”è®°çš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--user-dir', help='ç”¨æˆ·æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ‰¹é‡å¤„ç†ï¼‰')
    parser.add_argument('--model', choices=['flash', 'flash-lite', 'pro'],
                        default='flash-lite', help='Gemini æ¨¡å‹')
    parser.add_argument('-o', '--output', default='xhs_analysis',
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: xhs_analysisï¼‰')
    parser.add_argument('--api-key', help='Gemini API Key')

    args = parser.parse_args()

    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = XHSAnalyzer(model=args.model, api_key=args.api_key)
    except ValueError as e:
        print(f"âŒ {e}")
        return

    print(f"\n{'='*80}")
    print(f"ğŸ–¼ï¸  å°çº¢ä¹¦å›¾æ–‡ç¬”è®°åˆ†æå·¥å…·")
    print(f"{'='*80}")

    if args.dir:
        process_single_note(args.dir, analyzer, args.output)
    elif args.user_dir:
        batch_process_user(args.user_dir, analyzer, args.output)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
