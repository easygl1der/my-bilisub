#!/usr/bin/env python3
"""
ä½¿ç”¨ Gemini API æ‰¹é‡ç”Ÿæˆå­—å¹•æ‘˜è¦å’Œæ±‡æ€»æŠ¥å‘Šï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰

æ–¹æ¡ˆ2ï¼šåˆ†æ‰¹æ‘˜è¦å†æ±‡æ€»
1. è¯»å–ä½œè€…æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰ SRT æ–‡ä»¶
2. æ¯ä¸ª SRT å‘ç»™ Gemini â†’ ç”ŸæˆçŸ¥è¯†åº“å‹æ‘˜è¦ï¼ˆåŸºäº knowledge æ¨¡å¼ï¼‰
3. æŠŠæ‰€æœ‰æ‘˜è¦åˆå¹¶å‘ç»™ Gemini â†’ ç”Ÿæˆæ€»æŠ¥å‘Š
4. ä¿å­˜åˆ° {ä½œè€…å}_AIæ€»ç»“.md

ä½¿ç”¨ç¤ºä¾‹:
    # å¤„ç†æŒ‡å®šä½œè€…çš„å­—å¹•æ–‡ä»¶å¤¹
    python gemini_subtitle_summary.py "output/subtitles/å°å¤©fotos"

    # æŒ‡å®šå¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼‰
    python gemini_subtitle_summary.py "output/subtitles/å°å¤©fotos" -j 5

    # æŒ‡å®šGeminiæ¨¡å‹
    python gemini_subtitle_summary.py "output/subtitles/å°å¤©fotos" --model flash-lite
"""

import asyncio
import os
import sys
import time
import json
import argparse
import re
import threading
import csv
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor

# ä¼˜å…ˆä½¿ç”¨æ–° SDK
try:
    from google import genai
    USE_NEW_SDK = True
except ImportError:
    try:
        import google.generativeai as genai
        USE_NEW_SDK = False
    except ImportError:
        print("æœªå®‰è£… google-genai æˆ– google-generativeai åº“")
        print("è¯·è¿è¡Œ: pip install google-genai")
        sys.exit(1)



# çº¿ç¨‹å®‰å…¨çš„æ‰“å°é”
print_lock = threading.Lock()


# ==================== é…ç½® ====================

GEMINI_MODELS = {
    'flash-lite': 'gemini-2.5-flash-lite',
    'flash': 'gemini-2.5-flash',
    'pro': 'gemini-2.5-pro',
}


def get_api_key() -> str:
    """è·å– Gemini API Key"""
    # 1. ç¯å¢ƒå˜é‡
    api_key = os.environ.get('GEMINI_API_KEY')
    if api_key:
        return api_key

    # 2. é…ç½®æ–‡ä»¶
    try:
        sys.path.insert(0, str(Path(__file__).parent))
        from config.config_api import API_CONFIG
        api_key = API_CONFIG.get('gemini', {}).get('api_key')
        if api_key:
            return api_key
    except ImportError:
        pass

    return None


class GeminiClient:
    """Gemini API å®¢æˆ·ç«¯ï¼ˆå…¼å®¹æ–°æ—§ SDKï¼‰"""

    def __init__(self, model: str = 'flash-lite', api_key: str = None):
        self.api_key = api_key or get_api_key()
        self.model_name = GEMINI_MODELS.get(model, GEMINI_MODELS['flash-lite'])
        self.use_new_sdk = USE_NEW_SDK

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° Gemini API Key")

        if self.use_new_sdk:
            # æ–° SDK
            self.client = genai.Client(api_key=self.api_key)
        else:
            # æ—§ SDK
            import google.generativeai as genai_old
            genai_old.configure(api_key=self.api_key)

    def generate_content(self, prompt: str) -> Dict:
        """
        ç”Ÿæˆå†…å®¹

        Returns:
            {'text': 'ç”Ÿæˆå†…å®¹', 'tokens': int, 'input_tokens': int, 'output_tokens': int,
             'success': bool, 'error': str}
        """
        try:
            if self.use_new_sdk:
                # æ–° SDK
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt
                )
                text = response.text
                # æ–° SDK token ä¿¡æ¯
                input_tokens = 0
                output_tokens = 0
                total_tokens = 0
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    metadata = response.usage_metadata
                    input_tokens = getattr(metadata, 'prompt_token_count', 0) or 0
                    output_tokens = getattr(metadata, 'candidates_token_count', 0) or 0
                    total_tokens = getattr(metadata, 'total_token_count', 0) or 0
            else:
                # æ—§ SDK
                import google.generativeai as genai_old
                model = genai_old.GenerativeModel(self.model_name)
                response = model.generate_content(prompt)
                text = response.text
                input_tokens = 0
                output_tokens = 0
                total_tokens = 0
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    metadata = response.usage_metadata
                    input_tokens = getattr(metadata, 'prompt_token_count', 0) or 0
                    output_tokens = getattr(metadata, 'candidates_token_count', 0) or 0
                    total_tokens = getattr(metadata, 'total_token_count', 0) or 0

            return {
                'text': text.strip() if text else '',
                'tokens': total_tokens,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'success': True
            }

        except Exception as e:
            return {
                'text': '',
                'tokens': 0,
                'input_tokens': 0,
                'output_tokens': 0,
                'success': False,
                'error': str(e)
            }


# ==================== MD æ±‡æ€»æ–‡ä»¶è¯»å– ====================

def parse_summary_md(md_path: Path) -> Dict[str, Dict]:
    """
    ä»æ±‡æ€» MD æ–‡ä»¶è§£æè§†é¢‘ä¿¡æ¯è¡¨æ ¼

    Returns:
        {æ ‡é¢˜: {åºå·, æ ‡é¢˜, é“¾æ¥, BVå·, æ—¶é•¿, æ’­æ”¾é‡, è¯„è®ºæ•°, å‘å¸ƒæ—¶é—´}}
    """
    videos = {}

    if not md_path.exists():
        return videos

    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–è¡¨æ ¼æ•°æ®
        in_table = False
        for line in content.split('\n'):
            # è·³è¿‡è¡¨å¤´åˆ†éš”çº¿
            if line.startswith('|:----'):
                in_table = True
                continue

            # æ£€æµ‹è¡¨æ ¼ç»“æŸ
            if in_table and not line.startswith('|'):
                break

            # è§£æè¡¨æ ¼è¡Œ
            if in_table and line.startswith('|'):
                parts = [p.strip() for p in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾ç©ºå…ƒç´ 
                if len(parts) >= 8:
                    try:
                        seq = parts[0]
                        title = parts[1]
                        link = parts[2]
                        # ä»é“¾æ¥ä¸­æå–çº¯URL
                        if '[' in link and '](' in link:
                            link = link.split('](')[1].rstrip(')')
                        bvid = parts[3]
                        duration = parts[4]
                        views = parts[5]
                        comments = parts[6]
                        pub_time = parts[7]

                        # ç”¨æ ‡é¢˜ä½œä¸ºkey
                        videos[title] = {
                            'åºå·': seq,
                            'æ ‡é¢˜': title,
                            'é“¾æ¥': link,
                            'BVå·': bvid,
                            'æ—¶é•¿': duration,
                            'æ’­æ”¾é‡': views,
                            'è¯„è®ºæ•°': comments,
                            'å‘å¸ƒæ—¶é—´': pub_time,
                        }
                    except (ValueError, IndexError):
                        continue

    except Exception as e:
        print(f"âš ï¸ è¯»å– MD æ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")

    return videos


def find_summary_md(subtitle_dir: Path) -> Optional[Path]:
    """
    æŸ¥æ‰¾å¯¹åº”çš„æ±‡æ€» MD æ–‡ä»¶
    """
    author_name = subtitle_dir.name
    possible_paths = [
        subtitle_dir.parent / f"{author_name}_æ±‡æ€».md",
        subtitle_dir / f"{author_name}_æ±‡æ€».md",
        subtitle_dir.parent / "output" / "subtitles" / f"{author_name}_æ±‡æ€».md",
    ]

    for path in possible_paths:
        if path.exists():
            return path

    # åœ¨åŒçº§ç›®å½•æœç´¢
    for md_file in subtitle_dir.parent.glob("*_æ±‡æ€».md"):
        if author_name in md_file.stem:
            return md_file

    return None


def parse_existing_report_results(report_path: Path) -> List[Dict]:
    """ä»ç°æœ‰æŠ¥å‘Šä¸­è§£æå·²æœ‰çš„ç»“æœï¼ˆç”¨äºè¿½åŠ æ¨¡å¼ï¼‰"""
    results = []
    if not report_path.exists():
        return results

    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ç®€å•è§£æï¼šæå– ### æ ‡é¢˜å’Œåé¢çš„å†…å®¹
        lines = content.split('\n')
        current_title = None
        current_content = []

        for line in lines:
            if line.strip().startswith('### ') and not line.strip().startswith('### ğŸ“¹'):
                # ä¿å­˜ä¸Šä¸€ä¸ª
                if current_title:
                    results.append({
                        'title': current_title,
                        'summary': '\n'.join(current_content).strip(),
                        'file': f'{current_title}.srt',
                        'success': True
                    })
                # å¼€å§‹æ–°çš„
                current_title = line.strip()[4:].strip()
                current_content = []
            elif current_title:
                current_content.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ª
        if current_title:
            results.append({
                'title': current_title,
                'summary': '\n'.join(current_content).strip(),
                'file': f'{current_title}.srt',
                'success': True
            })

    except Exception as e:
        print(f"âš ï¸ è§£æç°æœ‰æŠ¥å‘Šå¤±è´¥: {e}")

    return results


def format_video_info_header(video_info: Dict, srt_filename: str) -> str:
    """
    æ ¼å¼åŒ–è§†é¢‘åŸºæœ¬ä¿¡æ¯å¤´éƒ¨
    """
    if not video_info:
        return f"> **è§†é¢‘æ–‡ä»¶**: {srt_filename}\n\n"

    info_lines = [
        "## ğŸ“¹ è§†é¢‘ä¿¡æ¯",
        f"| é¡¹ç›® | å†…å®¹ |",
        f"|------|------|",
    ]

    # æŒ‰é¡ºåºæ·»åŠ ä¿¡æ¯
    order = ['åºå·', 'æ ‡é¢˜', 'é“¾æ¥', 'BVå·', 'æ—¶é•¿', 'æ’­æ”¾é‡', 'è¯„è®ºæ•°', 'å‘å¸ƒæ—¶é—´']
    for key in order:
        value = video_info.get(key, '')
        if value:
            if key == 'é“¾æ¥':
                info_lines.append(f"| **{key}** | [{value}]({value}) |")
            elif key == 'æ ‡é¢˜':
                info_lines.append(f"| **{key}** | {value} |")
            else:
                info_lines.append(f"| **{key}** | {value} |")

    return '\n'.join(info_lines) + '\n\n---\n\n'


# ==================== SRT å¤„ç† ====================

def parse_srt(srt_path: Path) -> List[Dict]:
    """
    è§£æ SRT æ–‡ä»¶ï¼Œè¿”å›å­—å¹•æ¡ç›®åˆ—è¡¨

    Returns:
        [
            {'index': 1, 'start': '00:00:01,000', 'end': '00:00:04,000', 'text': 'å­—å¹•å†…å®¹'},
            ...
        ]
    """
    entries = []

    with open(srt_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # SRT æ ¼å¼è§£æ
    pattern = r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.*?)(?=\n\n|\n\d+\n|$)'
    matches = re.findall(pattern, content, re.DOTALL)

    for match in matches:
        index, start, end, text = match
        entries.append({
            'index': int(index),
            'start': start,
            'end': end,
            'text': text.strip().replace('\n', ' ')
        })

    return entries


def srt_to_text(srt_path: Path, max_length: int = 15000) -> str:
    """
    å°† SRT è½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼Œæ§åˆ¶é•¿åº¦é¿å…è¶…é™

    Args:
        srt_path: SRT æ–‡ä»¶è·¯å¾„
        max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰

    Returns:
        å­—å¹•æ–‡æœ¬å†…å®¹
    """
    entries = parse_srt(srt_path)

    # åˆå¹¶æ‰€æœ‰å­—å¹•æ–‡æœ¬
    full_text = ' '.join([e['text'] for e in entries])

    # å¦‚æœè¶…è¿‡é•¿åº¦é™åˆ¶ï¼Œæˆªæ–­å¹¶ä¿ç•™å‰80%
    if len(full_text) > max_length:
        full_text = full_text[:int(max_length * 0.8)] + '\n\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­...]'

    return full_text


# ==================== Gemini è°ƒç”¨ ====================

class GeminiSummarizer:
    """Gemini å­—å¹•æ‘˜è¦ç”Ÿæˆå™¨"""

    def __init__(self, model: str = 'flash-lite', api_key: str = None):
        self.client = GeminiClient(model=model, api_key=api_key)
        self.model_name = self.client.model_name

    def generate_summary(self, text: str, title: str = "") -> Dict:
        """
        ä¸ºå•ä¸ªå­—å¹•ç”ŸæˆçŸ¥è¯†åº“å‹æ‘˜è¦ï¼ˆåŸºäº knowledge æ¨¡å¼ï¼‰

        Args:
            text: å­—å¹•æ–‡æœ¬
            title: è§†é¢‘æ ‡é¢˜

        Returns:
            {'summary': 'æ‘˜è¦å†…å®¹', 'tokens': int}
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿å°†è§†é¢‘å­—å¹•å†…å®¹è½¬åŒ–ä¸ºç»“æ„åŒ–çš„çŸ¥è¯†åº“ç¬”è®°ã€‚è¯·è¯¦ç»†åˆ†æä»¥ä¸‹è§†é¢‘å­—å¹•ï¼Œè¾“å‡ºç”¨äºæ„å»º"ç¬¬äºŒå¤§è„‘"çš„ç¬”è®°ã€‚

{'# ' + title if title else ''}

## ğŸ“‹ è§†é¢‘åŸºæœ¬ä¿¡æ¯
- **æ ¸å¿ƒä¸»é¢˜**: [ä¸€å¥è¯æ¦‚æ‹¬]
- **å†…å®¹ç»“æ„**: [æµæ°´è´¦å¼/è§‚ç‚¹è®ºè¯å¼/æ–°é—»æ±‡æ€»å¼/æ•…äº‹å™è¿°å¼]

## ğŸ“– è§†é¢‘å¤§æ„ï¼ˆ100-200å­—ï¼‰
[ç”¨ç²¾ç‚¼çš„ä¹¦é¢è¯­è¨€æ¦‚æ‹¬è§†é¢‘æ ¸å¿ƒå†…å®¹ï¼Œå»é™¤å†—ä½™çš„å‰æƒ…æè¦å’Œæ— å…³ä¿¡æ¯]

## ğŸ¯ æ ¸å¿ƒè§‚ç‚¹ï¼ˆä¸‰æ®µè®ºï¼‰
[å¦‚æœè§†é¢‘æœ‰æ˜ç¡®è®ºç‚¹ï¼Œç”¨ä¸‰æ®µè®ºå½¢å¼å‘ˆç°]
- **å¤§å‰æ**: [æ™®éæ€§å‰ææˆ–èƒŒæ™¯]
- **å°å‰æ**: [å…·ä½“æƒ…å¢ƒæˆ–æ¡ä»¶]
- **ç»“è®º**: [æœ€ç»ˆè§‚ç‚¹æˆ–ä¸»å¼ ]

[å¦‚æœæ˜¯æ–°é—»åˆ†äº«ç±»ï¼Œåˆ™åˆ—å‡º]
- **æ–°é—»æ¡ç›®1**: [æ ‡é¢˜ + å…³é”®ä¿¡æ¯]
- **æ–°é—»æ¡ç›®2**: [æ ‡é¢˜ + å…³é”®ä¿¡æ¯]

## ğŸ“Š è®ºç‚¹è®ºæ®ç»“æ„
1. **ä¸»è¦è®ºç‚¹**
   - è®ºè¿°å†…å®¹: [è¯¦ç»†è¯´æ˜]
   - æ”¯æŒè®ºæ®: [æ•°æ®ã€æ¡ˆä¾‹ã€é€»è¾‘æ¨ç†]
   - å¯ä¿¡åº¦è¯„ä¼°: [é«˜/ä¸­/ä½ï¼Œè¯´æ˜ç†ç”±]

2. **æ¬¡è¦è®ºç‚¹**ï¼ˆå¦‚æœ‰ï¼‰
   - è®ºè¿°å†…å®¹: [è¯¦ç»†è¯´æ˜]
   - æ”¯æŒè®ºæ®: [æ•°æ®ã€æ¡ˆä¾‹ã€é€»è¾‘æ¨ç†]

## ğŸ’ é‡‘å¥/å¥½è¯å¥½å¥æå–
[è¯·æå–ä»¥ä¸‹ç±»å‹çš„å¥å­]

### 1. å¼•ç»æ®å…¸
- åŸå¥: "..."

### 2. æ•…äº‹/æ¡ˆä¾‹
- åŸå¥/æè¿°: "..."

### 3. ç²¾è¾Ÿè®ºæ®
- åŸå¥: "..."

### 4. æ·±åˆ»è§‚ç‚¹
- åŸå¥: "..."

### 5. å¥½è¯å¥½å¥
- åŸå¥: "..."

## ğŸ“ ä¹¦é¢æ–‡ç¨¿
[å°†å­—å¹•å†…å®¹æ•´ç†æˆç²¾ç‚¼çš„ä¹¦é¢è¡¨è¾¾æ–‡ç¨¿ï¼Œè¦æ±‚ï¼š
- å»é™¤æ‰€æœ‰å£è¯­åŒ–å†—ä½™ï¼ˆå¦‚"é‚£ä¸ª"ã€"å°±æ˜¯"ã€"ç„¶å"ç­‰ï¼‰
- ä½¿ç”¨æ­£å¼ã€ç»“æ„åŒ–çš„ä¹¦é¢è¯­è¨€
- ä¿ç•™æ ¸å¿ƒä¿¡æ¯å’Œé€»è¾‘é“¾æ¡
- é€‚åˆä½œä¸ºæ¨¡å‹è®­ç»ƒçš„è¯­è¨€ææ–™
- å­—æ•°æ§åˆ¶åœ¨åŸæ–‡çš„30%-50%]

## âš ï¸ å†…å®¹è´¨é‡åˆ†æ
### æƒ…ç»ªæ“æ§æ£€æµ‹
- **åˆ¶é€ ç„¦è™‘/FOMOæƒ…ç»ª**: [æ˜¯/å¦]
- **åˆ†æ**: [å¦‚æœæœ‰ï¼Œè¯´æ˜ä½¿ç”¨äº†ä»€ä¹ˆæ‰‹æ³•]

### ä¿¡æ¯å¯é æ€§
- **ä¿¡æ¯æºå¯ä¿¡åº¦**: [é«˜/ä¸­/ä½]
- **äº‹å®æ ¸æŸ¥**: [æœ‰å“ªäº›å¯éªŒè¯çš„äº‹å®]
- **æ½œåœ¨åè§**: [æ˜¯å¦å­˜åœ¨æ˜æ˜¾åè§]

### çŸ¥è¯†ä»·å€¼è¯„ä¼°
- **æ–°é¢–æ€§**: [â˜…â˜…â˜…â˜…â˜…]
- **å®ç”¨æ€§**: [â˜…â˜…â˜…â˜…â˜…]
- **æ·±åº¦**: [â˜…â˜…â˜…â˜…â˜…]
- **æ¨èæ”¶è—**: [æ˜¯/å¦]

---
è¯·ç¡®ä¿è¾“å‡ºç»“æ„å®Œæ•´ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å®è´¨å†…å®¹ã€‚å¦‚æœæŸéƒ¨åˆ†ç¡®å®ä¸é€‚ç”¨ï¼Œè¯·æ ‡æ³¨"[ä¸é€‚ç”¨]"å¹¶è¯´æ˜åŸå› ã€‚

---

## å­—å¹•å†…å®¹

{text}"""

        result = self.client.generate_content(prompt)

        if result['success']:
            return {
                'summary': result['text'],
                'tokens': result['tokens'],
                'success': True
            }
        else:
            return {
                'summary': f"ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                'tokens': 0,
                'success': False,
                'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
            }

    def generate_final_summary(self, summaries: List[Dict], author_name: str,
                               custom_prompt: str = None) -> Dict:
        """
        ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Šï¼ˆçŸ¥è¯†åº“å‹ï¼‰

        Args:
            summaries: æ‘˜è¦åˆ—è¡¨ [{'title': '', 'summary': ''}, ...]
            author_name: ä½œè€…åç§°
            custom_prompt: è‡ªå®šä¹‰æ±‡æ€»æç¤ºè¯

        Returns:
            {'report': 'æ±‡æ€»æŠ¥å‘Š', 'tokens': int}
        """
        # æ„å»ºæ‘˜è¦æ±‡æ€»æ–‡æœ¬
        summaries_text = ""
        for i, item in enumerate(summaries, 1):
            summaries_text += f"\n## è§†é¢‘ {i}: {item['title']}\n{item['summary']}\n"

        default_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿å°†å¤šä¸ªè§†é¢‘å†…å®¹è½¬åŒ–ä¸ºç»“æ„åŒ–çš„çŸ¥è¯†åº“ç¬”è®°ã€‚è¯·åŸºäºä»¥ä¸‹è§†é¢‘å­—å¹•çš„çŸ¥è¯†åº“ç¬”è®°ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢çš„ä½œè€…å†…å®¹åˆ†ææŠ¥å‘Šã€‚

ä½œè€…: {author_name}
è§†é¢‘æ•°é‡: {len(summaries)}

## å„è§†é¢‘çŸ¥è¯†åº“ç¬”è®°

{summaries_text}

---

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆä½¿ç”¨ Markdown æ ¼å¼ï¼‰ï¼š

# {author_name} è§†é¢‘å†…å®¹åˆ†ææŠ¥å‘Š

## ğŸ“‹ ä½œè€…æ¦‚è¿°
- **å†…å®¹é¢†åŸŸ/ä¸»é¢˜**: [ä½œè€…ä¸»è¦å…³æ³¨çš„å†…å®¹é¢†åŸŸ]
- **åˆ›ä½œé£æ ¼ç‰¹ç‚¹**: [å™è¿°æ–¹å¼ã€è¡¨è¾¾é£æ ¼ã€è§†é¢‘èŠ‚å¥ç­‰]
- **ç›®æ ‡å—ä¼—åˆ†æ**: [ä¸»è¦é¢å‘å“ªç±»äººç¾¤]
- **å†…å®¹æ›´æ–°é¢‘ç‡**: [ä»è§†é¢‘æ•°é‡æ¨æ–­æ›´æ–°è§„å¾‹]

## ğŸ¯ æ ¸å¿ƒä¸»é¢˜æ±‡æ€»
åˆ—å‡ºä½œè€…æœ€å¸¸è®¨è®ºçš„3-5ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼Œæ¯ä¸ªä¸»é¢˜åŒ…æ‹¬ï¼š
- **ä¸»é¢˜åç§°**: [ä¸»é¢˜]
- **è®¨è®ºé¢‘æ¬¡**: [é«˜/ä¸­/ä½]
- **ä»£è¡¨æ€§è§‚ç‚¹**: [ä½œè€…åœ¨è¯¥ä¸»é¢˜ä¸Šçš„æ ¸å¿ƒç«‹åœº]
- **ç›¸å…³è§†é¢‘**: [æ¶‰åŠè¯¥ä¸»é¢˜çš„è§†é¢‘]

## ğŸ’¡ è§‚ç‚¹å€¾å‘ä¸æ€ç»´æ–¹å¼
- **ä¸»è¦è§‚ç‚¹å’Œç«‹åœº**: [ä½œè€…çš„æ ¸å¿ƒä¿¡å¿µå’Œä»·å€¼å–å‘]
- **è®ºè¯é£æ ¼**: [æ•°æ®é©±åŠ¨/ç»éªŒåˆ†äº«/ç†è®ºåˆ†æ/æƒ…æ„Ÿå…±é¸£]
- **ç‹¬ç‰¹è§è§£**: [ä½œè€…åŒºåˆ«äºä»–äººçš„ç‹¬ç‰¹è§†è§’]
- **å¯èƒ½çš„è®¤çŸ¥åå·®**: [å®¢è§‚åˆ†æä½œè€…å¯èƒ½å­˜åœ¨çš„åè§]

## ğŸ“Š å†…å®¹ç‰¹è‰²åˆ†æ
### æ ‡é¢˜é£æ ¼
- å‘½åè§„å¾‹: [å¦‚ï¼šè®¾é—®å¼/æ•°å­—å¼/çƒ­ç‚¹å¼]
- å…³é”®è¯åå¥½: [å¸¸ä½¿ç”¨çš„å…³é”®è¯ç±»å‹]

### å™è¿°æ–¹å¼
- å¼€å¤´é£æ ¼: [å¦‚ä½•å¼•å…¥è¯é¢˜]
- ç»“æ„æ¨¡å¼: [å±‚å±‚é€’è¿›/å¹¶åˆ—å¼/æ•…äº‹å‹]
- ç»“å°¾é£æ ¼: [æ€»ç»“å‡å/ç•™ç™½æ€è€ƒ/è¡ŒåŠ¨å·å¬]

### ä¸ªæ€§åŒ–å…ƒç´ 
- å£å¤´ç¦…/æ ‡å¿—æ€§è¡¨è¾¾
- å¸¸ç”¨æ¡ˆä¾‹/ç±»æ¯”
- ä¸ªæ€§åŒ–è§†è§‰/éŸ³é¢‘å…ƒç´ ï¼ˆå¦‚æœ‰æåŠï¼‰

## ğŸŒŸ ä»£è¡¨æ€§å†…å®¹æç‚¼
### é«˜ä»·å€¼è§†é¢‘ï¼ˆ2-3ä¸ªï¼‰
- **è§†é¢‘æ ‡é¢˜**: [æ ‡é¢˜]
  - æ ¸å¿ƒä»·å€¼: [ä¸ºä»€ä¹ˆå€¼å¾—çœ‹]
  - å…³é”®æ”¶è·: [è§‚ä¼—èƒ½è·å¾—ä»€ä¹ˆ]
  - æ¨èæŒ‡æ•°: â˜…â˜…â˜…â˜…â˜…

### é‡‘å¥/è§‚ç‚¹æ±‡æ€»ï¼ˆè·¨è§†é¢‘ï¼‰
[ä»æ‰€æœ‰è§†é¢‘ä¸­æå–å‡ºçš„æœ€å€¼å¾—è®°å½•çš„å¥å­å’Œè§‚ç‚¹]

## ğŸ“ˆ å†…å®¹è¶‹åŠ¿åˆ†æ
- **å†…å®¹æ¼”è¿›**: [ä½œè€…çš„å†…å®¹é£æ ¼/ä¸»é¢˜éšæ—¶é—´çš„å˜åŒ–]
- **å½“å‰çƒ­ç‚¹**: [ä½œè€…æœ€è¿‘å…³æ³¨çš„è¯é¢˜]
- **æœªæ¥å±•æœ›**: [åŸºäºå†…å®¹è¶‹åŠ¿çš„é¢„æµ‹]

## ğŸ“ å­¦ä¹ ä»·å€¼è¯„ä¼°
- **æ–°é¢–æ€§**: â˜…â˜…â˜…â˜…â˜… (å†…å®¹æ˜¯å¦ç‹¬ç‰¹æ–°é¢–)
- **å®ç”¨æ€§**: â˜…â˜…â˜…â˜…â˜… (æ˜¯å¦å¯è½åœ°åº”ç”¨)
- **æ·±åº¦**: â˜…â˜…â˜…â˜…â˜… (æ€è€ƒæ·±åº¦å¦‚ä½•)
- **ç³»ç»Ÿæ€§**: â˜…â˜…â˜…â˜…â˜… (çŸ¥è¯†ä½“ç³»æ˜¯å¦å®Œæ•´)
- **æ¨èæ”¶è—**: [æ˜¯/å¦]

## ğŸ‘¥ é€‚åˆäººç¾¤
[åˆ—å‡ºæœ€é€‚åˆè§‚çœ‹/å­¦ä¹ è¿™ä¸ªUPä¸»å†…å®¹çš„äººç¾¤ç±»å‹]

## ğŸ“š å»¶ä¼¸å­¦ä¹ å»ºè®®
åŸºäºä½œè€…çš„å†…å®¹ï¼Œæ¨èï¼š
- ç›¸å…³ä¸»é¢˜çš„æ·±å…¥å­¦ä¹ æ–¹å‘
- å¯ä»¥äº’è¡¥çš„å…¶ä»–ä½œè€…/èµ„æº
- å®è·µå»ºè®®

---
è¯·ç¡®ä¿æŠ¥å‘Šå†…å®¹è¯¦å®ã€ç»“æ„æ¸…æ™°ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰å®è´¨å†…å®¹ã€‚å°½é‡å¼•ç”¨å…·ä½“è§†é¢‘ä¸­çš„ä¾‹å­æ¥æ”¯æ’‘åˆ†æã€‚"""

        prompt = custom_prompt or default_prompt
        result = self.client.generate_content(prompt)

        if result['success']:
            return {
                'report': result['text'],
                'tokens': result['tokens'],
                'success': True
            }
        else:
            return {
                'report': f"ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                'tokens': 0,
                'success': False,
                'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
            }


# ==================== ä¸»å¤„ç†é€»è¾‘ ====================

def process_video_analysis_file(video_analysis_file: Path, index: int, total: int,
                                 csv_data: Dict[str, Dict] = None) -> Dict:
    """
    å¤„ç†è§†é¢‘åˆ†ææ–‡ä»¶ï¼ˆç›´æ¥è¯»å–å·²æœ‰çš„åˆ†æç»“æœï¼Œæ— éœ€å†æ¬¡è°ƒç”¨ Geminiï¼‰

    Args:
        video_analysis_file: è§†é¢‘åˆ†æ markdown æ–‡ä»¶è·¯å¾„
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°
        csv_data: CSV æ•°æ®å­—å…¸

    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    result = {
        'index': index,
        'title': '',
        'success': False,
        'summary': '',
        'error': None,
        'is_video_analysis': True  # æ ‡è®°ä¸ºè§†é¢‘åˆ†æç±»å‹
    }

    try:
        # æå–æ ‡é¢˜
        title = video_analysis_file.stem.replace('_è§†é¢‘åˆ†æ', '')
        result['title'] = title

        with print_lock:
            print(f"[{index}/{total}] ğŸ“– è¯»å–è§†é¢‘åˆ†æ: {title[:50]}")

        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(video_analysis_file, 'r', encoding='utf-8') as f:
            content = f.read()

        if not content:
            result['error'] = 'æ–‡ä»¶å†…å®¹ä¸ºç©º'
            return result

        # è§†é¢‘åˆ†ææ–‡ä»¶å·²ç»æ˜¯å®Œæ•´çš„ Markdown æ ¼å¼ï¼Œç›´æ¥ä½œä¸ºæ‘˜è¦
        result['summary'] = content
        result['success'] = True

        with print_lock:
            print(f"   â””â”€ âœ… æˆåŠŸ")

    except Exception as e:
        result['error'] = str(e)
        with print_lock:
            print(f"   â””â”€ âŒ å¤±è´¥: {str(e)[:50]}")

    return result


def process_single_video(srt_file: Path, index: int, total: int, model: str, api_key: str,
                        csv_data: Dict[str, Dict] = None) -> Dict:
    """
    å¤„ç†å•ä¸ªè§†é¢‘å­—å¹•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œç”¨äºå¹¶å‘ï¼‰

    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    video_start_time = time.time()
    title = srt_file.stem

    # çº¿ç¨‹å®‰å…¨çš„æ‰“å°
    with print_lock:
        print(f"\n{'='*60}")
        print(f"[çº¿ç¨‹ {threading.current_thread().name}] [{index}/{total}] å¤„ç†: {title}")
        print(f"{'='*60}")

    try:
        # è½¬æ¢ SRT ä¸ºæ–‡æœ¬
        srt_text = srt_to_text(srt_file)

        with print_lock:
            print(f"ğŸ“„ æ–‡æœ¬é•¿åº¦: {len(srt_text):,} å­—ç¬¦")
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨ Gemini API...")

        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„å®¢æˆ·ç«¯
        summarizer = GeminiSummarizer(model=model, api_key=api_key)
        result = summarizer.generate_summary(srt_text, title)

        video_elapsed = time.time() - video_start_time

        if result['success']:
            input_tokens = result.get('input_tokens', 0)
            output_tokens = result.get('output_tokens', 0)
            total_tokens_used = result['tokens']

            with print_lock:
                print(f"  âœ… æˆåŠŸ!")
                print(f"  ğŸ“Š Tokens: è¾“å…¥ {input_tokens:,} | è¾“å‡º {output_tokens:,} | æ€»è®¡ {total_tokens_used:,}")
                print(f"  ğŸ“ æ‘˜è¦é•¿åº¦: {len(result['summary']):,} å­—ç¬¦")
                print(f"  â±ï¸  è€—æ—¶: {video_elapsed:.2f}ç§’")

            return {
                'title': title,
                'summary': result['summary'],
                'file': srt_file.name,
                'index': index,
                'success': True,
                'tokens': total_tokens_used,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens
            }
        else:
            with print_lock:
                print(f"  âŒ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                print(f"  â±ï¸  è€—æ—¶: {video_elapsed:.2f}ç§’")

            return {
                'title': title,
                'summary': f"**å¤„ç†å¤±è´¥**: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                'file': srt_file.name,
                'index': index,
                'success': False,
                'failed': True,
                'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
            }

    except Exception as e:
        video_elapsed = time.time() - video_start_time
        with print_lock:
            print(f"  âŒ å¼‚å¸¸: {str(e)}")
            print(f"  â±ï¸  è€—æ—¶: {video_elapsed:.2f}ç§’")

        return {
            'title': title,
            'summary': f"**å¤„ç†å¼‚å¸¸**: {str(e)}",
            'file': srt_file.name,
            'index': index,
            'success': False,
            'failed': True,
            'error': str(e)
        }


def load_existing_results(report_path: Path) -> set:
    """ä»ç°æœ‰æŠ¥å‘Šä¸­åŠ è½½å·²å¤„ç†çš„è§†é¢‘æ ‡é¢˜"""
    processed_titles = set()
    if not report_path.exists():
        return processed_titles

    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # æŸ¥æ‰¾æ‰€æœ‰ ### æ ‡é¢˜ï¼ˆè§†é¢‘æ ‡é¢˜ï¼‰
            for line in content.split('\n'):
                line = line.strip()
                if line.startswith('### ') and not line.startswith('### ğŸ“¹'):
                    title = line[4:].strip()
                    processed_titles.add(title)
    except Exception as e:
        print(f"âš ï¸ è¯»å–ç°æœ‰æŠ¥å‘Šå¤±è´¥: {e}")

    return processed_titles


def process_subtitles(subtitle_dir: str, model: str = 'flash-lite',
                      custom_prompt: str = None, max_workers: int = 3,
                      incremental: bool = False, append: bool = False) -> tuple:
    """
    å¤„ç†å­—å¹•æ–‡ä»¶å¤¹ï¼Œç”Ÿæˆæ‘˜è¦å’Œæ±‡æ€»æŠ¥å‘Šï¼ˆæ”¯æŒå¹¶å‘ï¼‰

    Args:
        subtitle_dir: å­—å¹•æ–‡ä»¶å¤¹è·¯å¾„
        model: Gemini æ¨¡å‹
        custom_prompt: è‡ªå®šä¹‰æ±‡æ€»æç¤ºè¯
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼‰
        incremental: å¢é‡æ¨¡å¼ï¼Œè·³è¿‡å·²å¤„ç†çš„è§†é¢‘
        append: è¿½åŠ æ¨¡å¼ï¼Œä¿ç•™å·²æœ‰ç»“æœ

    Returns:
        (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡, æ±‡æ€»æŠ¥å‘Šè·¯å¾„)
    """
    subtitle_path = Path(subtitle_dir)

    if not subtitle_path.is_dir():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {subtitle_path}")
        return 0, 0, None

    # è·å–ä½œè€…å
    author_name = subtitle_path.name
    print(f"ğŸ“‚ ä½œè€…: {author_name}")
    print(f"ğŸ“ ç›®å½•: {subtitle_path}")

    # æŸ¥æ‰¾å¹¶è¯»å–æ±‡æ€» MD æ–‡ä»¶ä¸­çš„è§†é¢‘ä¿¡æ¯
    summary_md_path = find_summary_md(subtitle_path)
    video_info_map = {}
    if summary_md_path:
        print(f"ğŸ“‹ è¯»å–è§†é¢‘ä¿¡æ¯: {summary_md_path.name}")
        video_info_map = parse_summary_md(summary_md_path)
        print(f"   æ‰¾åˆ° {len(video_info_map)} ä¸ªè§†é¢‘ä¿¡æ¯")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ±‡æ€» MD æ–‡ä»¶ï¼Œå°†ä¸æ˜¾ç¤ºè§†é¢‘è¯¦ç»†ä¿¡æ¯")

    # æŸ¥æ‰¾æ‰€æœ‰ SRT æ–‡ä»¶å’Œè§†é¢‘åˆ†ææ–‡ä»¶
    all_srt_files = list(subtitle_path.glob("*.srt"))
    video_analysis_files = list(subtitle_path.glob("*_è§†é¢‘åˆ†æ.md"))

    if not all_srt_files and not video_analysis_files:
        print(f"âŒ æœªæ‰¾åˆ° SRT æ–‡ä»¶æˆ–è§†é¢‘åˆ†ææ–‡ä»¶")
        return 0, 0, None

    # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
    file_count = len(all_srt_files) + len(video_analysis_files)
    if all_srt_files:
        print(f"ğŸ“„ æ‰¾åˆ° {len(all_srt_files)} ä¸ªå­—å¹•æ–‡ä»¶")
    if video_analysis_files:
        print(f"ğŸ¬ æ‰¾åˆ° {len(video_analysis_files)} ä¸ªè§†é¢‘åˆ†ææ–‡ä»¶")
    print(f"ğŸ“Š å…±è®¡ {file_count} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")

    # æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    output_dir = subtitle_path.parent
    report_path = output_dir / f"{author_name}_AIæ€»ç»“.md"

    # å¢é‡æ¨¡å¼ï¼šè·³è¿‡å·²å¤„ç†çš„è§†é¢‘
    srt_files = all_srt_files
    existing_results = []

    if incremental or append:
        processed_titles = load_existing_results(report_path)
        if processed_titles:
            print(f"ğŸ“‹ å·²å¤„ç† {len(processed_titles)} ä¸ªè§†é¢‘ï¼ˆå¢é‡æ¨¡å¼ï¼‰")

            # è¿‡æ»¤æ‰å·²å¤„ç†çš„ SRT æ–‡ä»¶
            new_srt_files = []
            for srt_file in all_srt_files:
                title = srt_file.stem
                if title not in processed_titles:
                    new_srt_files.append(srt_file)
                else:
                    print(f"   â­ï¸  è·³è¿‡ SRT: {title}")

            srt_files = new_srt_files

            # è¿‡æ»¤æ‰å·²å¤„ç†çš„è§†é¢‘åˆ†ææ–‡ä»¶
            new_video_files = []
            for video_file in video_analysis_files:
                title = video_file.stem.replace('_è§†é¢‘åˆ†æ', '')
                if title not in processed_titles:
                    new_video_files.append(video_file)
                else:
                    print(f"   â­ï¸  è·³è¿‡è§†é¢‘åˆ†æ: {title}")

            video_analysis_files = new_video_files

            if append and report_path.exists():
                # è¯»å–å·²æœ‰ç»“æœç”¨äºè¿½åŠ 
                existing_results = parse_existing_report_results(report_path)

    # åˆå¹¶æ‰€æœ‰å¾…å¤„ç†æ–‡ä»¶
    all_files = srt_files + video_analysis_files

    if not all_files:
        print("â„¹ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°è§†é¢‘")
        if append and existing_results:
            print(f"   ä¿ç•™å·²æœ‰çš„ {len(existing_results)} ä¸ªç»“æœ")
        return 0, 0, report_path

    print(f"ğŸ“„ å¾…å¤„ç† {len(all_files)} ä¸ªæ–‡ä»¶ï¼ˆSRT: {len(srt_files)}, è§†é¢‘åˆ†æ: {len(video_analysis_files)}ï¼‰")
    print(f"âš¡ å¹¶å‘æ¨¡å¼: {max_workers} ä¸ªçº¿ç¨‹åŒæ—¶å¤„ç†")
    print("=" * 60)

    # è·å– API Key
    api_key = get_api_key()
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° API Key")
        return 0, 0, None

    # å¼€å§‹è®¡æ—¶
    start_time = time.time()

    # çº¿ç¨‹å®‰å…¨çš„ç»“æœå­˜å‚¨
    results_lock = threading.Lock()
    all_results = []

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="Worker") as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = []
        for i, file_path in enumerate(all_files, 1):
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹
            is_video_analysis = file_path.suffix == '.md' and '_è§†é¢‘åˆ†æ' in file_path.stem

            if is_video_analysis:
                # è§†é¢‘åˆ†ææ–‡ä»¶ï¼šç›´æ¥è¯»å–å†…å®¹ï¼Œæ— éœ€å†æ¬¡è°ƒç”¨ Gemini
                future = executor.submit(process_video_analysis_file, file_path, i, len(all_files), video_info_map)
            else:
                # SRT æ–‡ä»¶ï¼šè°ƒç”¨ Gemini ç”Ÿæˆæ‘˜è¦
                future = executor.submit(process_single_video, file_path, i, len(all_files), model, api_key, video_info_map)
            futures.append(future)

        # æ”¶é›†ç»“æœ
        for future in futures:
            result = future.result()
            with results_lock:
                all_results.append(result)
            # æ¯å¤„ç†å®Œä¸€ä¸ªå°±ä¿å­˜è¿›åº¦
            with results_lock:
                temp_results = list(all_results)
            _save_progress(report_path, author_name, all_files, temp_results, video_info_map, existing_results)

    # æŒ‰åŸå§‹é¡ºåºæ’åºç»“æœ
    all_results.sort(key=lambda x: x['index'])

    # åˆå¹¶å·²æœ‰ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    all_summaries = existing_results + all_results

    # ç»Ÿè®¡ç»“æœ
    summaries = []
    success_count = 0
    fail_count = 0
    total_tokens = 0
    total_input_tokens = 0
    total_output_tokens = 0

    for r in all_summaries:
        summaries.append(r)
        if r.get('success'):
            success_count += 1
            total_tokens += r.get('tokens', 0)
            total_input_tokens += r.get('input_tokens', 0)
            total_output_tokens += r.get('output_tokens', 0)
        else:
            fail_count += 1

    # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print(f"ğŸ“ ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š...")

    # åˆ›å»º summarizer ç”¨äºç”Ÿæˆæ€»æŠ¥å‘Š
    summarizer = GeminiSummarizer(model=model, api_key=api_key)

    # è¿‡æ»¤å‡ºæˆåŠŸçš„æ‘˜è¦ç”¨äºç”Ÿæˆæ€»æŠ¥å‘Š
    successful_summaries = [s for s in summaries if not s.get('failed')]
    final_result = summarizer.generate_final_summary(successful_summaries, author_name, custom_prompt)

    # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# {author_name} è§†é¢‘å†…å®¹åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**è§†é¢‘æ•°é‡**: {len(all_summaries)}\n\n")
        f.write(f"**æˆåŠŸå¤„ç†**: {success_count}\n\n")
        f.write(f"**ä½¿ç”¨æ¨¡å‹**: {summarizer.model_name}\n\n")
        f.write(f"**SDK ç‰ˆæœ¬**: {'æ–°ç‰ˆ google.genai' if USE_NEW_SDK else 'æ—§ç‰ˆ google.generativeai'}\n\n")
        f.write(f"**Token ç»Ÿè®¡**: è¾“å…¥ {total_input_tokens:,} | è¾“å‡º {total_output_tokens:,} | æ€»è®¡ {total_tokens:,}\n\n")
        f.write("---\n\n")

        if final_result['success']:
            f.write(final_result['report'])
        else:
            f.write(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {final_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        f.write("\n\n---\n\n")
        f.write("## é™„å½•: å„è§†é¢‘æ‘˜è¦\n\n")

        for item in summaries:
            # æ·»åŠ è§†é¢‘ä¿¡æ¯å¤´éƒ¨
            title = item['title']
            video_info = None
            if video_info_map:
                # å°è¯•åŒ¹é…è§†é¢‘ä¿¡æ¯
                for info_title, info in video_info_map.items():
                    if title in info_title or info_title in title:
                        video_info = info
                        break

            if video_info:
                f.write(format_video_info_header(video_info, item['file']))

            # æ ‡è®°åˆ†æç±»å‹
            if item.get('is_video_analysis'):
                f.write(f"### {title} ğŸ¬\n\n")
                f.write(f"*ğŸ“Œ æ¥æº: Gemini è§†é¢‘åˆ†æï¼ˆæ— å­—å¹•å¤‡é€‰æ–¹æ¡ˆï¼‰*\n\n")
            else:
                f.write(f"### {title}\n\n")

            f.write(f"{item['summary']}\n\n")
            f.write(f"*æ¥æºæ–‡ä»¶: {item['file']}*\n\n")

    print(f"âœ… æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # æœ€ç»ˆç»Ÿè®¡
    total_elapsed = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"ğŸ“Š å¤„ç†å®Œæˆ!")
    print(f"  æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count} | æ€»è®¡: {len(all_summaries)}")
    if existing_results:
        print(f"  (å·²æœ‰: {len(existing_results)} | æ–°å¤„ç†: {len(all_results)})")
    print(f"  æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
    if len(all_summaries) > 0:
        print(f"  å¹³å‡æ¯è§†é¢‘: {total_elapsed/len(all_summaries):.2f}ç§’")
    print(f"ğŸ“Š Token ç»Ÿè®¡:")
    print(f"  è¾“å…¥ Tokens: {total_input_tokens:,}")
    print(f"  è¾“å‡º Tokens: {total_output_tokens:,}")
    print(f"  æ€»è®¡ Tokens: {total_tokens:,}")

    return success_count, fail_count, report_path


def _save_progress(report_path: Path, author_name: str, srt_files: list, results: list,
                   video_info_map: dict = None, existing_results: list = None):
    """ä¿å­˜å½“å‰è¿›åº¦åˆ°æ–‡ä»¶"""
    all_results = (existing_results or []) + results

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# {author_name} è§†é¢‘å†…å®¹åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**è§†é¢‘æ•°é‡**: {len(all_results)}\n\n")

        # ç»Ÿè®¡å½“å‰è¿›åº¦
        current_success = sum(1 for r in all_results if r.get('success'))
        current_fail = sum(1 for r in all_results if not r.get('success'))
        current_tokens = sum(r.get('tokens', 0) for r in all_results if r.get('success'))
        current_input = sum(r.get('input_tokens', 0) for r in all_results if r.get('success'))
        current_output = sum(r.get('output_tokens', 0) for r in all_results if r.get('success'))

        f.write(f"**å·²å¤„ç†**: {len(all_results)}\n\n")
        f.write(f"**æˆåŠŸ**: {current_success} | **å¤±è´¥**: {current_fail}\n\n")
        f.write(f"**Token**: è¾“å…¥ {current_input:,} | è¾“å‡º {current_output:,} | æ€»è®¡ {current_tokens:,}\n\n")
        f.write("---\n\n")
        f.write("## å„è§†é¢‘æ‘˜è¦ï¼ˆæŒ‰å¤„ç†é¡ºåºï¼‰\n\n")

        for item in all_results:
            # æ·»åŠ è§†é¢‘ä¿¡æ¯å¤´éƒ¨
            title = item['title']
            video_info = None
            if video_info_map:
                # å°è¯•åŒ¹é…è§†é¢‘ä¿¡æ¯
                for info_title, info in video_info_map.items():
                    if title in info_title or info_title in title:
                        video_info = info
                        break

            if video_info:
                f.write(format_video_info_header(video_info, item['file']))

            f.write(f"### {title}\n\n")
            f.write(f"{item['summary']}\n\n")
            f.write(f"*æ¥æºæ–‡ä»¶: {item['file']}*\n\n")

        if current_fail > 0:
            f.write("---\n\n")
            f.write("## å¤±è´¥åˆ—è¡¨\n\n")
            for item in results:
                if item.get('failed'):
                    f.write(f"- **{item['title']}**: {item.get('error', 'æœªçŸ¥é”™è¯¯')}\n")


# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Gemini API æ‰¹é‡ç”Ÿæˆå­—å¹•æ‘˜è¦å’Œæ±‡æ€»æŠ¥å‘Š",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # å¤„ç†æŒ‡å®šä½œè€…çš„å­—å¹•æ–‡ä»¶å¤¹
    python gemini_subtitle_summary.py "output/subtitles/å°å¤©fotos"

    # æŒ‡å®šå¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼‰
    python gemini_subtitle_summary.py "output/subtitles/å°å¤©fotos" -j 5

    # æŒ‡å®šGeminiæ¨¡å‹
    python gemini_subtitle_summary.py "output/subtitles/å°å¤©fotos" --model flash-lite
        """
    )

    parser.add_argument('subtitle_dir', help='å­—å¹•æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä½œè€…æ–‡ä»¶å¤¹ï¼‰')
    parser.add_argument('-m', '--model', choices=['flash', 'flash-lite', 'pro'],
                        default='flash-lite', help='Gemini æ¨¡å‹ï¼ˆé»˜è®¤: flash-liteï¼‰')
    parser.add_argument('-j', '--jobs', type=int, default=3,
                        help='å¹¶å‘å¤„ç†æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('-p', '--prompt', help='è‡ªå®šä¹‰æ±‡æ€»æç¤ºè¯')
    parser.add_argument('--api-key', help='Gemini API Keyï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('-i', '--incremental', action='store_true',
                        help='å¢é‡æ¨¡å¼ï¼šè·³è¿‡å·²å¤„ç†çš„è§†é¢‘')
    parser.add_argument('-a', '--append', action='store_true',
                        help='è¿½åŠ æ¨¡å¼ï¼šä¿ç•™å·²æœ‰ç»“æœï¼Œåªå¤„ç†æ–°è§†é¢‘')

    args = parser.parse_args()

    # å¤„ç†å­—å¹•
    process_subtitles(args.subtitle_dir, args.model, args.prompt, args.jobs,
                     incremental=args.incremental, append=args.append)


if __name__ == "__main__":
    main()
