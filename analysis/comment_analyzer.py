#!/usr/bin/env python3
"""
ç¤¾äº¤åª’ä½“è¯„è®ºåŒºè§‚ç‚¹åˆ†æå·¥å…·

åŠŸèƒ½ï¼š
1. æ”¯æŒå°çº¢ä¹¦ã€Reddit ç­‰å¹³å°è¯„è®ºçˆ¬å–
2. æŒ‰ç‚¹èµæ•°ç­›é€‰é«˜è´¨é‡è¯„è®º
3. ä½¿ç”¨ AI (GLM/Gemini) åˆ†æè¯„è®ºè§‚ç‚¹å’Œè®ºè¯è´¨é‡
4. è¿‡æ»¤å¼•æˆ˜ã€æ— å…³ã€ä½è´¨é‡è¯„è®º
5. ç”Ÿæˆç»“æ„åŒ–çš„è§‚ç‚¹åˆ†ææŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹:
    # åˆ†æå°çº¢ä¹¦ç¬”è®°è¯„è®º
    python comment_analyzer.py -xhs "ç¬”è®°URL" -o output.md

    # åˆ†æ MediaCrawler å¯¼å‡ºçš„è¯„è®º
    python comment_analyzer.py -csv MediaCrawler/output/xhs/search/comments.csv -o output.md

    # æŒ‡å®šä½¿ç”¨ GLM API
    python comment_analyzer.py -csv comments.csv --ai glm -o output.md
"""

import os
import sys
import time
import json
import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import requests

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ==================== é…ç½® ====================

# Gemini æ¨¡å‹é…ç½®
GEMINI_MODELS = {
    'flash-lite': 'gemini-2.5-flash-lite',
    'flash': 'gemini-2.5-flash',
    'pro': 'gemini-2.5-pro',
}

# GLM æ¨¡å‹é…ç½®
GLM_MODELS = {
    'flash': 'glm-4-flash',
    'air': 'glm-4-air',
    'plus': 'glm-4-plus',
    'std': 'glm-4',
}

# åˆ†ææç¤ºè¯
ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„è®ºåŒºè§‚ç‚¹åˆ†æå¸ˆï¼Œæ“…é•¿ä»è¯„è®ºä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯å’Œè§‚ç‚¹ã€‚

è¯·å¯¹ä»¥ä¸‹è¯„è®ºåˆ—è¡¨è¿›è¡Œåˆ†æï¼Œæ¯æ¡è¯„è®ºéƒ½æ ‡æ³¨äº†ç‚¹èµæ•°ã€‚

## ä»»åŠ¡è¦æ±‚

1. **è¿‡æ»¤ä½è´¨é‡è¯„è®º**ï¼šæ’é™¤ä»¥ä¸‹ç±»å‹çš„è¯„è®º
   - å¼•æˆ˜ã€äººèº«æ”»å‡»ã€æƒ…ç»ªå®£æ³„
   - æ— æ„ä¹‰çš„çŒæ°´ï¼ˆ"å¥½"ã€"é¡¶"ã€"666"ç­‰ï¼‰
   - ä¸ä¸»é¢˜æ— å…³çš„å†…å®¹
   - çº¯ç²¹çš„è¡¨æƒ…ç¬¦å·æˆ–é‡å¤å†…å®¹

2. **è¯„ä¼°è®ºè¯è´¨é‡**ï¼šå¯¹ä¿ç•™çš„è¯„è®ºè¿›è¡Œè¯„åˆ†
   - è®ºæ®æ˜¯å¦å……åˆ†ï¼ˆæœ‰äº‹å®ã€æ•°æ®ã€é€»è¾‘æ¨ç†ï¼‰
   - è§‚ç‚¹æ˜¯å¦æ¸…æ™°æ˜ç¡®
   - æ˜¯å¦æœ‰ç‹¬åˆ°çš„è§è§£
   - æ˜¯å¦æä¾›äº†æœ‰ç”¨çš„ä¿¡æ¯

3. **è§‚ç‚¹èšç±»**ï¼šå°†ç›¸ä¼¼è§‚ç‚¹å½’ç±»

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ğŸ“Š åˆ†ææ¦‚è§ˆ
- æ€»è¯„è®ºæ•°: [æ•°é‡]
- é«˜è´¨é‡è¯„è®ºæ•°: [æ•°é‡]
- ä¸»è¦è§‚ç‚¹ç±»åˆ«: [æ•°é‡] ç±»

## ğŸ¯ æ ¸å¿ƒè§‚ç‚¹ï¼ˆæŒ‰ä»·å€¼æ’åºï¼‰

### è§‚ç‚¹ 1: [è§‚ç‚¹æ ‡é¢˜]
- **æ”¯æŒåº¦**: â­â­â­â­â­ (5/5)
- **ä»£è¡¨æ€§è¯„è®º**: "[å¼•ç”¨æœ€æœ‰åŠ›çš„ä¸€æ¡è¯„è®º]"
- **ç‚¹èµæ•°**: XXXX
- **è®ºè¯è´¨é‡**: [é«˜/ä¸­/ä½]
- **åˆ†æ**: [ä¸ºä»€ä¹ˆè¿™ä¸ªè§‚ç‚¹æœ‰ä»·å€¼ï¼Œè®ºè¯æ˜¯å¦æ‰å®]

**ç›¸å…³è¯„è®ºæ‘˜è¦**:
- è¯„è®º1 (XXXèµ): "[æ‘˜è¦]"
- è¯„è®º2 (XXXèµ): "[æ‘˜è¦]"

### è§‚ç‚¹ 2: [è§‚ç‚¹æ ‡é¢˜]
[åŒä¸Šæ ¼å¼]

## ğŸ’ å€¼å¾—å…³æ³¨çš„è§è§£
[åˆ—å‡º1-3æ¡ç‰¹åˆ«æœ‰æ´å¯ŸåŠ›çš„è¯„è®ºåŠå…¶åˆ†æ]

## âš ï¸ äº‰è®®ç‚¹/ä¸åŒè§‚ç‚¹
[å¦‚æœå­˜åœ¨æ˜æ˜¾çš„å¯¹ç«‹è§‚ç‚¹ï¼Œåˆ—å‡ºåŒæ–¹è®ºæ®]

## ğŸ“ æ€»ç»“
[ç”¨100-200å­—æ€»ç»“è¯„è®ºåŒºçš„ä¸»è¦è§‚ç‚¹å’Œå…±è¯†]

---

## å¾…åˆ†æçš„è¯„è®ºåˆ—è¡¨ï¼š

{comments}

---

è¯·å¼€å§‹åˆ†æã€‚è®°ä½ï¼šåªä¿ç•™æœ‰å®è´¨å†…å®¹çš„è¯„è®ºï¼Œé‡ç‚¹å…³æ³¨ç‚¹èµæ•°é«˜ä¸”è®ºè¯å……åˆ†çš„è§‚ç‚¹ã€‚
"""


# ==================== API é…ç½® ====================

def get_glm_api_key() -> str:
    """è·å– GLM API Key"""
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡
    api_key = os.environ.get('ZHIPU_API_KEY') or os.environ.get('GLM_API_KEY')
    if api_key:
        return api_key

    # ä» config_api.py è·å–
    try:
        from config.config_api import API_CONFIG
        api_key = API_CONFIG.get('zhipu', {}).get('api_key')
        if api_key:
            return api_key
    except ImportError:
        pass

    return None


def get_gemini_api_key() -> str:
    """è·å– Gemini API Key"""
    api_key = os.environ.get('GEMINI_API_KEY')
    if api_key:
        return api_key

    try:
        from config_api import API_CONFIG
        api_key = API_CONFIG.get('gemini', {}).get('api_key')
        if api_key:
            return api_key
    except ImportError:
        pass

    return None


def call_glm_api(prompt: str, model: str = 'glm-4-flash') -> Tuple[str, dict]:
    """è°ƒç”¨ GLM API"""
    api_key = get_glm_api_key()
    if not api_key:
        raise ValueError("æœªæ‰¾åˆ° GLM API Key")

    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "top_p": 0.7,
        "max_tokens": 4000
    }

    response = requests.post(url, headers=headers, json=data, timeout=60)
    response.raise_for_status()
    result = response.json()

    content = result['choices'][0]['message']['content']

    # Token ä¿¡æ¯
    token_info = {
        'prompt_tokens': result.get('usage', {}).get('prompt_tokens', 0),
        'candidates_tokens': result.get('usage', {}).get('completion_tokens', 0),
        'total_tokens': result.get('usage', {}).get('total_tokens', 0),
    }

    return content, token_info


def call_gemini_api(prompt: str, model: str = 'gemini-2.5-flash-lite') -> Tuple[str, dict]:
    """è°ƒç”¨ Gemini API"""
    api_key = get_gemini_api_key()
    if not api_key:
        raise ValueError("æœªæ‰¾åˆ° Gemini API Key")

    import google.generativeai as genai
    genai.configure(api_key=api_key)

    gemini_model = genai.GenerativeModel(model)
    response = gemini_model.generate_content(prompt)

    token_info = {
        'prompt_tokens': 0,
        'candidates_tokens': 0,
        'total_tokens': 0
    }

    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        token_info['prompt_tokens'] = response.usage_metadata.prompt_token_count or 0
        token_info['candidates_tokens'] = response.usage_metadata.candidates_token_count or 0
        token_info['total_tokens'] = response.usage_metadata.total_token_count or 0

    return response.text, token_info


# ==================== æ•°æ®æº ====================

class CommentSource:
    """è¯„è®ºæ•°æ®æºåŸºç±»"""

    def fetch_comments(self, url: str, max_comments: int = 100) -> List[Dict]:
        """è·å–è¯„è®ºåˆ—è¡¨"""
        raise NotImplementedError


class CsvCommentSource(CommentSource):
    """CSV æ–‡ä»¶è¯„è®ºæ•°æ®æº - æ”¯æŒ MediaCrawler è¾“å‡º"""

    def __init__(self):
        self.pandas = None
        self._check_pandas()

    def _check_pandas(self):
        try:
            import pandas as pd
            self.pandas = pd
        except ImportError:
            pass

    def fetch_comments(self, url: str, max_comments: int = 100) -> List[Dict]:
        """ä» CSV/JSON æ–‡ä»¶è¯»å–è¯„è®º"""
        file_path = Path(url)

        if not file_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []

        print(f"ğŸ“‚ ä»æ–‡ä»¶è¯»å–è¯„è®º...")

        if file_path.suffix == '.json':
            return self._load_json(file_path, max_comments)
        elif file_path.suffix == '.csv':
            return self._load_csv(file_path, max_comments)
        else:
            print("âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨ .csv æˆ– .json")
            return []

    def _load_json(self, file_path: Path, max_comments: int) -> List[Dict]:
        """åŠ è½½ JSON æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        comments = []
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            items = data.get('comments', data.get('data', []))
        else:
            return []

        for item in items[:max_comments]:
            comments.append(self._normalize_comment(item))

        print(f"   âœ… è¯»å–åˆ° {len(comments)} æ¡è¯„è®º")
        return comments

    def _load_csv(self, file_path: Path, max_comments: int) -> List[Dict]:
        """åŠ è½½ CSV æ–‡ä»¶"""
        if not self.pandas:
            print("âš ï¸  pandas æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ Python å†…ç½® csv æ¨¡å—")
            return self._load_csv_builtin(file_path, max_comments)

        try:
            df = self.pandas.read_csv(file_path)
            comments = []

            for _, row in df.head(max_comments).iterrows():
                comment = self._normalize_comment(row.to_dict())
                if comment.get('content'):
                    comments.append(comment)

            print(f"   âœ… è¯»å–åˆ° {len(comments)} æ¡è¯„è®º")
            return comments
        except Exception as e:
            print(f"   âš ï¸  pandas è¯»å–å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨å†…ç½® csv æ¨¡å—")
            return self._load_csv_builtin(file_path, max_comments)

    def _load_csv_builtin(self, file_path: Path, max_comments: int) -> List[Dict]:
        """ä½¿ç”¨å†…ç½® csv æ¨¡å—åŠ è½½"""
        import csv
        comments = []

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader):
                if i >= max_comments:
                    break
                comment = self._normalize_comment(row)
                if comment.get('content'):
                    comments.append(comment)

        print(f"   âœ… è¯»å–åˆ° {len(comments)} æ¡è¯„è®º")
        return comments

    def _normalize_comment(self, item: Dict) -> Dict:
        """æ ‡å‡†åŒ–è¯„è®ºæ ¼å¼ - æ”¯æŒå¤šç§å­—æ®µå"""
        # å†…å®¹å­—æ®µ - æ”¯æŒä¸­è‹±æ–‡
        content = (
            item.get('content') or item.get('text') or item.get('comment') or item.get('note_comment') or
            item.get('è¯„è®ºå†…å®¹') or item.get('æ­£æ–‡') or item.get('è¯„è®º') or
            item.get('comment_text') or item.get('body') or ''
        )

        # ç‚¹èµå­—æ®µ - æ”¯æŒå¤šç§æ ¼å¼
        likes_field = (
            item.get('likes') or item.get('like_count') or item.get('likeCount') or
            item.get('sub_comment_count') or item.get('ç‚¹èµæ•°') or item.get('ç‚¹èµ') or
            item.get('liked_count') or item.get('score') or 0
        )

        # å¤„ç†ç‚¹èµæ•°
        try:
            likes = int(str(likes_field).replace(',', '').strip())
        except:
            likes = 0

        # ä½œè€…å­—æ®µ
        author = (
            item.get('author') or item.get('nickname') or item.get('user_name') or
            item.get('ç”¨æˆ·å') or item.get('æ˜µç§°') or item.get('ip_location') or '[æœªçŸ¥]'
        )

        return {
            'content': str(content).strip(),
            'likes': likes,
            'author': str(author),
            'platform': item.get('platform', 'csv')
        }


class MediaCrawlerSource(CommentSource):
    """MediaCrawler è¾“å‡ºç›®å½•æ•°æ®æº"""

    def __init__(self):
        self.media_crawler_path = Path(__file__).parent.parent / "MediaCrawler"

    def fetch_comments(self, url: str, max_comments: int = 100) -> List[Dict]:
        """ä» MediaCrawler è¾“å‡ºç›®å½•è¯»å–è¯„è®º"""
        # url å¯èƒ½æ˜¯å¹³å°åç§° (xhs, reddit ç­‰)
        platform = url.lower().replace('-', '').replace('_', '')
        platform_map = {
            'xhs': 'xhs',
            'xiaohongshu': 'xhs',
            'reddit': 'reddit',
            'bili': 'bili',
            'bilibili': 'bili',
            'zhihu': 'zhihu',
        }

        platform = platform_map.get(platform, url)

        # æŸ¥æ‰¾è¯„è®ºæ–‡ä»¶
        output_dir = self.media_crawler_path / "output" / platform
        if not output_dir.exists():
            print(f"âŒ æœªæ‰¾åˆ° MediaCrawler è¾“å‡ºç›®å½•: {output_dir}")
            return []

        # æŸ¥æ‰¾ comments æ–‡ä»¶
        comments_file = None
        for ext in ['csv', 'json']:
            candidate = output_dir / "search" / f"comments.{ext}"
            if candidate.exists():
                comments_file = candidate
                break

        if not comments_file:
            # å°è¯•æŸ¥æ‰¾ä»»ä½•è¯„è®ºæ–‡ä»¶
            for f in output_dir.rglob("*comment*"):
                if f.suffix in ['.csv', '.json']:
                    comments_file = f
                    break

        if not comments_file:
            print(f"âŒ åœ¨ {output_dir} ä¸­æœªæ‰¾åˆ°è¯„è®ºæ–‡ä»¶")
            print(f"   ğŸ’¡ è¯·å…ˆè¿è¡Œ MediaCrawler çˆ¬å–è¯„è®º")
            return []

        print(f"ğŸ“‚ æ‰¾åˆ°è¯„è®ºæ–‡ä»¶: {comments_file.relative_to(self.media_crawler_path)}")

        # ä½¿ç”¨ CsvCommentSource åŠ è½½
        csv_source = CsvCommentSource()
        return csv_source.fetch_comments(str(comments_file), max_comments)


class RedditCommentSource(CommentSource):
    """Reddit è¯„è®ºæ•°æ®æº"""

    def __init__(self):
        self.praw = None
        self._check_praw()

    def _check_praw(self):
        try:
            import praw
            self.praw = praw
        except ImportError:
            pass

    def fetch_comments(self, url: str, max_comments: int = 100) -> List[Dict]:
        """ä» Reddit è·å–è¯„è®º"""
        if not self.praw:
            print("âš ï¸  PRAW æœªå®‰è£…")
            print("   å®‰è£…: pip install praw")
            return []

        print(f"ğŸ“± å°è¯•ä» Reddit è·å–è¯„è®º...")

        client_id = os.environ.get('REDDIT_CLIENT_ID')
        client_secret = os.environ.get('REDDIT_CLIENT_SECRET')

        if not client_id or not client_secret:
            print("âŒ æœªé…ç½® Reddit API å‡­è¯")
            print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡ REDDIT_CLIENT_ID å’Œ REDDIT_CLIENT_SECRET")
            return []

        try:
            reddit = self.praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent='CommentAnalyzer/1.0'
            )

            match = re.search(r'/comments/([a-z0-9]+)', url)
            if match:
                submission = reddit.submission(id=match.group(1))
            else:
                submission = reddit.submission(url=url)

            submission.comments.replace_more(limit=0)

            comments = []
            for comment in submission.comments.list()[:max_comments]:
                if hasattr(comment, 'body') and hasattr(comment, 'score'):
                    comments.append({
                        'content': comment.body,
                        'likes': comment.score,
                        'author': str(comment.author) if comment.author else '[deleted]',
                        'platform': 'reddit'
                    })

            print(f"   âœ… è·å–åˆ° {len(comments)} æ¡è¯„è®º")
            return comments

        except Exception as e:
            print(f"âŒ Reddit çˆ¬å–å¤±è´¥: {e}")
            return []


class XhsCommentSource(CommentSource):
    """å°çº¢ä¹¦è¯„è®ºæ•°æ®æº - é€šè¿‡ MediaCrawler"""

    def __init__(self):
        self.media_crawler_path = Path(__file__).parent.parent / "MediaCrawler"
        self.has_crawler = (self.media_crawler_path / "main.py").exists()

    def fetch_comments(self, url: str, max_comments: int = 100) -> List[Dict]:
        """ä»å°çº¢ä¹¦è·å–è¯„è®º"""
        if not self.has_crawler:
            print("âŒ MediaCrawler ä¸å¯ç”¨")
            return []

        print(f"ğŸ“± å°è¯•ä»å°çº¢ä¹¦è·å–è¯„è®º...")
        print(f"   URL: {url}")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜æ•°æ®
        source = MediaCrawlerSource()
        comments = source.fetch_comments('xhs', max_comments)

        if comments:
            print(f"   âœ… ä½¿ç”¨ç¼“å­˜çš„è¯„è®ºæ•°æ®")
            return comments

        # å°è¯•è¿è¡Œ MediaCrawler
        print(f"\n   ğŸ’¡ æœªæ‰¾åˆ°ç¼“å­˜çš„è¯„è®ºæ•°æ®")
        print(f"   è¿è¡Œä»¥ä¸‹å‘½ä»¤çˆ¬å–å°çº¢ä¹¦è¯„è®ºï¼š")
        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"   1. ç¼–è¾‘ MediaCrawler/config/base_config.py:")
        print(f"      - è®¾ç½® PLATFORM = \"xhs\"")
        print(f"      - è®¾ç½® CRAWLER_TYPE = \"detail\"")
        print(f"      - è®¾ç½® ENABLE_GET_COMMENTS = True")
        print(f"      - è®¾ç½® CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = {max_comments}")
        print(f"      - åœ¨ XHS_SPECIFIED_NOTE_URL_LIST æ·»åŠ ç¬”è®° URL")
        print(f"   ")
        print(f"   2. è¿è¡Œçˆ¬è™«:")
        print(f"      cd MediaCrawler && python main.py")
        print(f"   ")
        print(f"   3. çˆ¬å–å®Œæˆåï¼Œè¿è¡Œ:")
        print(f"      python comment_analyzer.py -mediacrawler xhs")
        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

        return []


# ==================== è¯„è®ºåˆ†æå™¨ ====================

class CommentAnalyzer:
    """è¯„è®ºåˆ†æå™¨ - æ”¯æŒ GLM å’Œ Gemini"""

    def __init__(self, ai_provider: str = 'glm', model: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            ai_provider: AI æä¾›å•† ('glm' æˆ– 'gemini')
            model: æ¨¡å‹åç§°ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
        """
        self.ai_provider = ai_provider

        if ai_provider == 'glm':
            self.model = model or 'flash'
            self.model_name = GLM_MODELS.get(self.model, GLM_MODELS['flash'])
            # æ£€æŸ¥ API Key
            if not get_glm_api_key():
                raise ValueError("æœªæ‰¾åˆ° GLM API Keyï¼Œè¯·åœ¨ config_api.py ä¸­é…ç½®æˆ–è®¾ç½® ZHIPU_API_KEY ç¯å¢ƒå˜é‡")
        else:
            self.model = model or 'flash-lite'
            self.model_name = GEMINI_MODELS.get(self.model, GEMINI_MODELS['flash-lite'])
            # æ£€æŸ¥å¹¶é…ç½® Gemini
            api_key = get_gemini_api_key()
            if not api_key:
                raise ValueError("æœªæ‰¾åˆ° Gemini API Key")
            import google.generativeai as genai
            genai.configure(api_key=api_key)

    def filter_comments(self, comments: List[Dict], min_likes: int = 0,
                        min_length: int = 10) -> List[Dict]:
        """è¿‡æ»¤è¯„è®º"""
        filtered = []
        for comment in comments:
            content = comment.get('content', '').strip()
            likes = comment.get('likes', 0)

            if len(content) < min_length:
                continue
            if likes < min_likes:
                continue

            filtered.append(comment)

        # æŒ‰ç‚¹èµæ•°æ’åº
        filtered.sort(key=lambda x: x.get('likes', 0), reverse=True)
        return filtered

    def analyze_comments(self, comments: List[Dict]) -> Tuple[str, dict]:
        """ä½¿ç”¨ AI åˆ†æè¯„è®º"""
        if not comments:
            return "æ²¡æœ‰å¯åˆ†æçš„è¯„è®º", {}

        comment_text = self._format_comments(comments)
        prompt = ANALYSIS_PROMPT.format(comments=comment_text)

        print(f"   â””â”€ ä½¿ç”¨æ¨¡å‹: {self.ai_provider.upper()} {self.model_name}")
        print(f"   â””â”€ åˆ†æä¸­...")

        if self.ai_provider == 'glm':
            return call_glm_api(prompt, self.model_name)
        else:
            return call_gemini_api(prompt, self.model_name)

    def _format_comments(self, comments: List[Dict]) -> str:
        """æ ¼å¼åŒ–è¯„è®ºä¸ºæ–‡æœ¬"""
        lines = []
        for i, comment in enumerate(comments, 1):
            content = comment.get('content', '')
            likes = comment.get('likes', 0)
            author = comment.get('author', '[æœªçŸ¥]')

            # æ¸…ç†å†…å®¹
            content = re.sub(r'\s+', ' ', content).strip()

            lines.append(f"{i}. [{likes}èµ] {author}: {content}")

        return '\n\n'.join(lines)


# ==================== è¾“å‡ºç®¡ç† ====================

def save_report(comments: List[Dict], analysis: str, output_path: str,
                source_url: str = "", token_info: dict = None, ai_provider: str = 'glm'):
    """ä¿å­˜åˆ†ææŠ¥å‘Š"""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        # å¤´éƒ¨
        f.write(f"# è¯„è®ºåŒºè§‚ç‚¹åˆ†ææŠ¥å‘Š\n\n")

        # å…ƒä¿¡æ¯
        f.write(f"## ğŸ“Œ å…ƒä¿¡æ¯\n\n")
        f.write(f"| é¡¹ç›® | å†…å®¹ |\n")
        f.write(f"|------|------|\n")
        f.write(f"| **åˆ†ææ—¶é—´** | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |\n")
        f.write(f"| **AI æ¨¡å‹** | {ai_provider.upper()} |\n")
        f.write(f"| **è¯„è®ºæ€»æ•°** | {len(comments)} |\n")

        if source_url:
            f.write(f"| **æ¥æº** | {source_url} |\n")

        if token_info and token_info.get('total_tokens', 0) > 0:
            f.write(f"| **Token ä½¿ç”¨** | è¾“å…¥: {token_info.get('prompt_tokens', 0):,} | è¾“å‡º: {token_info.get('candidates_tokens', 0):,} | **æ€»è®¡: {token_info.get('total_tokens', 0):,}** |\n")

        f.write(f"\n---\n\n")

        # åˆ†æç»“æœ
        f.write(analysis)

    print(f"   â””â”€ ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(
        description="ç¤¾äº¤åª’ä½“è¯„è®ºåŒºè§‚ç‚¹åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

1. åˆ†æ CSV/JSON æ–‡ä»¶:
   python comment_analyzer.py -csv comments.csv -o output.md

2. ä½¿ç”¨ MediaCrawler è¾“å‡º:
   python comment_analyzer.py -mediacrawler xhs -o output.md

3. æŒ‡å®š AI æä¾›å•†:
   python comment_analyzer.py -csv comments.csv --ai gemini -o output.md

4. è®¾ç½®æœ€å°ç‚¹èµæ•°:
   python comment_analyzer.py -csv comments.csv --min-likes 10 -o output.md
        """
    )

    # æ•°æ®æºé€‰é¡¹
    parser.add_argument('-csv', '--csv-file', help='CSV/JSON è¯„è®ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-mediacrawler', metavar='PLATFORM',
                        help='ä» MediaCrawler è¾“å‡ºè¯»å– (xhs/reddit/bili/zhihu)')
    parser.add_argument('-xhs', '--xiaohongshu', help='å°çº¢ä¹¦ç¬”è®° URL (éœ€è¦å…ˆçˆ¬å–)')
    parser.add_argument('-reddit', help='Reddit å¸–å­ URL')

    # AI é€‰é¡¹
    parser.add_argument('--ai', choices=['glm', 'gemini'], default='glm',
                        help='AI æä¾›å•†ï¼ˆé»˜è®¤: glmï¼Œå› ä¸ºä½ æœ‰ GLM API Keyï¼‰')
    parser.add_argument('--model', help='æŒ‡å®šæ¨¡å‹ï¼ˆglm: flash/air/plus, gemini: flash/flash-lite/proï¼‰')

    # è¿‡æ»¤é€‰é¡¹
    parser.add_argument('-n', '--max-comments', type=int, default=50,
                        help='æœ€å¤§åˆ†æè¯„è®ºæ•°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--min-likes', type=int, default=0,
                        help='æœ€å°ç‚¹èµæ•°ï¼ˆé»˜è®¤: 0ï¼‰')
    parser.add_argument('--min-length', type=int, default=10,
                        help='æœ€å°è¯„è®ºé•¿åº¦ï¼ˆé»˜è®¤: 10ï¼‰')

    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('-o', '--output', default='comment_analysis.md',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: comment_analysis.mdï¼‰')

    args = parser.parse_args()

    # æ£€æŸ¥æ•°æ®æº
    if not any([args.csv_file, args.mediacrawler, args.xiaohongshu, args.reddit]):
        parser.print_help()
        print("\nâŒ è¯·æŒ‡å®šè‡³å°‘ä¸€ä¸ªæ•°æ®æº (-csv / -mediacrawler / -xhs / -reddit)")
        return

    # ç¡®å®šæ•°æ®æº
    source_url = ""
    source: CommentSource = None

    if args.csv_file:
        source = CsvCommentSource()
        source_url = args.csv_file
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ æ•°æ®æº: CSV/JSON æ–‡ä»¶")
        print(f"{'='*80}")
    elif args.mediacrawler:
        source = MediaCrawlerSource()
        source_url = f"MediaCrawler/{args.mediacrawler}"
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ æ•°æ®æº: MediaCrawler ({args.mediacrawler})")
        print(f"{'='*80}")
    elif args.xiaohongshu:
        source = XhsCommentSource()
        source_url = args.xiaohongshu
        print(f"\n{'='*80}")
        print(f"ğŸ“± æ•°æ®æº: å°çº¢ä¹¦")
        print(f"{'='*80}")
    elif args.reddit:
        source = RedditCommentSource()
        source_url = args.reddit
        print(f"\n{'='*80}")
        print(f"ğŸ“± æ•°æ®æº: Reddit")
        print(f"{'='*80}")

    # è·å–è¯„è®º
    comments = source.fetch_comments(source_url, args.max_comments)

    if not comments:
        print("âŒ æœªèƒ½è·å–åˆ°è¯„è®º")
        return

    print(f"\nğŸ“Š åŸå§‹è¯„è®ºæ•°: {len(comments)}")

    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = CommentAnalyzer(ai_provider=args.ai, model=args.model)
    except ValueError as e:
        print(f"âŒ {e}")
        return

    # è¿‡æ»¤è¯„è®º
    filtered = analyzer.filter_comments(
        comments,
        min_likes=args.min_likes,
        min_length=args.min_length
    )

    print(f"ğŸ“Š è¿‡æ»¤åè¯„è®ºæ•°: {len(filtered)}")

    if not filtered:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è¯„è®º")
        return

    # æ˜¾ç¤ºå‰å‡ æ¡è¯„è®ºé¢„è§ˆ
    print(f"\nğŸ“ è¯„è®ºé¢„è§ˆ:")
    for i, comment in enumerate(filtered[:3], 1):
        content = comment.get('content', '')
        if len(content) > 80:
            content = content[:80] + "..."
        print(f"   {i}. [{comment.get('likes', 0)}èµ] {content}")

    if len(filtered) > 3:
        print(f"   ... è¿˜æœ‰ {len(filtered) - 3} æ¡")

    # åˆ†æè¯„è®º
    print(f"\nğŸ” å¼€å§‹åˆ†æ...")

    analysis, token_info = analyzer.analyze_comments(filtered)

    # ä¿å­˜æŠ¥å‘Š
    save_report(filtered, analysis, args.output, source_url, token_info, args.ai)

    # æ˜¾ç¤º token ä¿¡æ¯
    if token_info and token_info.get('total_tokens', 0) > 0:
        print(f"   â””â”€ ğŸ“Š Token ä½¿ç”¨: è¾“å…¥ {token_info.get('prompt_tokens', 0):,} | "
              f"è¾“å‡º {token_info.get('candidates_tokens', 0):,} | "
              f"æ€»è®¡ {token_info.get('total_tokens', 0):,}")

    print(f"\nâœ… åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    main()
