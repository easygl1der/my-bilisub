#!/usr/bin/env python3
"""
å¢å¼ºå‹CSVè§†é¢‘æ‰¹é‡å¤„ç†å·¥ä½œæµ - æ•´åˆMediaCrawleræ•°æ®

åŠŸèƒ½ï¼š
1. ä»MediaCrawleræ•°æ®æå–è§†é¢‘é“¾æ¥
2. ä»CSVæ–‡ä»¶è¯»å–è§†é¢‘åˆ—è¡¨
3. è‡ªåŠ¨è¿‡æ»¤æŒ‡å®šçŠ¶æ€çš„è§†é¢‘
4. æ‰¹é‡å¤„ç†ï¼šä¸‹è½½ â†’ Whisper â†’ GLMä¼˜åŒ–
5. æ›´æ–°CSVå¤„ç†çŠ¶æ€
"""

import os
import sys
import csv
import json
import time
from pathlib import Path
from datetime import datetime
import subprocess
import shutil
from typing import List, Dict

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# ==================== MediaCrawleræ•°æ®æå–æ¨¡å— ====================

def find_latest_file(directory, pattern):
    """æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶"""
    files = list(Path(directory).glob(pattern))
    if not files:
        return None
    return max(files, key=lambda x: x.stat().st_mtime)


def extract_from_mediacrawler_csv(csv_file):
    """ä»MediaCrawlerçš„CSVæå–é“¾æ¥"""
    videos = []
    with open(csv_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            note_id = row.get('note_id', row.get('ç¬”è®°ID', ''))
            title = row.get('title', row.get('æ ‡é¢˜', 'æ— æ ‡é¢˜'))
            note_type = row.get('type', row.get('ç±»å‹', 'video'))

            if note_id:
                url = f"https://www.xiaohongshu.com/explore/{note_id}"
                videos.append({
                    'note_id': note_id,
                    'title': title,
                    'url': url,
                    'type': note_type,
                    'source': 'mediacrawler_csv',
                    'status': '',
                    'error': ''
                })
    return videos


def extract_from_mediacrawler_json(json_file):
    """ä»MediaCrawlerçš„JSONæå–é“¾æ¥"""
    videos = []
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

        # å¤„ç†ä¸åŒçš„JSONæ ¼å¼
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            items = data.get('notes', data.get('videos', []))
        else:
            items = []

        for item in items:
            note_id = item.get('note_id', '')
            title = item.get('title', 'æ— æ ‡é¢˜')
            note_type = item.get('type', 'video')

            if note_id:
                url = f"https://www.xiaohongshu.com/explore/{note_id}"
                videos.append({
                    'note_id': note_id,
                    'title': title,
                    'url': url,
                    'type': note_type,
                    'source': 'mediacrawler_json',
                    'status': '',
                    'error': ''
                })
    return videos


def extract_links_from_mediacrawler(data_dir="data/xhs"):
    """
    ä»MediaCrawleræ•°æ®ç›®å½•æå–è§†é¢‘é“¾æ¥

    Args:
        data_dir: MediaCrawleræ•°æ®ç›®å½•

    Returns:
        list: è§†é¢‘ä¿¡æ¯åˆ—è¡¨
    """
    print(f"\nğŸ” æ­£åœ¨æŸ¥æ‰¾MediaCrawleræ•°æ®...")
    print(f"   ç›®å½•: {data_dir}")

    if not os.path.exists(data_dir):
        print(f"   âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return None

    # æŸ¥æ‰¾CSVæ–‡ä»¶
    csv_file = find_latest_file(data_dir, "xhs_notes_*.csv")
    json_file = find_latest_file(data_dir, "xhs_notes_*.json")

    videos = None

    if csv_file:
        print(f"   âœ… æ‰¾åˆ°CSV: {csv_file.name}")
        videos = extract_from_mediacrawler_csv(csv_file)
    elif json_file:
        print(f"   âœ… æ‰¾åˆ°JSON: {json_file.name}")
        videos = extract_from_mediacrawler_json(json_file)
    else:
        print(f"   âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return None

    if not videos:
        print(f"   âš ï¸  æ•°æ®æ–‡ä»¶ä¸ºç©º")
        return None

    print(f"   âœ… æå–åˆ° {len(videos)} ä¸ªè§†é¢‘é“¾æ¥")
    return videos


# ==================== CSVè¯»å–æ¨¡å— ====================

def read_csv_with_filter(csv_file, status_filter=None):
    """
    ä»CSVè¯»å–è§†é¢‘å¹¶è¿‡æ»¤

    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        status_filter: çŠ¶æ€è¿‡æ»¤å™¨ï¼ˆNone=å…¨éƒ¨, 'success'=åªæˆåŠŸçš„, 'fail'=åªå¤±è´¥çš„ï¼‰

    Returns:
        list: è§†é¢‘ä¿¡æ¯åˆ—è¡¨
    """
    videos = []

    with open(csv_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)

        for row in reader:
            # è·³è¿‡æ²¡æœ‰é“¾æ¥çš„è¡Œ
            if not row.get('é“¾æ¥'):
                continue

            # çŠ¶æ€è¿‡æ»¤
            if status_filter:
                current_status = row.get('subtitle_status', '').strip().lower()
                if status_filter == 'success' and current_status != 'success':
                    continue
                elif status_filter == 'fail' and current_status != 'fail':
                    continue

            videos.append({
                'index': row.get('åºå·', ''),
                'title': row.get('æ ‡é¢˜', ''),
                'url': row['é“¾æ¥'],
                'type': row.get('ç±»å‹', 'video'),
                'likes': row.get('ç‚¹èµæ•°', ''),
                'comments': row.get('è¯„è®ºæ•°', ''),
                'publish_time': row.get('å‘å¸ƒæ—¶é—´', ''),
                'status': row.get('subtitle_status', ''),
                'error': row.get('subtitle_error', ''),
                'source': 'csv_file'
            })

    return videos


def save_videos_to_csv(videos, output_file):
    """å°†è§†é¢‘åˆ—è¡¨ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
    if not videos:
        print("âŒ æ²¡æœ‰è§†é¢‘æ•°æ®å¯ä¿å­˜")
        return None

    # ç¡®å®šå­—æ®µ
    fieldnames = ['åºå·', 'æ ‡é¢˜', 'é“¾æ¥', 'ç±»å‹', 'ç‚¹èµæ•°', 'è¯„è®ºæ•°', 'å‘å¸ƒæ—¶é—´', 'subtitle_status', 'subtitle_error']

    with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for i, video in enumerate(videos, 1):
            writer.writerow({
                'åºå·': i,
                'æ ‡é¢˜': video.get('title', ''),
                'é“¾æ¥': video.get('url', ''),
                'ç±»å‹': video.get('type', 'video'),
                'ç‚¹èµæ•°': video.get('likes', ''),
                'è¯„è®ºæ•°': video.get('comments', ''),
                'å‘å¸ƒæ—¶é—´': video.get('publish_time', ''),
                'subtitle_status': video.get('status', ''),
                'subtitle_error': video.get('error', '')
            })

    print(f"âœ… è§†é¢‘åˆ—è¡¨å·²ä¿å­˜: {output_file}")
    return output_file


# ==================== è§†é¢‘å¤„ç†æ¨¡å— ====================

def process_single_video(video_info, model='medium', prompt='optimization'):
    """
    å¤„ç†å•ä¸ªè§†é¢‘

    Returns:
        dict: å¤„ç†ç»“æœ
    """
    url = video_info['url']

    result = {
        'url': url,
        'title': video_info['title'],
        'success': False,
        'error': None,
        'whisper_time': 0,
        'optimize_time': 0,
        'total_time': 0
    }

    print(f"\n{'='*80}")
    print(f"ğŸ¬ å¤„ç†è§†é¢‘: {video_info['title']}")
    print(f"ğŸ“ URL: {url[:80]}...")
    print(f"{'='*80}")

    start_time = time.time()

    try:
        # æ­¥éª¤1ï¼šWhisperè¯†åˆ«
        print("\nğŸ“ æ­¥éª¤ 1/2: Whisperè¯­éŸ³è¯†åˆ«...")
        whisper_start = time.time()

        cmd = [
            'python', 'ultimate_transcribe.py',
            '-u', url,
            '--model', model,
            '--no-ocr'
        ]

        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=1800
        )

        result['whisper_time'] = time.time() - whisper_start

        if proc.returncode != 0:
            result['error'] = "Whisperå¤±è´¥"
            print(f"âŒ {result['error']}")
            return result

        print(f"âœ… Whisperå®Œæˆ (è€—æ—¶: {result['whisper_time']:.1f}ç§’)")

        # æŸ¥æ‰¾SRTæ–‡ä»¶
        import glob
        srt_files = glob.glob('output/transcripts/*.srt')
        if not srt_files:
            result['error'] = "æœªæ‰¾åˆ°SRTæ–‡ä»¶"
            print(f"âŒ {result['error']}")
            return result

        srt_file = max(srt_files, key=os.path.getmtime)
        print(f"ğŸ“„ å­—å¹•æ–‡ä»¶: {os.path.basename(srt_file)}")

        # æ­¥éª¤2ï¼šGLMä¼˜åŒ–
        print("\nğŸ¤– æ­¥éª¤ 2/2: GLMå­—å¹•ä¼˜åŒ–...")
        print(f"   æ¨¡å¼: {prompt}")

        optimize_start = time.time()
        cmd = [
            'python', 'optimize_srt_glm.py',
            '-s', srt_file,
            '-p', prompt
        ]

        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=300
        )

        result['optimize_time'] = time.time() - optimize_start

        if proc.returncode != 0:
            print(f"âš ï¸  GLMä¼˜åŒ–å¤±è´¥ï¼Œä½†ä¿ç•™äº†åŸå§‹å­—å¹•")
        else:
            print(f"âœ… GLMä¼˜åŒ–å®Œæˆ (è€—æ—¶: {result['optimize_time']:.1f}ç§’)")

        result['success'] = True
        result['total_time'] = time.time() - start_time

        print(f"\nâœ… å¤„ç†å®Œæˆ! æ€»è€—æ—¶: {result['total_time']:.1f}ç§’")

    except subprocess.TimeoutExpired:
        result['error'] = "å¤„ç†è¶…æ—¶"
        print(f"âŒ {result['error']}")
    except Exception as e:
        result['error'] = str(e)
        print(f"âŒ å¤„ç†å‡ºé”™: {e}")

    return result


def update_csv_status(csv_file, processed_results):
    """
    æ›´æ–°CSVæ–‡ä»¶çš„å¤„ç†çŠ¶æ€

    Args:
        csv_file: åŸCSVæ–‡ä»¶
        processed_results: å¤„ç†ç»“æœåˆ—è¡¨
    """
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_file = csv_file.replace('.csv', f'_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv')
    shutil.copy2(csv_file, backup_file)
    print(f"\nğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {os.path.basename(backup_file)}")

    # è¯»å–åŸæ•°æ®
    with open(csv_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        fieldnames = reader.fieldnames

    # åˆ›å»ºURLåˆ°ç»“æœçš„æ˜ å°„
    url_to_result = {}
    for result in processed_results:
        url_to_result[result['url']] = result

    # æ›´æ–°çŠ¶æ€
    for row in rows:
        url = row.get('é“¾æ¥', '')
        if url in url_to_result:
            result = url_to_result[url]
            if result['success']:
                row['subtitle_status'] = 'success'
                row['subtitle_error'] = ''
            else:
                row['subtitle_status'] = 'fail'
                row['subtitle_error'] = result.get('error', 'Unknown error')

    # å†™å…¥æ›´æ–°åçš„CSV
    output_file = csv_file.replace('.csv', '_processed.csv')
    with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)

    print(f"âœ… æ›´æ–°åçš„CSVå·²ä¿å­˜: {output_file}")


def save_workflow_report(videos, results, output_file):
    """ä¿å­˜å·¥ä½œæµæŠ¥å‘Š"""
    report = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_videos': len(videos),
        'successful': sum(1 for r in results if r['success']),
        'failed': sum(1 for r in results if not r['success']),
        'total_time': sum(r['total_time'] for r in results),
        'results': results
    }

    # JSONæŠ¥å‘Š
    json_file = output_file.replace('.md', '.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # MarkdownæŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# è§†é¢‘å¤„ç†å·¥ä½œæµæŠ¥å‘Š\n\n")
        f.write(f"**æ—¶é—´**: {report['timestamp']}\n\n")
        f.write(f"## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- æ€»è§†é¢‘æ•°: {report['total_videos']}\n")
        f.write(f"- æˆåŠŸ: {report['successful']}\n")
        f.write(f"- å¤±è´¥: {report['failed']}\n")
        f.write(f"- æ€»è€—æ—¶: {report['total_time']:.1f}ç§’ ({report['total_time']/60:.1f}åˆ†é’Ÿ)\n")
        f.write(f"- å¹³å‡: {report['total_time']/len(results):.1f}ç§’/è§†é¢‘\n\n")

        f.write(f"## ğŸ“ è¯¦ç»†ç»“æœ\n\n")

        for i, (video, result) in enumerate(zip(videos, results), 1):
            status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
            f.write(f"### {i}. {video['title']}\n\n")
            f.write(f"**çŠ¶æ€**: {status}\n\n")
            f.write(f"- URL: {video['url'][:80]}...\n")
            f.write(f"- Whisperè€—æ—¶: {result['whisper_time']:.1f}ç§’\n")
            f.write(f"- GLMä¼˜åŒ–è€—æ—¶: {result['optimize_time']:.1f}ç§’\n")
            f.write(f"- æ€»è€—æ—¶: {result['total_time']:.1f}ç§’\n")
            if result['error']:
                f.write(f"- é”™è¯¯: {result['error']}\n")
            f.write("\n")

    print(f"\nğŸ“Š æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"   JSON: {json_file}")
    print(f"   Markdown: {output_file}")


# ==================== ä¸»ç¨‹åº ====================

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="å¢å¼ºå‹CSVè§†é¢‘æ‰¹é‡å¤„ç†å·¥ä½œæµ - æ•´åˆMediaCrawler",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

1. ä»MediaCrawleræ•°æ®æå–å¹¶å¤„ç†:
   python enhanced_workflow.py --mediacrawler

2. ä»MediaCrawleræ•°æ®æå–å¹¶ä¿å­˜ä¸ºCSV:
   python enhanced_workflow.py --mediacrawler --export-crawled videos.csv

3. å¤„ç†CSVä¸­çš„æ‰€æœ‰è§†é¢‘:
   python enhanced_workflow.py --csv videos.csv

4. åªå¤„ç†æˆåŠŸçš„è§†é¢‘:
   python enhanced_workflow.py --csv videos.csv --filter success

5. åªå¤„ç†å¤±è´¥çš„è§†é¢‘:
   python enhanced_workflow.py --csv videos.csv --filter fail

6. æŒ‡å®šæ¨¡å‹å’Œä¼˜åŒ–æ¨¡å¼:
   python enhanced_workflow.py --csv videos.csv --model medium --prompt tech

7. å¤„ç†å‰3ä¸ªè§†é¢‘:
   python enhanced_workflow.py --csv videos.csv --limit 3
        """
    )

    # è¾“å…¥æºï¼ˆä¸‰ç§æ¨¡å¼äº’æ–¥ï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        '--mediacrawler',
        action='store_true',
        help='ä»MediaCrawleræ•°æ®æå–é“¾æ¥'
    )
    input_group.add_argument(
        '--csv',
        metavar='FILE',
        help='ä»CSVæ–‡ä»¶è¯»å–'
    )

    parser.add_argument(
        '--data-dir',
        default='data/xhs',
        help='MediaCrawleræ•°æ®ç›®å½•ï¼ˆé»˜è®¤: data/xhsï¼‰'
    )
    parser.add_argument(
        '--export-crawled',
        metavar='FILE',
        help='å°†ä»MediaCrawleræå–çš„æ•°æ®å¯¼å‡ºä¸ºCSV'
    )
    parser.add_argument(
        '--filter',
        choices=['all', 'success', 'fail'],
        default='all',
        help='è¿‡æ»¤è§†é¢‘çŠ¶æ€ï¼ˆé»˜è®¤: allï¼‰'
    )
    parser.add_argument(
        '--model',
        default='medium',
        choices=['tiny', 'base', 'small', 'medium', 'large'],
        help='Whisperæ¨¡å‹ï¼ˆé»˜è®¤: mediumï¼‰'
    )
    parser.add_argument(
        '--prompt',
        default='optimization',
        choices=['optimization', 'simple', 'conservative', 'aggressive', 'tech', 'interview', 'vlog'],
        help='GLMä¼˜åŒ–æ¨¡å¼ï¼ˆé»˜è®¤: optimizationï¼‰'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=0,
        help='é™åˆ¶å¤„ç†æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰'
    )
    parser.add_argument(
        '--no-update',
        action='store_true',
        help='ä¸æ›´æ–°CSVæ–‡ä»¶'
    )

    args = parser.parse_args()

    videos = None
    csv_file = None

    # æ–¹å¼1: ä»MediaCrawleræå–
    if args.mediacrawler:
        print("\n" + "="*80)
        print("ğŸ”¥ MediaCrawleræ•°æ®æå–æ¨¡å¼")
        print("="*80)

        videos = extract_links_from_mediacrawler(args.data_dir)

        if not videos:
            print("\nâŒ æ— æ³•ä»MediaCrawleræå–æ•°æ®")
            return

        # å¦‚æœæŒ‡å®šäº†å¯¼å‡ºCSV
        if args.export_crawled:
            csv_file = args.export_crawled
            save_videos_to_csv(videos, csv_file)
            print(f"\nâœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {csv_file}")
            print("   ä½ ç°åœ¨å¯ä»¥ä½¿ç”¨ --csv å‚æ•°å¤„ç†è¿™ä¸ªæ–‡ä»¶")
            return

        # å¦åˆ™åˆ›å»ºä¸´æ—¶CSV
        csv_file = f"temp_mediacrawler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        save_videos_to_csv(videos, csv_file)
        print(f"\nâœ… ä¸´æ—¶CSVå·²åˆ›å»º: {csv_file}")

    # æ–¹å¼2: ä»CSVè¯»å–
    elif args.csv:
        csv_file = args.csv
        print(f"\nğŸ“– ä»CSVè¯»å–è§†é¢‘åˆ—è¡¨: {csv_file}")

        status_filter = None if args.filter == 'all' else args.filter
        videos = read_csv_with_filter(csv_file, status_filter)

        if not videos:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘")
            return

        print(f"ğŸ” è¿‡æ»¤æ¡ä»¶: {args.filter}")

    # é™åˆ¶æ•°é‡
    if args.limit > 0:
        original_count = len(videos)
        videos = videos[:args.limit]
        print(f"âš ï¸  é™åˆ¶å¤„ç†æ•°é‡: {args.limit} (åŸå…±{original_count}ä¸ª)")

    print(f"\nâœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
    print(f"\né…ç½®:")
    print(f"  Whisperæ¨¡å‹: {args.model}")
    print(f"  GLMä¼˜åŒ–: {args.prompt}")
    print(f"  æ›´æ–°CSV: {'å¦' if args.no_update else 'æ˜¯'}")
    print(f"\nå¼€å§‹å¤„ç†...\n")

    # æ‰¹é‡å¤„ç†
    results = []
    for i, video in enumerate(videos, 1):
        print(f"\n{'#'*80}")
        print(f"# è¿›åº¦: [{i}/{len(videos)}]")
        print(f"{'#'*80}")

        result = process_single_video(
            video,
            model=args.model,
            prompt=args.prompt
        )

        results.append(result)

        # ç­‰å¾…ä¸€ä¸‹å†å¤„ç†ä¸‹ä¸€ä¸ª
        if i < len(videos):
            print("\nâ³ ç­‰å¾…3ç§’åå¤„ç†ä¸‹ä¸€ä¸ªè§†é¢‘...")
            time.sleep(3)

    # ä¿å­˜æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"{'='*80}")

    report_file = csv_file.replace('.csv', '_workflow_report.md')
    save_workflow_report(videos, results, report_file)

    # æ›´æ–°CSV
    if not args.no_update and csv_file:
        update_csv_status(csv_file, results)

    # æ‰“å°æ€»ç»“
    successful = sum(1 for r in results if r['success'])
    failed = len(results) - successful
    total_time = sum(r['total_time'] for r in results)

    print(f"\nğŸ“Š å¤„ç†æ€»ç»“:")
    print(f"   æ€»æ•°: {len(results)}")
    print(f"   æˆåŠŸ: {successful}")
    print(f"   å¤±è´¥: {failed}")
    print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"   å¹³å‡: {total_time/len(results):.1f}ç§’/è§†é¢‘")


if __name__ == "__main__":
    main()
